# syntax=docker/dockerfile:1.4
# SPDX-License-Identifier: MIT
# Copyright (c) 2025 OmniNode Team
#
# ONEX Infrastructure Runtime Dockerfile
#
# This multi-stage Dockerfile builds a minimal production image for the
# ONEX Infrastructure runtime kernel. It follows security best practices:
#   - Non-root user execution
#   - Minimal runtime dependencies
#   - BuildKit cache mounts for efficient builds
#
# Build:
#   docker build -f docker/Dockerfile.runtime -t omnibase-infra-runtime .
#
# Build with version labels:
#   docker build \
#     --build-arg RUNTIME_VERSION=1.0.0 \
#     --build-arg BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ") \
#     --build-arg VCS_REF=$(git rev-parse --short HEAD) \
#     --build-arg UV_VERSION=0.6.14 \
#     -f docker/Dockerfile.runtime -t omnibase-infra-runtime .
#
# Run:
#   docker run -p 8085:8085 omnibase-infra-runtime
#
# Environment variables:
#   RUNTIME_PROFILE   - Runtime profile selection (default: all)
#   ONEX_CONTRACTS_DIR - Contract configuration directory (default: /app/contracts)
#   ONEX_LOG_LEVEL    - Logging level (default: INFO)
#   ONEX_ENVIRONMENT  - Environment name (default: local)
#   ONEX_INPUT_TOPIC  - Input topic for event bus (default: requests)
#   ONEX_OUTPUT_TOPIC - Output topic for event bus (default: responses)
#   ONEX_GROUP_ID     - Consumer group ID (default: onex-runtime)
#
# =============================================================================
# PERFORMANCE OPTIMIZATION TIPS
# =============================================================================
#
# This Dockerfile is optimized for fast builds and minimal image size:
#
# 1. BUILDKIT CACHE MOUNTS: Uses --mount=type=cache for apt and uv
#    - Persistent caches across builds reduce download times
#    - Enable with: DOCKER_BUILDKIT=1 docker build ...
#
# 2. LAYER ORDERING: Dependencies installed before source code
#    - Dependency layer cached even when source code changes
#    - uv sync --no-install-project separates deps from package install
#
# 3. MULTI-STAGE BUILD: Separate builder and runtime stages
#    - Builder stage includes build tools (gcc, etc.)
#    - Runtime stage only includes minimal runtime dependencies
#    - Final image size reduced by ~60% vs single-stage build
#
# 4. .dockerignore: Excludes unnecessary files from build context
#    - Tests, docs, IDE files excluded
#    - Reduces context transfer time to Docker daemon
#
# 5. APT CACHE SHARING: --mount=type=cache,sharing=locked for apt
#    - Multiple concurrent builds share apt cache safely
#    - Reduces network bandwidth and build time
#
# 6. PARALLEL BUILDS: Use --build-arg to enable parallelization
#    - export DOCKER_BUILDKIT=1
#    - docker build --build-arg MAKEFLAGS=-j$(nproc) ...
#
# Expected build times (with warm cache):
#   - First build: ~3-5 minutes
#   - Cached dependency build (code change): ~30-60 seconds
#   - Cached full build (no changes): ~5-10 seconds
#
# =============================================================================

ARG PYTHON_VERSION=3.12
# Pin uv version for reproducible builds (must match CI: .github/workflows/*.yml)
ARG UV_VERSION=0.6.14

# ============================================================================
# uv Binary Stage - Named stage so COPY --from can reference it
# ============================================================================
# NOTE: Docker's COPY --from does NOT interpolate ARG variables when
# referencing external images directly. Using a named stage works around
# this limitation. The ARG declarations above are visible here because
# they appear before any FROM directive.
FROM ghcr.io/astral-sh/uv:${UV_VERSION} AS uv-bin

# ============================================================================
# Builder Stage - Install dependencies with uv
# ============================================================================
FROM python:${PYTHON_VERSION}-slim AS builder

# Build-time environment configuration
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    # uv configuration: create venv inside project directory
    UV_PROJECT_ENVIRONMENT=/app/.venv

# Install system dependencies required for building Python packages
# Cache apt packages for faster rebuilds
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt/lists,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    # Build essentials for compiling native extensions
    build-essential \
    # PostgreSQL client library for psycopg2/asyncpg
    libpq-dev \
    # SSL/TLS support
    libssl-dev \
    # Git for private repository access
    git \
    # Curl for health checks during build verification
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv from official image (recommended Docker approach)
# Version pinned via UV_VERSION build arg for reproducible builds
COPY --from=uv-bin /uv /uvx /usr/local/bin/

WORKDIR /app

# ============================================================================
# LAYER CACHING OPTIMIZATION:
# Copy dependency files FIRST (before source code) for maximum cache reuse.
# Dependencies change infrequently, while source code changes frequently.
# This ordering ensures dependency installation is cached across source changes.
# ============================================================================

# Copy dependency manifests for layer caching
# pyproject.toml and uv.lock define all dependencies
COPY pyproject.toml README.md ./
COPY uv.lock ./

# Install dependencies with uv (BEFORE copying source code)
# - Uses BuildKit cache mount for uv cache directory (shared across builds)
# - Installs only main dependencies (no dev dependencies)
# - --no-install-project flag installs dependencies without installing the package itself
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --no-dev --no-install-project

# Copy source code (required for local package install with uv)
# This layer invalidates when source code changes, but dependency layer remains cached
COPY src/ ./src/

# Install the root package itself (now that source code is available)
# This is a separate step to maximize cache reuse
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --no-dev

# Install omniintelligence as a composition-layer dependency.
# This is NOT declared in pyproject.toml because it is a runtime plugin,
# not a build-time dependency of omnibase_infra. Installing it here enables
# plugin auto-discovery by the runtime's handler loader.
#
# We freeze the venv's installed packages into a constraints file so that
# pip cannot mutate any transitive dependency version already locked by
# uv (e.g. python-dotenv). This ensures the runtime image is fully
# reproducible.
RUN uv pip freeze | grep -v '^\(-e\|@\)' > /tmp/constraints.txt

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --constraint /tmp/constraints.txt \
    omniintelligence==0.2.0

# ============================================================================
# Runtime Stage - Minimal production image
# ============================================================================
FROM python:${PYTHON_VERSION}-slim AS runtime

# Build-time arguments for OCI version labels
# These can be passed at build time for better image tracing
ARG RUNTIME_VERSION=0.1.0
ARG BUILD_DATE
ARG VCS_REF

# OCI Image Labels (https://github.com/opencontainers/image-spec/blob/main/annotations.md)
LABEL org.opencontainers.image.title="OmniBase Infra Runtime" \
      org.opencontainers.image.description="ONEX Infrastructure Runtime Service - Contract-driven kernel for event-based infrastructure orchestration" \
      org.opencontainers.image.vendor="OmniNode" \
      org.opencontainers.image.source="https://github.com/OmniNode-ai/omnibase_infra" \
      org.opencontainers.image.documentation="https://github.com/OmniNode-ai/omnibase_infra/blob/main/README.md" \
      org.opencontainers.image.licenses="MIT" \
      org.opencontainers.image.version="${RUNTIME_VERSION}" \
      org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.revision="${VCS_REF}"

# Runtime environment configuration
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    # Add virtual environment to PATH
    PATH="/app/.venv/bin:$PATH" \
    # Python path for module resolution
    PYTHONPATH=/app/src \
    # Runtime profile selection (all | minimal | kafka | consul | secrets)
    RUNTIME_PROFILE=all \
    # Contract configuration directory
    ONEX_CONTRACTS_DIR=/app/contracts \
    # Logging configuration
    ONEX_LOG_LEVEL=INFO \
    # Event bus configuration
    ONEX_ENVIRONMENT=local \
    ONEX_INPUT_TOPIC=requests \
    ONEX_OUTPUT_TOPIC=responses \
    ONEX_GROUP_ID=onex-runtime

# Install minimal runtime dependencies only
# No build tools needed - all Python packages are pre-compiled in builder stage
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt/lists,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    # PostgreSQL client library (runtime only, no -dev)
    libpq5 \
    # Curl for health checks
    curl \
    # CA certificates for HTTPS connections
    ca-certificates \
    # Tini - minimal init system for proper signal handling as PID 1
    # This ensures SIGTERM is properly forwarded to the Python process
    # See: https://github.com/krallin/tini
    tini \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user for security
# UID 1000 is typically the first non-system user, good for volume permissions
RUN groupadd --gid 1000 omniinfra && \
    useradd --uid 1000 --gid omniinfra --shell /bin/bash --create-home omniinfra

WORKDIR /app

# Copy virtual environment from builder stage
# This contains all installed Python packages
COPY --from=builder /app/.venv /app/.venv

# Copy source code with proper ownership
COPY --chown=omniinfra:omniinfra src/ /app/src/

# Include license file for compliance
COPY --chown=omniinfra:omniinfra LICENSE /app/LICENSE

# Copy contracts directory (must exist in build context)
COPY --chown=omniinfra:omniinfra contracts/ /app/contracts/

# Copy entrypoint script that stamps schema fingerprint before kernel start
COPY --chown=omniinfra:omniinfra docker/entrypoint-runtime.sh /app/entrypoint-runtime.sh
RUN chmod +x /app/entrypoint-runtime.sh

# Create runtime directories with proper permissions
RUN mkdir -p /app/logs /app/data /app/tmp && \
    chown -R omniinfra:omniinfra /app

# Switch to non-root user for security
USER omniinfra

# Default port for ONEX runtime
# This should match the port your runtime listens on for health checks
EXPOSE 8085

# Health check using the runtime's health endpoint
# - interval: Check every 30 seconds
# - timeout: Fail if check takes more than 10 seconds
# - start-period: Allow 40 seconds for startup before checking
# - retries: Mark unhealthy after 3 consecutive failures
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl --fail --silent http://localhost:8085/health || exit 1

# Use tini as init to properly handle signals (PID 1 problem)
# Without tini, signals like SIGTERM aren't properly forwarded to the Python process,
# causing Docker to send SIGKILL after timeout (exit code 137 instead of 0/143).
# tini reaps zombie processes and forwards signals correctly.
#
# The entrypoint script stamps the schema fingerprint into db_metadata before
# starting the kernel. This prevents crash-loops caused by NULL fingerprints.
# tini runs as PID 1, spawns the entrypoint script, which exec's into the CMD.
ENTRYPOINT ["/usr/bin/tini", "--", "/app/entrypoint-runtime.sh"]

# Default command runs the ONEX kernel via uv-installed script
# The kernel loads configuration from ONEX_CONTRACTS_DIR and starts the runtime
CMD ["onex-runtime"]
