# docker-compose.runtime.yml
# ONEX Infrastructure Runtime Services
#
# This compose file orchestrates the omnibase_infra runtime containers,
# providing contract-driven kernel bootstrap with configurable profiles
# for different deployment scenarios.
#
# Usage:
#   # Start main runtime (default profile)
#   docker compose -f docker-compose.runtime.yml up -d
#
#   # Start with effects profile
#   docker compose -f docker-compose.runtime.yml --profile effects up -d
#
#   # Start with observability services (agent actions consumer)
#   docker compose -f docker-compose.runtime.yml --profile observability up -d
#
#   # Start all services
#   docker compose -f docker-compose.runtime.yml --profile all up -d
#
#   # Start with workers (scalable)
#   docker compose -f docker-compose.runtime.yml --profile workers up -d
#
#   # Build and start (pick up code changes)
#   docker compose -f docker-compose.runtime.yml up -d --build
#
# Profiles:
#   - main: Core runtime kernel (default, always started)
#   - effects: Effect nodes for external service interactions
#   - workers: Scalable compute workers for parallel processing
#   - observability: Observability services (agent actions consumer, metrics)
#   - all: All services including workers and observability
#
# Network:
#   Uses internal omnibase-infra-network (created automatically by this compose file).
#   For external infrastructure services, use extra_hosts with REMOTE_HOST env var.
name: omnibase-infra-runtime
# ==========================================================================
# Shared Configuration Templates (YAML Anchors)
# ==========================================================================

# Base runtime configuration shared by all services
x-runtime-base: &runtime-base
  build:
    context: ..
    dockerfile: docker/Dockerfile.runtime
    secrets:
      - github_token
  networks:
    - omnibase-infra-network
  # Bridge hosts for infrastructure services
  # Supports both:
  #   - Legacy names (omninode-bridge-*): for backward compatibility with 192.168.86.200
  #   - New names (postgres, redpanda, etc.): for local docker-compose.infra.yml
  #
  # Set INFRA_HOST to switch between local (host-gateway) and remote (192.168.86.200)
  extra_hosts:
    # Legacy service names (omninode_bridge deployment)
    - "omninode-bridge-postgres:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "omninode-bridge-redpanda:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "omninode-bridge-consul:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "omninode-bridge-vault:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "omninode-bridge-redis:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    # New service names (docker-compose.infra.yml)
    - "postgres:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "redpanda:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "valkey:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "consul:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
    - "infisical:${INFRA_HOST:-${REMOTE_HOST:-host-gateway}}"
  restart: unless-stopped
  # Resource limits for container orchestration
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: 512M
      reservations:
        cpus: '0.25'
        memory: 128M
# Common environment variables for all runtime services
x-runtime-env: &runtime-env
  # -------------------------------------------------------------------------
  # Kafka/Redpanda Event Bus Configuration
  # -------------------------------------------------------------------------
  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-omninode-bridge-redpanda:9092}
  # -------------------------------------------------------------------------
  # PostgreSQL Database Configuration
  # SECURITY: POSTGRES_PASSWORD is required - no default for fail-fast behavior
  # -------------------------------------------------------------------------
  POSTGRES_HOST: ${POSTGRES_HOST:-omninode-bridge-postgres}
  POSTGRES_PORT: ${POSTGRES_PORT:-5432}
  POSTGRES_DATABASE: ${POSTGRES_DATABASE:-omninode_bridge}
  POSTGRES_USER: ${POSTGRES_USER:-postgres}
  POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  # -------------------------------------------------------------------------
  # Consul Service Discovery Configuration
  # -------------------------------------------------------------------------
  CONSUL_HOST: ${CONSUL_HOST:-omninode-bridge-consul}
  CONSUL_PORT: ${CONSUL_PORT:-8500}
  CONSUL_SCHEME: ${CONSUL_SCHEME:-http}
  # -------------------------------------------------------------------------
  # Vault Secret Management Configuration
  # SECURITY: VAULT_TOKEN is required - no default for fail-fast behavior
  # -------------------------------------------------------------------------
  VAULT_ADDR: ${VAULT_ADDR:-http://omninode-bridge-vault:8200}
  VAULT_TOKEN: ${VAULT_TOKEN}
  # -------------------------------------------------------------------------
  # Redis/Valkey Cache Configuration
  # SECURITY: REDIS_PASSWORD is required - no default for fail-fast behavior
  # -------------------------------------------------------------------------
  REDIS_HOST: ${REDIS_HOST:-omninode-bridge-redis}
  REDIS_PORT: ${REDIS_PORT:-6379}
  REDIS_PASSWORD: ${REDIS_PASSWORD}
  # -------------------------------------------------------------------------
  # ONEX Runtime Configuration
  # -------------------------------------------------------------------------
  ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
  ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
  ONEX_CONTRACTS_DIR: /app/contracts
  # -------------------------------------------------------------------------
  # OpenTelemetry Observability Configuration
  # -------------------------------------------------------------------------
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://localhost:4317}
  OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME:-omnibase-infra-runtime}
  OTEL_TRACES_EXPORTER: ${OTEL_TRACES_EXPORTER:-otlp}
  OTEL_METRICS_EXPORTER: ${OTEL_METRICS_EXPORTER:-otlp}
# ==========================================================================
# Services
# ==========================================================================
services:
  # ==========================================================================
  # Main Runtime Service - Core kernel bootstrap
  # ==========================================================================
  # Primary runtime service that handles the kernel bootstrap process.
  # Listens on the main request topic and publishes to the response topic.
  # This service must be healthy before effects or workers can start.
  runtime-main:
    !!merge <<: *runtime-base
    container_name: omnibase-infra-runtime-main
    profiles: ["main", "all"]
    environment:
      !!merge <<: *runtime-env
      RUNTIME_PROFILE: main
      ONEX_INPUT_TOPIC: ${ONEX_INPUT_TOPIC:-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_OUTPUT_TOPIC:-responses}
      ONEX_GROUP_ID: ${ONEX_GROUP_ID:-onex-runtime-main}
    # -------------------------------------------------------------------------
    # SECURITY NOTE: Port Exposure
    # -------------------------------------------------------------------------
    # By default, Docker binds ports to all interfaces (0.0.0.0), making the
    # service accessible from any network interface on the host.
    #
    # For production deployments, consider one of these alternatives:
    #   1. Bind to localhost only (service accessible only from host):
    #      - "127.0.0.1:${RUNTIME_MAIN_PORT:-8085}:8085"
    #   2. Use a reverse proxy (nginx, traefik, caddy) to handle external
    #      traffic and keep this service on an internal network only.
    #   3. Use Docker network isolation with no port exposure, accessing
    #      the service only through other containers on the same network.
    # -------------------------------------------------------------------------
    ports:
      - "${RUNTIME_MAIN_PORT:-8085}:8085"
    # Graceful shutdown configuration - allow 90s for clean shutdown
    # Root cause: DB pool cleanup can take 30s+ per connection (5 connections)
    # Runtime grace period (30s) + handler shutdown (30s) + buffer (30s) = 90s
    stop_grace_period: 90s
    volumes:
      # Persistent logs directory
      - runtime_logs:/app/logs
      # Persistent data directory for runtime state
      - runtime_data:/app/data
      # Contract files (read-only mount from host)
      - ../contracts:/app/contracts:ro
    # -------------------------------------------------------------------------
    # Health Check Configuration
    # -------------------------------------------------------------------------
    # This health check serves as both LIVENESS and READINESS probe in Docker.
    #
    # Understanding Liveness vs Readiness:
    #   - LIVENESS: "Should the container be restarted?"
    #     If the check fails, Docker will restart the container.
    #     The /health endpoint checks if the runtime process is responsive.
    #
    #   - READINESS: "Should traffic be routed to this container?"
    #     In Docker Compose, health checks also determine service_healthy
    #     condition for depends_on, controlling startup ordering.
    #
    # For Kubernetes or sophisticated deployments, consider separate endpoints:
    #   - /health for liveness (process alive, basic checks)
    #   - /ready for readiness (dependencies connected, ready to serve)
    #
    # The runtime provides both endpoints - see docker/README.md Health Checks.
    # -------------------------------------------------------------------------
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "com.omninode.service=runtime-main"
      - "com.omninode.profile=main"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Effects Runtime Service - External interaction handlers
  # ==========================================================================
  # Handles effect nodes that interact with external services (Consul, Kafka,
  # Vault, PostgreSQL). Processes effect-specific requests and publishes
  # results back to the event bus.
  runtime-effects:
    !!merge <<: *runtime-base
    container_name: omnibase-infra-runtime-effects
    profiles: ["effects", "all"]
    environment:
      !!merge <<: *runtime-env
      RUNTIME_PROFILE: effects
      ONEX_INPUT_TOPIC: ${ONEX_EFFECTS_INPUT_TOPIC:-effect-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_EFFECTS_OUTPUT_TOPIC:-effect-responses}
      ONEX_GROUP_ID: ${ONEX_EFFECTS_GROUP_ID:-onex-runtime-effects}
    # SECURITY: See runtime-main service for port binding security notes.
    # Consider binding to localhost: "127.0.0.1:${RUNTIME_EFFECTS_PORT:-8086}:8085"
    ports:
      - "${RUNTIME_EFFECTS_PORT:-8086}:8085"
    # Graceful shutdown configuration - allow 90s for clean shutdown
    stop_grace_period: 90s
    volumes:
      # Persistent logs directory
      - effects_logs:/app/logs
      # Persistent data directory for effect state
      - effects_data:/app/data
      # Contract files (read-only mount from host)
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      runtime-main:
        condition: service_healthy
    labels:
      - "com.omninode.service=runtime-effects"
      - "com.omninode.profile=effects"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Worker Runtime Service - Scalable compute workers
  # ==========================================================================
  # Scalable worker containers for parallel compute operations.
  # Can be scaled horizontally using docker compose up --scale runtime-worker=N
  # or via the WORKER_REPLICAS environment variable.
  runtime-worker:
    !!merge <<: *runtime-base
    # Note: container_name omitted to allow scaling multiple instances
    profiles: ["workers", "all"]
    environment:
      !!merge <<: *runtime-env
      RUNTIME_PROFILE: workers
      ONEX_INPUT_TOPIC: ${ONEX_WORKER_INPUT_TOPIC:-worker-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_WORKER_OUTPUT_TOPIC:-worker-responses}
      ONEX_GROUP_ID: ${ONEX_WORKER_GROUP_ID:-onex-runtime-workers}
    # Graceful shutdown configuration - allow 90s for clean shutdown
    stop_grace_period: 90s
    volumes:
      # Persistent logs directory (shared across workers)
      - worker_logs:/app/logs
      # Contract files (read-only mount from host)
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      runtime-main:
        condition: service_healthy
    deploy:
      mode: replicated
      replicas: ${WORKER_REPLICAS:-2}
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=runtime-worker"
      - "com.omninode.profile=workers"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Agent Actions Consumer - Observability service for agent action logging
  # ==========================================================================
  # Consumes agent action events from Kafka and persists them to PostgreSQL.
  # This service is part of the observability profile and provides traceability
  # for agent executions, tool calls, decisions, and errors.
  #
  # Features:
  #   - Batched inserts for high-throughput performance
  #   - Health check endpoint for container orchestration
  #   - Graceful shutdown with batch flushing
  #   - Correlation ID tracking across agent sessions
  agent-actions-consumer:
    !!merge <<: *runtime-base
    container_name: omnibase-infra-agent-actions-consumer
    profiles: ["observability", "all"]
    command: ["python", "-m", "omnibase_infra.services.observability.agent_actions.consumer"]
    environment:
      # Override runtime-env with agent-actions-specific configuration
      # The consumer uses OMNIBASE_INFRA_AGENT_ACTIONS_ prefix for isolation
      OMNIBASE_INFRA_AGENT_ACTIONS_KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-omninode-bridge-redpanda:9092}
      OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@${POSTGRES_HOST:-omninode-bridge-postgres}:${POSTGRES_PORT:-5432}/${POSTGRES_DATABASE:-omninode_bridge}
      OMNIBASE_INFRA_AGENT_ACTIONS_BATCH_SIZE: ${AGENT_ACTIONS_BATCH_SIZE:-100}
      OMNIBASE_INFRA_AGENT_ACTIONS_BATCH_TIMEOUT_MS: ${AGENT_ACTIONS_BATCH_TIMEOUT_MS:-1000}
      OMNIBASE_INFRA_AGENT_ACTIONS_HEALTH_CHECK_PORT: "8087"
      # Standard ONEX environment variables for logging
      ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
      ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
    # SECURITY: See runtime-main service for port binding security notes.
    # Consider binding to localhost: "127.0.0.1:${AGENT_ACTIONS_CONSUMER_PORT:-8087}:8087"
    ports:
      - "${AGENT_ACTIONS_CONSUMER_PORT:-8087}:8087"
    # Graceful shutdown configuration - allow 60s for batch flushing
    # Consumer needs time to flush pending batches before shutdown
    stop_grace_period: 60s
    volumes:
      # Persistent logs directory
      - agent_actions_logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8087/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=agent-actions-consumer"
      - "com.omninode.profile=observability"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
# ==========================================================================
# Secrets Configuration
# ==========================================================================
# Docker BuildKit secrets for secure build-time credential handling.
# The github_token secret is required for private package installation.
secrets:
  github_token:
    environment: "GITHUB_TOKEN"
# ==========================================================================
# Network Configuration
# ==========================================================================
# Internal network for omnibase-infra runtime services.
# Created automatically by this compose file.
networks:
  omnibase-infra-network:
    name: omnibase-infra-network
    driver: bridge
# ==========================================================================
# Volume Configuration
# ==========================================================================
# Named volumes for persistent data storage across container restarts.
volumes:
  # Main runtime logs and data
  runtime_logs:
    name: omnibase-infra-runtime-logs
  runtime_data:
    name: omnibase-infra-runtime-data
  # Effects runtime logs and data
  effects_logs:
    name: omnibase-infra-effects-logs
  effects_data:
    name: omnibase-infra-effects-data
  # Worker runtime logs (shared across worker instances)
  worker_logs:
    name: omnibase-infra-worker-logs
  # Agent actions consumer logs
  agent_actions_logs:
    name: omnibase-infra-agent-actions-logs
