# docker-compose.infra.yml
# ONEX Infrastructure & Runtime Services
#
# This compose file provides the complete ONEX platform: infrastructure services
# (PostgreSQL, Redpanda, Valkey) and runtime services (kernel, effects, workers).
#
# NOTE: Nested variable expansion caveat
# --------------------------------------
# Several DSN/URL values (OMNIBASE_INFRA_DB_URL, OMNIINTELLIGENCE_DB_URL,
# OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN) use nested variable expansion, e.g.:
#
#   ${VAR:-outer-default-${INNER:-inner-default}}
#
# This syntax requires Docker Compose v2.20.0 or later and will SILENTLY produce
# malformed values (e.g. literal "${POSTGRES_USER:-postgres}" in the DSN) on older
# versions. Verify your version with: docker compose version
#
# RECOMMENDED: Set these *_URL / *_DSN variables explicitly in your .env file so
# that nested expansion is never exercised and the DSN is always correct regardless
# of the Compose version in use.
#
# Usage:
#   # Core infrastructure only (postgres, redpanda, valkey)
#   docker compose -f docker/docker-compose.infra.yml up -d
#
#   # With runtime services
#   docker compose -f docker/docker-compose.infra.yml --profile runtime up -d
#
#   # With Consul (service discovery)
#   docker compose -f docker/docker-compose.infra.yml --profile consul up -d
#
#   # With secrets management (Infisical)
#   docker compose -f docker/docker-compose.infra.yml --profile secrets up -d
#
#   # Full stack (all services)
#   docker compose -f docker/docker-compose.infra.yml --profile full up -d
#
#   # Bootstrap mode (ordered startup with Infisical seed)
#   # Prefer using scripts/bootstrap-infisical.sh instead for the full sequence.
#   docker compose -f docker/docker-compose.infra.yml --profile bootstrap up -d
#
#   # Build runtime and start
#   docker compose -f docker/docker-compose.infra.yml --profile runtime up -d --build
#
#   # Stop services
#   docker compose -f docker/docker-compose.infra.yml down
#
#   # Stop and remove volumes (CAUTION: destroys data)
#   docker compose -f docker/docker-compose.infra.yml down -v
#
# Profiles:
#   - (default): Core infrastructure (postgres, redpanda, valkey, topic-manager)
#   - runtime: ONEX runtime services (omninode-runtime, effects, workers, observability)
#   - consul: HashiCorp Consul for service discovery
#   - secrets: Infisical for secrets management
#   - full: All services (infrastructure + runtime + consul + secrets)
#   - bootstrap: Infrastructure + secrets (for bootstrap-infisical.sh sequence)
#
# External Ports:
#   Infrastructure:
#     - PostgreSQL: 5436 (internal 5432)
#     - Redpanda: 29092 (internal 9092)
#     - Valkey: 16379 (internal 6379)
#     - Consul: 28500 (internal 8500) [consul profile]
#     - Infisical: 8880 (internal 8080) [secrets profile]
#   Runtime:
#     - omninode-runtime: 8085 [runtime profile]
#     - runtime-effects: 8086 [runtime profile]
#     - agent-actions-consumer: 8087 [runtime profile]
#     - intelligence-api: 8053 [runtime profile]
#     - contract-resolver: 8091 [runtime profile] (OMN-2756 transitional bridge)
#     - skill-lifecycle-consumer: 8092 [runtime profile] (OMN-2934)
name: omnibase-infra
# ==========================================================================
# IMPORTANT: Requires Docker Compose v2.20.0 or later (nested variable expansion).
# Older versions will SILENTLY produce malformed DSNs.
# Verify: docker compose version
# ==========================================================================
# Shared Configuration Templates (YAML Anchors)
# ==========================================================================

# Default healthcheck configuration
x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 10s
# Default logging configuration
x-logging-defaults: &logging-defaults
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"
# Runtime service base configuration
x-runtime-base: &runtime-base
  build:
    context: ..
    dockerfile: docker/Dockerfile.runtime
  networks:
    - omnibase-infra-network
  logging: *logging-defaults
  restart: unless-stopped
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: 512M
      reservations:
        cpus: '0.25'
        memory: 128M
# Runtime environment variables (OMN-2287: slimmed for Infisical bootstrap)
# When INFISICAL_ADDR is set, the runtime prefetches most config from Infisical.
# Only bootstrap-essential vars remain here; all others come from Infisical.
x-runtime-env: &runtime-env
  # --- Bootstrap essentials (always needed) ---
  POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required}
  POSTGRES_USER: ${POSTGRES_USER:-postgres}
  # See top-of-file note re: nested variable expansion.
  # RECOMMENDED: Set OMNIBASE_INFRA_DB_URL explicitly in .env instead.
  OMNIBASE_INFRA_DB_URL: ${OMNIBASE_INFRA_DB_URL:-postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required for DB URL fallback}@postgres:5432/omnibase_infra}
  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-redpanda:9092}
  ONEX_CONTRACTS_DIR: /app/contracts
  ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
  ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
  # --- Infisical config prefetch (OMN-2287) ---
  # Opt-in: config prefetch only activates when INFISICAL_ADDR is non-empty.
  # When the secrets profile IS active, set INFISICAL_ADDR=http://infisical:8080
  # in your .env or via --env-file so containers reach the Infisical service on
  # the compose network. Do NOT use http://localhost:8880 here — that is the
  # host-accessible port and is unreachable inside the container.
  #
  # When the secrets profile is NOT active (Infisical container absent), leave
  # INFISICAL_ADDR unset (or empty) and the runtime falls back to standard
  # environment variable resolution without attempting to contact Infisical.
  INFISICAL_ADDR: ${INFISICAL_ADDR:-}
  INFISICAL_CLIENT_ID: ${INFISICAL_CLIENT_ID:-}
  INFISICAL_CLIENT_SECRET: ${INFISICAL_CLIENT_SECRET:-}
  INFISICAL_PROJECT_ID: ${INFISICAL_PROJECT_ID:-}
  # Opt-in strict mode: when set to a non-empty value, missing Infisical keys
  # are treated as fatal errors rather than silent fallbacks. Leave empty
  # (the default) to allow standard environment variable resolution as fallback.
  INFISICAL_REQUIRED: ${INFISICAL_REQUIRED:-}
  # --- Fallback values (used when Infisical is not configured) ---
  CONSUL_HOST: ${CONSUL_HOST:-consul}
  CONSUL_PORT: ${CONSUL_PORT:-8500}
  VALKEY_HOST: ${VALKEY_HOST:-valkey}
  VALKEY_PORT: ${VALKEY_PORT:-6379}
  VALKEY_PASSWORD: ${VALKEY_PASSWORD:-valkey-dev-password}
  # OmniIntelligence domain plugin
  # See top-of-file note re: nested variable expansion.
  # RECOMMENDED: Set OMNIINTELLIGENCE_DB_URL explicitly in .env instead.
  OMNIINTELLIGENCE_DB_URL: ${OMNIINTELLIGENCE_DB_URL:-postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required for intelligence DB URL fallback}@postgres:5432/omniintelligence}
  # OmniMemory domain plugin
  OMNIMEMORY_ENABLED: ${OMNIMEMORY_ENABLED:-}
# ==========================================================================
# Services
# ==========================================================================
services:
  # ==========================================================================
  # PostgreSQL - Primary Data Store
  # ==========================================================================
  # SECURITY: Database credentials require explicit configuration.
  # Never use default passwords in production environments.
  # Set POSTGRES_PASSWORD in .env or environment before starting.
  postgres:
    image: postgres:16-alpine
    container_name: omnibase-infra-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      # SECURITY: Password is required and must be explicitly set (no default)
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      POSTGRES_DB: omnibase_infra
      # Per-service role passwords (DB-SPLIT-05 / OMN-2056)
      # Roles are created only when the corresponding password env var is set.
      # Empty defaults are intentional — the init script treats empty as "skip role".
      # Generate each with: openssl rand -hex 32
      ROLE_OMNIBASE_PASSWORD: ${ROLE_OMNIBASE_PASSWORD:-}
      ROLE_OMNICLAUDE_PASSWORD: ${ROLE_OMNICLAUDE_PASSWORD:-}
      ROLE_OMNIDASH_PASSWORD: ${ROLE_OMNIDASH_PASSWORD:-}
      ROLE_OMNIINTELLIGENCE_PASSWORD: ${ROLE_OMNIINTELLIGENCE_PASSWORD:-}
      ROLE_OMNIMEMORY_PASSWORD: ${ROLE_OMNIMEMORY_PASSWORD:-}
      ROLE_OMNINODE_PASSWORD: ${ROLE_OMNINODE_PASSWORD:-}
    ports:
      - "${POSTGRES_EXTERNAL_PORT:-5436}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations/forward:/docker-entrypoint-initdb.d:ro
    networks:
      - omnibase-infra-network
    healthcheck:
      !!merge <<: *healthcheck-defaults
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d omnibase_infra"]
    logging: *logging-defaults
    restart: unless-stopped
    labels:
      - "com.omninode.service=postgres"
      - "com.omninode.layer=infrastructure"
  # ==========================================================================
  # Redpanda - Kafka-Compatible Event Streaming
  # ==========================================================================
  redpanda:
    image: redpandadata/redpanda:v24.2.7
    container_name: omnibase-infra-redpanda
    command:
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:${REDPANDA_EXTERNAL_PORT:-29092}
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:${REDPANDA_PANDAPROXY_PORT:-18082}
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --mode dev-container
      - --smp 1
      - --memory ${REDPANDA_MEMORY:-8G}
      - --kafka-connections-max ${REDPANDA_KAFKA_CONNECTIONS_MAX:-10000}
      - --default-log-level=warn
    ports:
      - "${REDPANDA_EXTERNAL_PORT:-29092}:19092"
      # Pandaproxy (REST API for Kafka)
      - "${REDPANDA_PANDAPROXY_PORT:-18082}:18082"
      # Schema Registry
      - "${REDPANDA_SCHEMA_REGISTRY_PORT:-18081}:18081"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    networks:
      - omnibase-infra-network
    healthcheck:
      !!merge <<: *healthcheck-defaults
      test: ["CMD-SHELL", "rpk cluster health | grep -q 'Healthy:.*true' || exit 1"]
      start_period: 30s
    logging: *logging-defaults
    restart: unless-stopped
    labels:
      - "com.omninode.service=redpanda"
      - "com.omninode.layer=infrastructure"
  # ==========================================================================
  # Topic Provisioning
  # ==========================================================================
  # Topics are provisioned by TopicProvisioner on runtime boot (service_kernel.py).
  # The single source of truth for topic names and config is ALL_PLATFORM_TOPIC_SPECS
  # in src/omnibase_infra/topics/platform_topic_suffixes.py.
  #
  # For manual provisioning without the runtime (e.g., development with just Redpanda):
  #   uv run python -m omnibase_infra.event_bus.service_topic_manager
  # ==========================================================================
  # Valkey - Redis-Compatible Cache and Pub/Sub
  # ==========================================================================
  # SECURITY NOTE: Port 16379 is intentionally exposed for local development.
  # This allows host-based tools and tests to connect directly to Valkey.
  # The default binding to 0.0.0.0 is acceptable for development environments
  # where localhost isolation via host firewall provides sufficient security.
  # Password authentication is enabled by default (VALKEY_PASSWORD=valkey-dev-password).
  # For production, either:
  #   - Remove port exposure and use internal Docker networking only
  #   - Bind to 127.0.0.1: "127.0.0.1:16379:6379"
  #   - Set a strong VALKEY_PASSWORD in .env
  valkey:
    image: valkey/valkey:8.0-alpine
    container_name: omnibase-infra-valkey
    # Shell form avoids passing empty string arguments when VALKEY_PASSWORD is unset
    # $$ escapes prevent Docker Compose from interpolating; the container's
    # shell expands VALKEY_PASSWORD from the environment block below.
    command: >-
      sh -c 'valkey-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru $${VALKEY_PASSWORD:+--requirepass "$$VALKEY_PASSWORD"}'
    ports:
      - "${VALKEY_EXTERNAL_PORT:-16379}:6379"
    volumes:
      - valkey_data:/data
    networks:
      - omnibase-infra-network
    environment:
      # Pass password for healthcheck authentication
      # SECURITY: Set VALKEY_PASSWORD in .env for production. Default is for local dev only.
      VALKEY_PASSWORD: ${VALKEY_PASSWORD:-valkey-dev-password}
    healthcheck:
      !!merge <<: *healthcheck-defaults
      # Use password for healthcheck if VALKEY_PASSWORD is set
      test: ["CMD-SHELL", "valkey-cli $${VALKEY_PASSWORD:+-a \"$$VALKEY_PASSWORD\" --no-auth-warning} ping | grep -q PONG"]
    logging: *logging-defaults
    restart: unless-stopped
    labels:
      - "com.omninode.service=valkey"
      - "com.omninode.layer=infrastructure"
  # ==========================================================================
  # Consul - Service Discovery (Optional)
  # ==========================================================================
  consul:
    image: hashicorp/consul:1.18
    container_name: omnibase-infra-consul
    profiles: ["consul", "full"]
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0 -data-dir=/consul/data
    ports:
      - "${CONSUL_EXTERNAL_PORT:-28500}:8500"
      - "${CONSUL_DNS_PORT:-8600}:8600/udp"
    volumes:
      - consul_data:/consul/data
    networks:
      - omnibase-infra-network
    healthcheck:
      !!merge <<: *healthcheck-defaults
      test: ["CMD", "consul", "members"]
    logging: *logging-defaults
    restart: unless-stopped
    labels:
      - "com.omninode.service=consul"
      - "com.omninode.layer=infrastructure"
  # ==========================================================================
  # Infisical - Secrets Management (Optional)
  # ==========================================================================
  infisical:
    image: infisical/infisical:v0.146.0-postgres
    container_name: omnibase-infra-infisical
    profiles: ["secrets", "full", "bootstrap"]
    depends_on:
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
    environment:
      DB_CONNECTION_URI: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required for Infisical DB}@postgres:5432/infisical_db
      # Redis URL for Infisical's cache backend (Valkey is Redis-compatible)
      # NOTE: Uses VALKEY_PASSWORD variable interpolation so changing
      # VALKEY_PASSWORD automatically updates this URL.
      # Requires Docker Compose v2.20+ for nested variable expansion.
      REDIS_URL: ${INFISICAL_REDIS_URL:-redis://:${VALKEY_PASSWORD:-valkey-dev-password}@valkey:6379}
      # SECURITY: INFISICAL_ENCRYPTION_KEY and INFISICAL_AUTH_SECRET must be set
      # in .env (or environment). There are no inline defaults -- the service will
      # refuse to start without them to prevent running with known-insecure keys.
      # Generate with: openssl rand -hex 16 (encryption) / openssl rand -hex 32 (auth)
      # Development defaults are provided in docker/.env.example.
      ENCRYPTION_KEY: ${INFISICAL_ENCRYPTION_KEY:?INFISICAL_ENCRYPTION_KEY must be set in .env}
      AUTH_SECRET: ${INFISICAL_AUTH_SECRET:?INFISICAL_AUTH_SECRET must be set in .env}
      SITE_URL: ${INFISICAL_SITE_URL:-http://localhost:8880}
      TELEMETRY_ENABLED: "false"
    ports:
      - "${INFISICAL_EXTERNAL_PORT:-8880}:8080"
    networks:
      - omnibase-infra-network
    healthcheck:
      !!merge <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/api/status"]
      start_period: 60s
    logging: *logging-defaults
    restart: unless-stopped
    labels:
      - "com.omninode.service=infisical"
      - "com.omninode.layer=infrastructure"
  # ==========================================================================
  # ONEX Runtime - Main Kernel (Runtime Profile)
  # ==========================================================================
  # Primary runtime service that handles the kernel bootstrap process.
  omninode-runtime:
    !!merge <<: *runtime-base
    container_name: omninode-runtime
    profiles: ["runtime", "full"]
    depends_on:
      postgres:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    environment:
      !!merge <<: *runtime-env
      OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME:-omninode-runtime-main}
      RUNTIME_PROFILE: main
      ONEX_INPUT_TOPIC: ${ONEX_INPUT_TOPIC:-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_OUTPUT_TOPIC:-responses}
      ONEX_GROUP_ID: ${ONEX_GROUP_ID:-onex-runtime-main}
      # OMN-2342: Gate intelligence introspection/heartbeat publishing to this
      # container only. Workers and effects containers inherit x-runtime-env but
      # do NOT set this flag, so they process intelligence events without
      # starting duplicate heartbeat loops for the same deterministic node IDs.
      OMNIINTELLIGENCE_PUBLISH_INTROSPECTION: "true"
    ports:
      - "${RUNTIME_MAIN_PORT:-8085}:8085"
    stop_grace_period: 90s
    volumes:
      - runtime_logs:/app/logs
      - runtime_data:/app/data
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "com.omninode.service=runtime-main"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # ONEX Runtime - Effects Handler (Runtime Profile)
  # ==========================================================================
  # Handles effect nodes for external service interactions.
  runtime-effects:
    !!merge <<: *runtime-base
    container_name: omninode-runtime-effects
    profiles: ["runtime", "full"]
    depends_on:
      omninode-runtime:
        condition: service_healthy
    environment:
      !!merge <<: *runtime-env
      OTEL_SERVICE_NAME: omninode-runtime-effects
      RUNTIME_PROFILE: effects
      ONEX_INPUT_TOPIC: ${ONEX_EFFECTS_INPUT_TOPIC:-effect-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_EFFECTS_OUTPUT_TOPIC:-effect-responses}
      ONEX_GROUP_ID: ${ONEX_EFFECTS_GROUP_ID:-onex-runtime-effects}
    ports:
      - "${RUNTIME_EFFECTS_PORT:-8086}:8085"
    stop_grace_period: 90s
    volumes:
      - effects_logs:/app/logs
      - effects_data:/app/data
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "com.omninode.service=runtime-effects"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # ONEX Runtime - Workers (Runtime Profile)
  # ==========================================================================
  # Scalable worker containers for parallel compute operations.
  runtime-worker:
    !!merge <<: *runtime-base
    profiles: ["runtime", "full"]
    depends_on:
      omninode-runtime:
        condition: service_healthy
    environment:
      !!merge <<: *runtime-env
      OTEL_SERVICE_NAME: omninode-runtime-worker
      RUNTIME_PROFILE: workers
      ONEX_INPUT_TOPIC: ${ONEX_WORKER_INPUT_TOPIC:-worker-requests}
      ONEX_OUTPUT_TOPIC: ${ONEX_WORKER_OUTPUT_TOPIC:-worker-responses}
      ONEX_GROUP_ID: ${ONEX_WORKER_GROUP_ID:-onex-runtime-workers}
    stop_grace_period: 90s
    volumes:
      - worker_logs:/app/logs
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      mode: replicated
      replicas: ${WORKER_REPLICAS:-2}
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=runtime-worker"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Agent Actions Consumer - Observability (Runtime Profile)
  # ==========================================================================
  # Consumes agent action events from Kafka and persists to PostgreSQL.
  agent-actions-consumer:
    !!merge <<: *runtime-base
    container_name: omninode-agent-actions-consumer
    profiles: ["runtime", "full"]
    depends_on:
      postgres:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    command: ["python", "-m", "omnibase_infra.services.observability.agent_actions.consumer"]
    environment:
      OMNIBASE_INFRA_AGENT_ACTIONS_KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-redpanda:9092}
      # NOTE: Uses dedicated OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN rather than
      # OMNIBASE_INFRA_DB_URL because this consumer uses pydantic-settings with the
      # OMNIBASE_INFRA_AGENT_ACTIONS_ prefix and may use a different DB role.
      # See top-of-file note re: nested variable expansion.
      # RECOMMENDED: Set OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN explicitly in .env instead.
      OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN: ${OMNIBASE_INFRA_AGENT_ACTIONS_POSTGRES_DSN:-postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required for agent-actions DSN}@postgres:5432/omnibase_infra}
      OMNIBASE_INFRA_AGENT_ACTIONS_BATCH_SIZE: ${AGENT_ACTIONS_BATCH_SIZE:-100}
      OMNIBASE_INFRA_AGENT_ACTIONS_BATCH_TIMEOUT_MS: ${AGENT_ACTIONS_BATCH_TIMEOUT_MS:-1000}
      OMNIBASE_INFRA_AGENT_ACTIONS_HEALTH_CHECK_PORT: "8087"
      ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
      ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
      OMNIMEMORY_ENABLED: ${OMNIMEMORY_ENABLED:-false}
    ports:
      - "${AGENT_ACTIONS_CONSUMER_PORT:-8087}:8087"
    stop_grace_period: 60s
    volumes:
      - agent_actions_logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8087/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=agent-actions-consumer"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Skill Lifecycle Consumer - Observability (Runtime Profile) [OMN-2934]
  # ==========================================================================
  # Consumes skill-started and skill-completed events from Kafka and persists
  # them to the skill_executions PostgreSQL table for omnidash monitoring.
  skill-lifecycle-consumer:
    !!merge <<: *runtime-base
    container_name: omninode-skill-lifecycle-consumer
    profiles: ["runtime", "full"]
    depends_on:
      postgres:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    command: ["python", "-m", "omnibase_infra.services.observability.skill_lifecycle.consumer"]
    environment:
      OMNIBASE_INFRA_SKILL_LIFECYCLE_KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-redpanda:9092}
      OMNIBASE_INFRA_SKILL_LIFECYCLE_POSTGRES_DSN: ${OMNIBASE_INFRA_SKILL_LIFECYCLE_POSTGRES_DSN:-postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required for skill-lifecycle DSN}@postgres:5432/omnibase_infra}
      OMNIBASE_INFRA_SKILL_LIFECYCLE_BATCH_SIZE: ${SKILL_LIFECYCLE_BATCH_SIZE:-100}
      OMNIBASE_INFRA_SKILL_LIFECYCLE_BATCH_TIMEOUT_MS: ${SKILL_LIFECYCLE_BATCH_TIMEOUT_MS:-1000}
      OMNIBASE_INFRA_SKILL_LIFECYCLE_HEALTH_CHECK_PORT: "8092"
      OMNIBASE_INFRA_SKILL_LIFECYCLE_HEALTH_CHECK_HOST: "0.0.0.0"
      ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
      ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
    ports:
      - "${SKILL_LIFECYCLE_CONSUMER_PORT:-8092}:8092"
    stop_grace_period: 60s
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8092/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M
    labels:
      - "com.omninode.service=skill-lifecycle-consumer"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # OmniIntelligence HTTP API - Pattern Query Service (Runtime Profile)
  # ==========================================================================
  # Serves GET /api/v1/patterns for runtime enforcement nodes.
  # Shares Dockerfile.runtime (omniintelligence==0.5.0 is installed there).
  intelligence-api:
    !!merge <<: *runtime-base
    container_name: omnibase-intelligence-api
    profiles: ["runtime", "full"]
    depends_on:
      postgres:
        condition: service_healthy
    command:
      - uvicorn
      - omniintelligence.api.app:create_app
      - --factory
      - --host
      - "0.0.0.0"
      - --port
      - "8053"
      - --workers
      - "1"
    environment:
      # DatabaseSettings reads POSTGRES_* env vars (env_prefix="POSTGRES_")
      # Must point to the omniintelligence database, not omnibase_infra.
      POSTGRES_HOST: postgres
      POSTGRES_PORT: "5432"
      POSTGRES_DATABASE: omniintelligence
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required}
      ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
      ONEX_ENVIRONMENT: ${ONEX_ENVIRONMENT:-development}
    ports:
      - "${INTELLIGENCE_API_EXTERNAL_PORT:-8053}:8053"
    stop_grace_period: 30s
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8053/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=intelligence-api"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
  # ==========================================================================
  # Contract Resolver Bridge - Synchronous HTTP for Dashboard (Runtime Profile)
  # ==========================================================================
  # Transitional HTTP bridge (OMN-2756) that exposes NodeContractResolveCompute
  # via POST /api/nodes/contract.resolve so the dashboard can resolve contracts
  # without a Kafka round-trip. Node-shaped: same contract, same events, same
  # registry entry as the production node. Only the execution path changes when
  # the real ONEX node runner supports HTTP invocation.
  omninode-contract-resolver:
    !!merge <<: *runtime-base
    container_name: omninode-contract-resolver
    profiles: ["runtime", "full"]
    depends_on:
      omninode-runtime:
        condition: service_healthy
    command:
      - uvicorn
      - omnibase_infra.services.contract_resolver.launcher:app
      - --host
      - "0.0.0.0"
      - --port
      - "8091"
      - --workers
      - "1"
    environment:
      !!merge <<: *runtime-env
      OTEL_SERVICE_NAME: omninode-contract-resolver
      CORS_ORIGINS: ${CONTRACT_RESOLVER_CORS_ORIGINS:-http://localhost:3000,http://localhost:3001,http://localhost:8085}
      CONTRACT_RESOLVER_PORT: "8091"
      ONEX_LOG_LEVEL: ${ONEX_LOG_LEVEL:-INFO}
    ports:
      - "${CONTRACT_RESOLVER_PORT:-8091}:8091"
    stop_grace_period: 30s
    volumes:
      - contract_resolver_logs:/app/logs
      - ../contracts:/app/contracts:ro
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8091/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "com.omninode.service=contract-resolver"
      - "com.omninode.layer=runtime"
      - "com.omninode.version=${RUNTIME_VERSION:-0.1.0}"
      - "com.omninode.ticket=OMN-2756"
      - "com.omninode.transitional=true"
# ==========================================================================
# Network Configuration
# ==========================================================================
networks:
  omnibase-infra-network:
    name: omnibase-infra-network
    driver: bridge
# ==========================================================================
# Volume Configuration
# ==========================================================================
volumes:
  # Infrastructure volumes
  postgres_data:
    name: omnibase-infra-postgres-data
  redpanda_data:
    name: omnibase-infra-redpanda-data
  valkey_data:
    name: omnibase-infra-valkey-data
  consul_data:
    name: omnibase-infra-consul-data
  # Runtime volumes
  runtime_logs:
    name: omninode-runtime-logs
  runtime_data:
    name: omninode-runtime-data
  effects_logs:
    name: omninode-effects-logs
  effects_data:
    name: omninode-effects-data
  worker_logs:
    name: omninode-worker-logs
  agent_actions_logs:
    name: omninode-agent-actions-logs
  contract_resolver_logs:
    name: omninode-contract-resolver-logs
