"""
High-performance PostgreSQL client for OmniNode Bridge.

This module provides enterprise-grade PostgreSQL connectivity with advanced features:

üîí Security Features:
- SSL/TLS encryption with certificate validation
- Connection security validation based on environment
- Prepared statement caching to prevent SQL injection
- Security audit logging for compliance

‚ö° Performance Features:
- Optimized connection pooling with leak detection
- Prepared statement caching for frequently used queries
- Batch operation support with executemany optimization
- Ultra-high-performance COPY operations for bulk data
- Adaptive bulk insert methods

üõ°Ô∏è Resilience Features:
- Comprehensive error handling with specific exception types
- Automatic retry logic with exponential backoff
- Circuit breaker pattern integration
- Graceful degradation capabilities
- Resource leak detection and prevention

Example Usage:
    Basic usage with automatic configuration:
        client = PostgresClient()
        await client.connect()
        result = await client.fetch_all("SELECT * FROM users")

    High-performance batch operations:
        operations = [BatchOperation(...) for ...]
        results = await client.execute_batch(operations)

    Ultra-fast bulk insert:
        await client.copy_bulk_insert("table", columns, data)
"""

import asyncio
import logging
import os
import ssl
import time
from collections import defaultdict
from dataclasses import dataclass
from typing import Any
from urllib.parse import quote_plus
from uuid import UUID
from weakref import WeakSet

import asyncpg
from asyncpg import Pool
from asyncpg.exceptions import (
    AdminShutdownError,
    CannotConnectNowError,
    ConnectionFailureError,
    CrashShutdownError,
    DiskFullError,
    InsufficientPrivilegeError,
    InterfaceError,
    InvalidAuthorizationSpecificationError,
    InvalidCatalogNameError,
    InvalidPasswordError,
    InvalidSchemaNameError,
    OutOfMemoryError,
    PostgresError,
    TooManyConnectionsError,
    UndefinedTableError,
)
from asyncpg.prepared_stmt import PreparedStatement
from pydantic import BaseModel, Field, field_validator

from ..constants import DatabaseDefaults
from ..models.hooks import HookEvent
from ..utils.circuit_breaker_config import DATABASE_CIRCUIT_BREAKER

logger = logging.getLogger(__name__)


@dataclass
class BatchOperation:
    """Represents a batch database operation."""

    query: str
    params: tuple[Any, ...]
    operation_type: str  # 'execute', 'fetch', 'fetchrow'


@dataclass
class ConnectionMetrics:
    """Connection pool performance metrics."""

    acquired_connections: int = 0
    released_connections: int = 0
    failed_acquisitions: int = 0
    query_count: int = 0
    failed_queries: int = 0
    query_time_total: float = 0.0
    last_health_check: float = 0.0

    @property
    def avg_query_time_ms(self) -> float:
        """Calculate average query time in milliseconds."""
        if self.query_count == 0:
            return 0.0
        return (self.query_time_total / self.query_count) * 1000


@dataclass
class HealthCheckResult:
    """Health check result structure."""

    status: str
    timestamp: float
    connection_pool: dict[str, int | float] | None = None
    query_test: dict[str, str | bool] | None = None
    error: str | None = None
    details: dict[str, str | int | float] | None = None


@dataclass
class CleanupResult:
    """Data cleanup operation result."""

    total_deleted: int
    hook_events_deleted: int = 0
    event_metrics_deleted: int = 0
    sessions_deleted: int = 0
    connection_metrics_deleted: int = 0
    audit_log_deleted: int = 0
    operation_time_ms: float = 0.0


@dataclass
class PoolMetrics:
    """Connection pool metrics structure."""

    current_size: int
    max_size: int
    utilization_percent: float
    connection_lifecycle: dict[str, int]
    performance_metrics: dict[str, float]
    health_status: dict[str, str | int]


@dataclass
class PerformanceMetrics:
    """Performance metrics structure."""

    cache_hit_rate: float
    avg_query_time_ms: float
    connection_metrics: ConnectionMetrics
    pool_efficiency: dict[str, float]
    recent_errors: list[str]


@dataclass
class ServiceSession:
    """Service session data structure."""

    id: str
    service_name: str
    instance_id: str | None = None
    session_start: str | None = None
    session_end: str | None = None
    status: str = "active"
    metadata: dict[str, str | int | bool] | None = None


class HookEventData(BaseModel):
    """Hook event data structure with validation for external input."""

    id: str = Field(
        ..., min_length=1, max_length=100, description="Unique hook event identifier"
    )
    source: str = Field(
        ..., min_length=1, max_length=50, description="Event source system"
    )
    action: str = Field(
        ..., min_length=1, max_length=50, description="Action that triggered the event"
    )
    resource: str = Field(..., min_length=1, max_length=50, description="Resource type")
    resource_id: str = Field(
        ..., min_length=1, max_length=100, description="Resource identifier"
    )
    payload: dict[str, str | int | bool | list | dict] | None = Field(
        None, description="Event payload data"
    )
    metadata: dict[str, str | int | bool] | None = Field(
        None, description="Event metadata"
    )
    processed: bool = Field(False, description="Processing status")
    processing_errors: list[str] | None = Field(
        None, description="Processing error messages"
    )
    retry_count: int = Field(0, ge=0, le=10, description="Number of retry attempts")

    @field_validator("payload")
    @classmethod
    def validate_payload_size(cls, v: dict | None) -> dict | None:
        """Validate payload size to prevent memory issues."""
        if v is not None:
            # Rough estimate: limit payload to ~50KB when serialized
            import json

            try:
                payload_str = json.dumps(v)
                if len(payload_str) > 51200:  # 50KB
                    raise ValueError("Payload size exceeds 50KB limit")
            except (TypeError, ValueError) as e:
                if "exceeds 50KB limit" in str(e):
                    raise
                # If JSON serialization fails, it's invalid payload data
                raise ValueError("Payload contains non-serializable data")
        return v

    @field_validator("processing_errors")
    @classmethod
    def validate_processing_errors(cls, v: list[str] | None) -> list[str] | None:
        """Validate processing errors list."""
        if v is not None:
            if len(v) > 50:  # Limit number of errors
                raise ValueError("Too many processing errors (max 50)")
            for error in v:
                if not isinstance(error, str):
                    raise ValueError("All processing errors must be strings")
                if len(error) > 1000:  # Limit error message length
                    raise ValueError(
                        "Processing error message too long (max 1000 characters)"
                    )
        return v


@dataclass
class SSLValidationResult:
    """SSL connection validation result."""

    ssl_enabled: bool
    ssl_version: str | None = None
    cipher_suite: str | None = None
    certificate_info: dict[str, str] | None = None
    validation_errors: list[str] | None = None


class PreparedStatementCache:
    """Cache for prepared statements to improve performance."""

    def __init__(self, max_size: int = DatabaseDefaults.PREPARED_STATEMENT_CACHE_SIZE):
        self.max_size = max_size
        self._cache: dict[str, PreparedStatement] = {}
        self._usage_count: dict[str, int] = defaultdict(int)

    def get(self, query: str) -> PreparedStatement | None:
        """Get prepared statement from cache."""
        if query in self._cache:
            self._usage_count[query] += 1
            return self._cache[query]
        return None

    def put(self, query: str, statement: PreparedStatement) -> None:
        """Add prepared statement to cache."""
        if len(self._cache) >= self.max_size:
            # Remove least used statement
            least_used = min(self._usage_count.items(), key=lambda x: x[1])[0]
            del self._cache[least_used]
            del self._usage_count[least_used]

        self._cache[query] = statement
        self._usage_count[query] = 1

    def clear(self) -> None:
        """Clear the cache."""
        self._cache.clear()
        self._usage_count.clear()


class PerformanceOptimizedConnection:
    """Wrapper for asyncpg connection with performance optimizations."""

    def __init__(self, conn: asyncpg.Connection):
        self.conn = conn
        self.prepared_cache = PreparedStatementCache()
        self.query_count = 0
        self.created_at = time.time()

    async def prepare_cached(self, query: str) -> PreparedStatement:
        """Get or create cached prepared statement."""
        cached = self.prepared_cache.get(query)
        if cached is not None:
            return cached

        statement = await self.conn.prepare(query)
        self.prepared_cache.put(query, statement)
        return statement

    async def execute_prepared(self, query: str, *args) -> str:
        """Execute query using prepared statement."""
        statement = await self.prepare_cached(query)
        self.query_count += 1
        return (
            await statement.fetch(*args)
            if "SELECT" in query.upper()
            else await statement.fetchval(*args)
        )

    def __getattr__(self, name: str) -> Any:
        """Delegate other attributes to the wrapped connection."""
        return getattr(self.conn, name)


class PostgresClient:
    """
    High-performance async PostgreSQL client with enterprise features.

    Provides secure, resilient, and optimized database connectivity for OmniNode Bridge.
    Automatically configures based on environment variables with sensible defaults.

    Key Features:
        üîí Security:
        - SSL/TLS encryption with certificate validation
        - Environment-based security policy enforcement
        - SQL injection prevention via prepared statements
        - Comprehensive audit logging

        ‚ö° Performance:
        - Optimized connection pooling (5-50 connections)
        - Prepared statement caching (LRU with size limits)
        - Batch operations with executemany optimization
        - Ultra-fast COPY operations for bulk data (1M+ rows/sec)
        - Connection warmup for reduced cold-start latency

        üõ°Ô∏è Resilience:
        - Automatic retry with exponential backoff
        - Circuit breaker integration for fault tolerance
        - Connection leak detection and prevention
        - Graceful degradation on resource constraints
        - Comprehensive error classification and handling

    Environment Configuration:
        Automatically loads configuration from environment variables:
        - POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE
        - POSTGRES_USER, POSTGRES_PASSWORD
        - POSTGRES_SSL_ENABLED, POSTGRES_SSL_MODE
        - POSTGRES_POOL_MIN_SIZE, POSTGRES_POOL_MAX_SIZE
        - See environment_config.DatabaseConfig for full list

    Example Usage:
        Basic usage (recommended):
            client = PostgresClient()
            await client.connect()
            result = await client.fetch_all("SELECT * FROM users WHERE active = $1", True)

        High-performance batch operations:
            operations = [
                BatchOperation("INSERT INTO events (id, data) VALUES ($1, $2)",
                             (event_id, data), "execute")
                for event_id, data in events
            ]
            results = await client.execute_batch(operations)

        Ultra-fast bulk loading:
            columns = ["id", "name", "email", "created_at"]
            data = [(1, "John", "john@example.com", datetime.now()), ...]
            success = await client.copy_bulk_insert("users", columns, data)

    Performance Characteristics:
        - Connection establishment: <100ms with warmup
        - Query execution: <10ms for simple queries with prepared statements
        - Batch operations: 1000+ operations/second
        - Bulk inserts: 100,000+ rows/second via COPY
        - Memory usage: ~1MB per connection + query cache

    Thread Safety:
        This client is designed for asyncio and is not thread-safe.
        Use separate instances for different threads.
    """

    def __init__(
        self,
        host: str = None,
        port: int = None,
        database: str = None,
        user: str = None,
        password: str = None,
        min_size: int = None,  # Will be environment-configurable
        max_size: int = None,  # Will be environment-configurable
        ssl_enabled: bool = None,
        ssl_cert_path: str = None,
        ssl_key_path: str = None,
        ssl_ca_path: str = None,
        # Performance optimization parameters
        max_queries_per_connection: int = None,
        connection_max_age_seconds: int = None,
        query_timeout_seconds: int = None,
        acquire_timeout_seconds: int = None,
    ):
        """
        Initialize PostgreSQL client with automatic environment configuration.

        All parameters are optional and will fall back to environment variables
        configured via DatabaseConfig. This allows for flexible deployment
        without changing code.

        Args:
            host: PostgreSQL host. Defaults to POSTGRES_HOST env var or 'localhost'
            port: PostgreSQL port. Defaults to POSTGRES_PORT env var or 5436
            database: Database name. Defaults to POSTGRES_DATABASE env var or 'omninode_bridge'
            user: Database user. Defaults to POSTGRES_USER env var or 'postgres'
            password: Database password. Required via POSTGRES_PASSWORD env var

            min_size: Minimum connection pool size. Defaults to environment-specific:
                      - Development: 2, Staging: 5, Production: 10
            max_size: Maximum connection pool size. Defaults to environment-specific:
                      - Development: 10, Staging: 25, Production: 50

            ssl_enabled: Enable SSL/TLS connection. Defaults to environment-specific:
                        - Development: False, Staging: True, Production: True
            ssl_cert_path: Path to SSL client certificate for mutual TLS
            ssl_key_path: Path to SSL client private key for mutual TLS
            ssl_ca_path: Path to SSL CA certificate for server verification

            max_queries_per_connection: Rotate connection after X queries (default: 50,000)
            connection_max_age_seconds: Rotate connection after X seconds (default: 3600)
            query_timeout_seconds: Query timeout in seconds (default: 60)
            acquire_timeout_seconds: Connection acquisition timeout (default: 10)

        Raises:
            ValueError: If required password is not provided
            ValueError: If SSL configuration is invalid
            ValueError: If production security requirements are not met

        Environment Variables:
            See environment_config.DatabaseConfig for complete list of
            configurable environment variables.

        Example:
            # Automatic configuration (recommended)
            client = PostgresClient()

            # Override specific settings
            client = PostgresClient(
                host="custom-host",
                ssl_enabled=False,  # Development only
                max_size=25         # Custom pool size
            )
        """
        # Load configuration from environment-based config system
        from omninode_bridge.config.environment_config import DatabaseConfig

        db_config = DatabaseConfig()

        # Use provided parameters or fall back to configuration
        self.host = host or db_config.host
        self.port = port or db_config.port
        self.database = database or db_config.database
        self.user = user or db_config.user
        self.password = password or db_config.password

        # SSL/TLS configuration
        self.ssl_enabled = (
            ssl_enabled if ssl_enabled is not None else db_config.ssl_enabled
        )
        self.ssl_cert_path = ssl_cert_path or db_config.ssl_cert_path
        self.ssl_key_path = ssl_key_path or db_config.ssl_key_path
        self.ssl_ca_path = ssl_ca_path or db_config.ssl_ca_path
        self.ssl_check_hostname = db_config.ssl_check_hostname

        # Performance optimization configuration from centralized config
        self.min_size = min_size if min_size is not None else db_config.pool_min_size
        self.max_size = max_size if max_size is not None else db_config.pool_max_size

        # Performance tuning parameters
        self.max_queries_per_connection = (
            max_queries_per_connection
            if max_queries_per_connection is not None
            else db_config.max_queries_per_connection
        )
        self.connection_max_age_seconds = (
            connection_max_age_seconds
            if connection_max_age_seconds is not None
            else db_config.connection_max_age_seconds
        )
        self.query_timeout_seconds = (
            query_timeout_seconds
            if query_timeout_seconds is not None
            else db_config.query_timeout_seconds
        )
        self.acquire_timeout_seconds = (
            acquire_timeout_seconds
            if acquire_timeout_seconds is not None
            else db_config.acquire_timeout_seconds
        )

        # Connection pool monitoring and alerting
        self.pool_exhaustion_threshold = db_config.pool_exhaustion_threshold
        self.connection_leak_detection_enabled = db_config.leak_detection
        self._pool_stats = {
            "acquired_count": 0,
            "released_count": 0,
            "exhaustion_alerts": 0,
        }

        # Performance optimization features
        self._connection_metrics = ConnectionMetrics()
        self._active_connections: WeakSet = WeakSet()
        self._batch_operations_buffer: list[BatchOperation] = []
        self._batch_size_limit = (
            DatabaseDefaults.BATCH_SIZE_LIMIT
        )  # Maximum operations per batch
        self._batch_timeout_ms = (
            DatabaseDefaults.BATCH_TIMEOUT_MS
        )  # Maximum time to wait for batch

        # NOTE: Global prepared statement caching is DISABLED for connection pool compatibility.
        # Prepared statements in asyncpg are connection-specific and cannot be reused across
        # different connections from the pool. asyncpg handles per-connection statement caching
        # internally, which is safe and efficient for connection pools.
        self._prepared_statement_cache = PreparedStatementCache(
            max_size=DatabaseDefaults.MAX_PREPARED_STATEMENT_CACHE_SIZE
        )  # Kept for metrics compatibility, but not used for actual caching
        self._warmup_completed = False

        # Connection pool optimization settings
        # Prepared statements are handled per-connection by asyncpg internally
        self._enable_prepared_statements = (
            True  # Controls whether to use parameterized queries
        )
        self._enable_batch_operations = True
        self._enable_connection_warmup = True

        # Create DSN for connections (contains password) - properly encoded for special characters
        encoded_password = quote_plus(self.password)
        self.dsn = f"postgresql://{self.user}:{encoded_password}@{self.host}:{self.port}/{self.database}"

        # Create safe DSN for logging (password masked)
        self.safe_dsn = (
            f"postgresql://{self.user}:***@{self.host}:{self.port}/{self.database}"
        )
        self.pool: Pool | None = None
        self._connected = False
        self._ssl_context = self._create_ssl_context() if self.ssl_enabled else None
        self._connection_lock: asyncio.Lock = asyncio.Lock()

        # Validate SSL/TLS security requirements based on environment
        self._validate_security_requirements(db_config)

    def _create_ssl_context(self) -> ssl.SSLContext:
        """Create SSL context for secure PostgreSQL connections with comprehensive security."""
        try:
            context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)

            # Enhanced security settings
            context.minimum_version = ssl.TLSVersion.TLSv1_2
            context.maximum_version = ssl.TLSVersion.TLSv1_3
            context.set_ciphers(
                "ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!MD5:!DSS",
            )

            # Configure certificate verification
            if self.ssl_ca_path:
                if not os.path.exists(self.ssl_ca_path):
                    raise FileNotFoundError(
                        f"SSL CA certificate not found: {self.ssl_ca_path}",
                    )
                context.load_verify_locations(cafile=self.ssl_ca_path)
                context.verify_mode = ssl.CERT_REQUIRED
                logger.info(f"SSL CA certificate loaded from: {self.ssl_ca_path}")
            else:
                # Use system CA bundle for verification
                context.verify_mode = ssl.CERT_REQUIRED
                logger.info("Using system CA bundle for SSL verification")

            # Load client certificate if provided
            if self.ssl_cert_path and self.ssl_key_path:
                if not os.path.exists(self.ssl_cert_path):
                    raise FileNotFoundError(
                        f"SSL client certificate not found: {self.ssl_cert_path}",
                    )
                if not os.path.exists(self.ssl_key_path):
                    raise FileNotFoundError(
                        f"SSL client key not found: {self.ssl_key_path}",
                    )

                context.load_cert_chain(
                    certfile=self.ssl_cert_path,
                    keyfile=self.ssl_key_path,
                )
                logger.info(f"SSL client certificate loaded from: {self.ssl_cert_path}")

            # Hostname verification configuration
            check_hostname = self.ssl_check_hostname
            context.check_hostname = check_hostname

            if not check_hostname:
                logger.warning(
                    "SSL hostname verification is disabled - only use in development!",
                )

            # Additional security options
            context.options |= ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3 | ssl.OP_NO_COMPRESSION
            context.options |= ssl.OP_CIPHER_SERVER_PREFERENCE
            context.options |= ssl.OP_SINGLE_DH_USE | ssl.OP_SINGLE_ECDH_USE

            logger.info("Enhanced SSL context created for PostgreSQL connection")
            logger.info(f"SSL verification mode: {context.verify_mode.name}")
            logger.info(f"SSL hostname check: {context.check_hostname}")

            return context

        except Exception as e:
            logger.error(f"Failed to create SSL context: {e}")
            raise ValueError(f"SSL configuration error: {e}") from e

    def _validate_security_requirements(self, db_config) -> None:
        """Validate security requirements based on environment and compliance policies.

        Args:
            db_config: Database configuration dictionary

        Raises:
            ValueError: If security requirements are not met
        """
        try:
            environment = os.getenv("ENVIRONMENT", "development").lower()

            # Security Configuration Integration:
            # Future enhancement to integrate with SecureConfig service for:
            # - Centralized compliance policy management
            # - Dynamic security requirement enforcement
            # - Audit trail for security configuration changes
            # - Integration with omnibase_security patterns

            # Enforce SSL/TLS in production and staging
            if environment in ["production", "staging"]:
                if not self.ssl_enabled:
                    raise ValueError(
                        f"SSL/TLS is required in {environment} environment but is disabled. "
                        "Enable SSL with POSTGRES_SSL_ENABLED=true"
                    )

                if not self._ssl_context:
                    raise ValueError(
                        f"SSL context could not be created in {environment} environment. "
                        "Check SSL certificate configuration."
                    )

                logger.info(f"SSL/TLS validation passed for {environment} environment")

            # Note: Compliance policies validation removed for environment-based config
            # Basic SSL validation is performed above for production/staging environments

            # Validate hostname verification in production
            if environment == "production" and self.ssl_enabled:
                if not self.ssl_check_hostname:
                    logger.warning(
                        "SSL hostname verification is disabled in production - this may be a security risk"
                    )

            # Log security status
            logger.info(
                f"Database security validation completed for {environment} environment"
            )
            logger.info(f"SSL/TLS enabled: {self.ssl_enabled}")
            logger.info(f"SSL hostname verification: {self.ssl_check_hostname}")
            logger.info("Environment-based configuration validation completed")

        except Exception as e:
            logger.error(f"Security validation failed: {e}")
            raise ValueError(f"Database security requirements not met: {e}") from e

    @DATABASE_CIRCUIT_BREAKER()
    async def connect(
        self,
        max_retries: int = DatabaseDefaults.HEALTH_CHECK_RETRY_ATTEMPTS,
        base_delay: float = DatabaseDefaults.RETRY_BASE_DELAY,
    ) -> None:
        """
        Connect to PostgreSQL and create optimized connection pool with comprehensive retry logic.

        This method establishes a secure, high-performance connection pool with:
        - SSL/TLS encryption (if configured)
        - Connection warmup for reduced latency
        - Comprehensive error handling and classification
        - Automatic retry with exponential backoff
        - Database schema initialization
        - Connection pool health validation

        Args:
            max_retries: Maximum connection attempts before failing (default: 5)
                        Reasonable for handling transient network issues
            base_delay: Base delay in seconds for exponential backoff (default: 2.0)
                       Delay grows as: base_delay * (2 ** attempt)

        Raises:
            ConnectionFailureError: Network connectivity issues
            TooManyConnectionsError: Database connection limit exceeded
            InvalidPasswordError: Authentication failed
            InvalidAuthorizationSpecificationError: Authorization failed
            InvalidCatalogNameError: Database doesn't exist
            TimeoutError: Connection establishment timed out
            ValueError: SSL configuration errors

        Connection Process:
            1. Validates SSL/TLS configuration
            2. Creates asyncpg connection pool with optimizations
            3. Tests connection with health check query
            4. Initializes database schema (creates tables if needed)
            5. Warms up connection pool for optimal performance
            6. Validates final pool state

        Performance Notes:
            - Initial connection: ~100-500ms depending on SSL setup
            - Subsequent connections from pool: <10ms
            - Pool warmup creates min_size connections proactively
            - SSL handshake adds ~50-100ms to initial connection

        Example:
            # Basic connection (recommended)
            await client.connect()

            # Custom retry behavior for unreliable networks
            await client.connect(max_retries=10, base_delay=5.0)
        """
        # Use connection lock to prevent race conditions in concurrent connection attempts
        async with self._connection_lock:
            # Prevent multiple concurrent connection attempts
            if self.pool and self._connected:
                logger.info("PostgreSQL client already connected")
                return

            # Clean up any existing pool before creating new one
            if self.pool:
                try:
                    await self.disconnect()
                except Exception as e:
                    logger.warning(f"Error cleaning up existing pool: {e}")

            last_exception = None
            temp_pool = None

            for attempt in range(max_retries):
                try:
                    logger.info(
                        f"Attempting to connect to PostgreSQL (attempt {attempt + 1}/{max_retries})",
                    )

                    # Configure connection parameters with enhanced security and performance
                    connection_kwargs = {
                        "min_size": self.min_size,
                        "max_size": self.max_size,
                        "command_timeout": self.query_timeout_seconds,
                        # Note: asyncpg doesn't support connection_timeout parameter
                        # Connection acquisition timeout is handled at pool.acquire() level
                        "server_settings": {
                            "application_name": "omninode_bridge_hook_receiver",
                            "statement_timeout": f"{self.query_timeout_seconds}s",
                            "idle_in_transaction_session_timeout": "10min",
                            # Note: server-level parameters like shared_preload_libraries,
                            # log_statement, and log_min_duration_statement cannot be set
                            # at connection level and must be configured in postgresql.conf
                        },
                        # Enhanced connection pool settings for better resource management
                        "max_inactive_connection_lifetime": self.connection_max_age_seconds,
                        "max_queries": self.max_queries_per_connection,
                        "max_cached_statement_lifetime": DatabaseDefaults.MAX_CACHED_STATEMENT_LIFETIME,  # 5 minutes
                        "setup": self._setup_connection,  # Per-connection setup
                    }

                    # Add SSL configuration if enabled
                    if self._ssl_context:
                        connection_kwargs["ssl"] = self._ssl_context
                        logger.info("SSL/TLS enabled for PostgreSQL connection")

                    # Create pool with proper error handling
                    temp_pool = await asyncpg.create_pool(self.dsn, **connection_kwargs)

                    # Test the connection with timeout and proper cleanup
                    try:
                        async with asyncio.timeout(
                            DatabaseDefaults.CONNECTION_TEST_TIMEOUT,
                        ):  # Connection test timeout
                            async with temp_pool.acquire() as conn:
                                result = await conn.fetchval("SELECT 1")
                                if result != 1:
                                    raise RuntimeError(
                                        "Connection test failed - unexpected result",
                                    )

                                # Test SSL connection if enabled
                                if self._ssl_context:
                                    ssl_info = await conn.fetchrow(
                                        "SELECT ssl_is_used() as ssl_used, ssl_version() as ssl_version, ssl_cipher() as ssl_cipher",
                                    )
                                    if ssl_info:
                                        logger.info(
                                            f"SSL connection verified: {ssl_info['ssl_version']}, cipher: {ssl_info['ssl_cipher']}",
                                        )
                                    else:
                                        logger.warning(
                                            "SSL was configured but connection is not using SSL",
                                        )

                    except TimeoutError:
                        if temp_pool:
                            await temp_pool.close()
                        raise RuntimeError(
                            f"Connection test timed out after {DatabaseDefaults.CONNECTION_TEST_TIMEOUT} seconds"
                        )

                    # Initialize database schema
                    await self._ensure_tables_with_pool(temp_pool)

                    # Perform connection pool warmup for better performance BEFORE setting connected=True
                    if self._enable_connection_warmup:
                        await self._warmup_connection_pool_with_pool(temp_pool)

                    # Validate pool is ready and all connections are healthy
                    try:
                        async with temp_pool.acquire() as conn:
                            await conn.fetchval("SELECT 1")
                    except Exception as e:
                        if temp_pool:
                            await temp_pool.close()
                        raise RuntimeError(f"Pool validation failed: {e}")

                    # CRITICAL: Only set pool and connected=True after ALL setup operations complete
                    self.pool = temp_pool
                    temp_pool = None  # Prevent cleanup in finally block

                    # Reset all resource tracking on successful connection
                    self._reset_resource_tracking()

                    # CRITICAL: Set connected=True only after all operations succeed
                    self._connected = True

                    logger.info(
                        f"Successfully connected to PostgreSQL with pool size {self.min_size}-{self.max_size}",
                    )
                    if self._ssl_context:
                        logger.info("SSL/TLS connection established and verified")
                    return

                except TooManyConnectionsError as e:
                    last_exception = e
                    logger.error(
                        f"Connection attempt {attempt + 1} failed - too many connections: {e}"
                    )
                    # Don't retry on too many connections - likely a resource issue
                    break

                except (
                    ConnectionFailureError,
                    CannotConnectNowError,
                    InterfaceError,
                ) as e:
                    last_exception = e
                    logger.warning(
                        f"Connection attempt {attempt + 1} failed - network/connection issue: {e}"
                    )

                except (
                    InvalidPasswordError,
                    InvalidAuthorizationSpecificationError,
                    InsufficientPrivilegeError,
                ) as e:
                    last_exception = e
                    logger.error(
                        f"Connection attempt {attempt + 1} failed - authentication/authorization error: {e}"
                    )
                    # Don't retry on auth failures - configuration issue
                    break

                except (
                    InvalidCatalogNameError,
                    InvalidSchemaNameError,
                    UndefinedTableError,
                ) as e:
                    last_exception = e
                    logger.error(
                        f"Connection attempt {attempt + 1} failed - database/schema not found: {e}"
                    )
                    # Don't retry on missing database/schema
                    break

                except (OutOfMemoryError, DiskFullError) as e:
                    last_exception = e
                    logger.critical(
                        f"Connection attempt {attempt + 1} failed - resource exhaustion: {e}"
                    )
                    # Don't retry on resource exhaustion
                    break

                except (AdminShutdownError, CrashShutdownError) as e:
                    last_exception = e
                    logger.warning(
                        f"Connection attempt {attempt + 1} failed - database shutdown: {e}"
                    )
                    # These might be temporary, allow retry

                except PostgresError as e:
                    last_exception = e
                    logger.warning(
                        f"Connection attempt {attempt + 1} failed - database error: {e}"
                    )

                except (TimeoutError, OSError) as e:
                    last_exception = e
                    logger.warning(
                        f"Connection attempt {attempt + 1} failed - network/timeout error: {e}"
                    )

                except Exception as e:
                    last_exception = e
                    logger.warning(
                        f"Connection attempt {attempt + 1} failed - unexpected error: {e}"
                    )

                # Clean up failed pool if it was created
                if temp_pool:
                    try:
                        await temp_pool.close()
                    except Exception as cleanup_error:
                        logger.error(f"Error cleaning up failed pool: {cleanup_error}")
                    finally:
                        temp_pool = None

                    if attempt < max_retries - 1:
                        # Calculate exponential backoff delay
                        delay = min(
                            base_delay
                            * (DatabaseDefaults.RETRY_BACKOFF_MULTIPLIER**attempt),
                            DatabaseDefaults.CONNECTION_TIMEOUT,
                        )  # Cap at connection timeout
                        logger.info(f"Retrying connection in {delay:.1f} seconds...")
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"All {max_retries} connection attempts failed")

            # Cleanup and set failure state
            self._connected = False
            if self.pool:
                try:
                    await self.pool.close()
                except (asyncpg.InterfaceError, asyncpg.InternalClientError) as e:
                    logger.warning(f"Error closing connection pool during cleanup: {e}")
                except Exception as e:
                    logger.error(
                        f"Unexpected error closing connection pool: {e}",
                        exc_info=True,
                    )
                finally:
                    self.pool = None

            if last_exception:
                raise last_exception
            raise Exception("Failed to connect to PostgreSQL after all retry attempts")

    async def _setup_connection(self, conn: asyncpg.Connection) -> None:
        """Setup each database connection with security and performance settings."""
        try:
            # Set connection-level security settings
            await conn.execute(
                f"SET statement_timeout = '{self.query_timeout_seconds}s'",
            )
            await conn.execute("SET idle_in_transaction_session_timeout = '10min'")
            await conn.execute("SET lock_timeout = '5s'")

            # Performance settings - dynamically adjust based on environment
            environment = os.getenv("ENVIRONMENT", "development").lower()
            if environment == "production":
                await conn.execute("SET work_mem = '8MB'")
                await conn.execute("SET maintenance_work_mem = '128MB'")
                await conn.execute("SET random_page_cost = 1.1")  # SSD optimization
                await conn.execute("SET effective_cache_size = '1GB'")
            elif environment == "staging":
                await conn.execute("SET work_mem = '6MB'")
                await conn.execute("SET maintenance_work_mem = '96MB'")
                await conn.execute("SET random_page_cost = 1.1")
                await conn.execute("SET effective_cache_size = '512MB'")
            else:  # development
                await conn.execute("SET work_mem = '4MB'")
                await conn.execute("SET maintenance_work_mem = '64MB'")

            # Query optimization settings
            await conn.execute("SET enable_seqscan = on")
            await conn.execute("SET enable_indexscan = on")
            await conn.execute("SET enable_bitmapscan = on")
            await conn.execute("SET enable_hashjoin = on")

            # Connection-specific optimizations
            await conn.execute(
                f"SET tcp_keepalives_idle = {DatabaseDefaults.TCP_KEEPALIVES_IDLE}"
            )  # 5 minutes
            await conn.execute(
                f"SET tcp_keepalives_interval = {DatabaseDefaults.TCP_KEEPALIVES_INTERVAL}"
            )
            await conn.execute(
                f"SET tcp_keepalives_count = {DatabaseDefaults.TCP_KEEPALIVES_COUNT}"
            )

            # Security settings
            await conn.execute("SET row_security = on")

            logger.debug(
                "Connection setup completed with security and performance settings",
            )
        except Exception as e:
            logger.error(f"Error during connection setup: {e}")
            raise

    async def _warmup_connection_pool_with_pool(self, pool: Pool) -> None:
        """Warm up a specific connection pool for better initial performance.

        Args:
            pool: The asyncpg Pool to warm up
        """
        try:
            logger.debug("Starting connection pool warmup")

            # Get pool configuration
            min_size = self.min_size
            warmup_connections = min(min_size + 2, self.max_size)

            # Acquire and release multiple connections to warm up the pool
            connections = []
            try:
                for i in range(warmup_connections):
                    conn = await pool.acquire()
                    connections.append(conn)

                    # Perform a lightweight query to ensure connection is active
                    await conn.fetchrow("SELECT 1 as warmup_test")

                # Release all connections back to the pool
                for conn in connections:
                    await pool.release(conn)

                logger.debug(
                    f"Connection pool warmup completed with {warmup_connections} connections"
                )

            except Exception as warmup_error:
                logger.warning(f"Error during connection warmup: {warmup_error}")
                # Release any connections that were acquired
                for conn in connections:
                    try:
                        await pool.release(conn)
                    except asyncpg.InterfaceError as e:
                        logger.warning(
                            f"Failed to release connection during warmup cleanup: {e}"
                        )
                    except Exception as e:
                        logger.error(
                            f"Unexpected error releasing connection: {e}",
                            exc_info=True,
                        )

        except Exception as e:
            logger.error(f"Connection pool warmup failed: {e}")
            # Don't raise - warmup failure shouldn't prevent connection

    def _reset_resource_tracking(self) -> None:
        """Reset all resource tracking data structures on successful connection."""
        try:
            # Reset pool statistics
            self._pool_stats = {
                "acquired_count": 0,
                "released_count": 0,
                "exhaustion_alerts": 0,
            }

            # Reset connection metrics
            self._connection_metrics = ConnectionMetrics()

            # Clear prepared statement cache
            self._prepared_statement_cache.clear()

            # Clear batch operations buffer
            self._batch_operations_buffer.clear()

            # Clear active connections tracking
            self._active_connections.clear()

            # Reset warmup status
            self._warmup_completed = False

            logger.debug("Resource tracking reset completed")

        except Exception as e:
            logger.warning(f"Error resetting resource tracking: {e}")
            # Don't raise - this is cleanup, not critical

    async def disconnect(self) -> None:
        """Disconnect from PostgreSQL with comprehensive cleanup."""
        if not self.pool:
            logger.info("PostgreSQL client already disconnected")
            # Still perform cleanup of accumulated data in case pool was None but state exists
            self._cleanup_accumulated_data()
            return

        # Forcefully clear connection state to prevent new operations during shutdown
        was_connected = self._connected
        self._connected = False

        try:
            # Check if pool is already closed
            if hasattr(self.pool, "_closed") and self.pool._closed:
                logger.info("Connection pool already closed")
                return

            # Get pool statistics before closing for logging
            try:
                pool_size = self.pool.get_size()
                logger.info(
                    f"Closing PostgreSQL connection pool (current size: {pool_size})",
                )
            except (AttributeError, RuntimeError, asyncpg.InterfaceError) as e:
                # Pool might be in inconsistent state, skip statistics
                logger.debug(f"Could not retrieve pool size during close: {e}")
                logger.info("Closing PostgreSQL connection pool (size unavailable)")

            # Gracefully close all connections with timeout
            close_timeout = float(
                DatabaseDefaults.GRACEFUL_SHUTDOWN_TIMEOUT
            )  # Graceful shutdown timeout
            try:
                await asyncio.wait_for(self.pool.close(), timeout=close_timeout)
                logger.info("PostgreSQL connection pool closed gracefully")
            except TimeoutError:
                logger.warning(
                    f"Pool close timed out after {close_timeout}s, forcing termination",
                )
                # Force terminate remaining connections
                try:
                    await asyncio.wait_for(
                        self.pool.terminate(),
                        timeout=float(DatabaseDefaults.FORCE_TERMINATE_TIMEOUT),
                    )
                    logger.info("PostgreSQL connection pool force terminated")
                except TimeoutError:
                    logger.error(
                        "Force termination also timed out - pool may have resource leaks"
                    )
                except Exception as term_error:
                    logger.error(f"Error during force termination: {term_error}")

        except Exception as e:
            logger.error(f"Error during PostgreSQL disconnect: {e}")
            # Attempt force termination as fallback
            try:
                if (
                    self.pool
                    and hasattr(self.pool, "_closed")
                    and not self.pool._closed
                ):
                    await asyncio.wait_for(
                        self.pool.terminate(),
                        timeout=float(DatabaseDefaults.FORCE_TERMINATE_TIMEOUT),
                    )
                    logger.info(
                        "PostgreSQL connection pool force terminated as fallback"
                    )
            except Exception as terminate_error:
                logger.error(
                    f"Error during fallback force termination: {terminate_error}"
                )

        finally:
            # Always perform comprehensive cleanup regardless of errors
            try:
                self._cleanup_accumulated_data()
            except Exception as cleanup_error:
                logger.warning(
                    f"Error during accumulated data cleanup: {cleanup_error}"
                )

            # Reset all state
            self._connected = False
            self.pool = None
            self._warmup_completed = False

            if was_connected:
                logger.info("PostgreSQL client disconnected and all resources cleaned")
            else:
                logger.debug("PostgreSQL client cleanup completed")

    def _cleanup_accumulated_data(self) -> None:
        """Clean up all accumulated data structures to prevent memory leaks."""
        try:
            # Clear prepared statement cache
            if hasattr(self, "_prepared_statement_cache"):
                self._prepared_statement_cache.clear()
                logger.debug("Prepared statement cache cleared")

            # Clear batch operations buffer
            if hasattr(self, "_batch_operations_buffer"):
                self._batch_operations_buffer.clear()
                logger.debug("Batch operations buffer cleared")

            # Clear active connections tracking
            if hasattr(self, "_active_connections"):
                self._active_connections.clear()
                logger.debug("Active connections tracking cleared")

            # Reset pool statistics to prevent accumulation across reconnects
            self._pool_stats = {
                "acquired_count": 0,
                "released_count": 0,
                "exhaustion_alerts": 0,
            }

            # Reset connection metrics
            self._connection_metrics = ConnectionMetrics()

            logger.debug("All accumulated data structures cleared")

        except Exception as e:
            logger.warning(f"Error during data cleanup: {e}")
            # Don't raise - cleanup errors shouldn't prevent disconnect

    async def _ensure_tables_with_pool(self, pool: Pool) -> None:
        """Create necessary tables using provided pool (for connection setup)."""
        try:
            async with pool.acquire() as conn:
                await self._execute_table_creation(conn)
            logger.info("Database tables ensured during connection setup")
        except Exception as e:
            logger.error(f"Error ensuring tables during connection setup: {e}")
            raise

    async def _ensure_tables(self) -> None:
        """Create necessary tables if they don't exist."""
        if not self.pool:
            raise RuntimeError("Cannot ensure tables - no database connection")

        async with self.pool.acquire() as conn:
            await self._execute_table_creation(conn)

    async def _execute_table_creation(self, conn: asyncpg.Connection) -> None:
        """Execute table creation SQL on the provided connection."""
        create_tables_sql = (
            """
        -- Service sessions table with enhanced security
        CREATE TABLE IF NOT EXISTS service_sessions (
            id UUID PRIMARY KEY,
            service_name VARCHAR(255) NOT NULL,
            instance_id VARCHAR(255),
            session_start TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
            session_end TIMESTAMP WITH TIME ZONE,
            status VARCHAR(50) NOT NULL DEFAULT 'active' CHECK (status IN ('active', 'ended', 'terminated', 'failed')),
            metadata JSONB DEFAULT '{}',
            created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
            -- Security: Add constraint to prevent future dates
            CONSTRAINT session_start_valid CHECK (session_start <= NOW()),
            CONSTRAINT session_end_valid CHECK (session_end IS NULL OR session_end >= session_start)
        );

        -- Hook events table for persistence and debugging with security enhancements
        CREATE TABLE IF NOT EXISTS hook_events (
            id UUID PRIMARY KEY,
            source VARCHAR(255) NOT NULL,
            action VARCHAR(255) NOT NULL,
            resource VARCHAR(255) NOT NULL,
            resource_id VARCHAR(255) NOT NULL,
            payload JSONB NOT NULL DEFAULT '{}',
            metadata JSONB NOT NULL DEFAULT '{}',
            processed BOOLEAN NOT NULL DEFAULT FALSE,
            processing_errors TEXT[],
            retry_count INTEGER NOT NULL DEFAULT 0 CHECK (retry_count >= 0 AND retry_count <= """
            + str(DatabaseDefaults.MAX_RETRY_COUNT)
            + """),
            created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
            processed_at TIMESTAMP WITH TIME ZONE,
            -- Security: Add data retention constraint
            CONSTRAINT processing_time_valid CHECK (processed_at IS NULL OR processed_at >= created_at),
            CONSTRAINT retry_count_limit CHECK (retry_count <= """
            + str(DatabaseDefaults.MAX_RETRY_COUNT)
            + """)
        );

        -- Event processing metrics with enhanced constraints
        CREATE TABLE IF NOT EXISTS event_metrics (
            id SERIAL PRIMARY KEY,
            event_id UUID NOT NULL,
            processing_time_ms FLOAT NOT NULL CHECK (processing_time_ms >= 0),
            kafka_publish_success BOOLEAN NOT NULL,
            error_message TEXT,
            created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
            -- Security: Prevent unrealistic processing times
            CONSTRAINT processing_time_reasonable CHECK (processing_time_ms < 300000) -- 5 minutes max
        );

        -- Security audit table for connection and authentication events
        CREATE TABLE IF NOT EXISTS security_audit_log (
            id SERIAL PRIMARY KEY,
            event_type VARCHAR(100) NOT NULL,
            client_info JSONB,
            success BOOLEAN NOT NULL,
            error_details TEXT,
            created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
        );

        -- Performance monitoring table
        CREATE TABLE IF NOT EXISTS connection_metrics (
            id SERIAL PRIMARY KEY,
            pool_size INTEGER NOT NULL,
            active_connections INTEGER NOT NULL,
            idle_connections INTEGER NOT NULL,
            total_queries BIGINT NOT NULL DEFAULT 0,
            failed_queries INTEGER NOT NULL DEFAULT 0,
            avg_query_time_ms FLOAT,
            recorded_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
        );

        -- Indexes for performance with security considerations
        CREATE INDEX IF NOT EXISTS idx_service_sessions_service_status
            ON service_sessions(service_name, status) WHERE status = 'active';
        CREATE INDEX IF NOT EXISTS idx_service_sessions_created_at
            ON service_sessions(created_at DESC);
        CREATE INDEX IF NOT EXISTS idx_service_sessions_cleanup
            ON service_sessions(session_end) WHERE session_end IS NOT NULL;

        CREATE INDEX IF NOT EXISTS idx_hook_events_source_action
            ON hook_events(source, action);
        CREATE INDEX IF NOT EXISTS idx_hook_events_processed
            ON hook_events(processed, created_at DESC) WHERE NOT processed;
        CREATE INDEX IF NOT EXISTS idx_hook_events_created_at
            ON hook_events(created_at DESC);
        CREATE INDEX IF NOT EXISTS idx_hook_events_retry
            ON hook_events(retry_count, created_at) WHERE retry_count > 0;

        CREATE INDEX IF NOT EXISTS idx_event_metrics_event_id
            ON event_metrics(event_id);
        CREATE INDEX IF NOT EXISTS idx_event_metrics_created_at
            ON event_metrics(created_at DESC);
        CREATE INDEX IF NOT EXISTS idx_event_metrics_performance
            ON event_metrics(processing_time_ms, kafka_publish_success);

        CREATE INDEX IF NOT EXISTS idx_security_audit_event_type
            ON security_audit_log(event_type, created_at DESC);
        CREATE INDEX IF NOT EXISTS idx_security_audit_failed
            ON security_audit_log(created_at DESC) WHERE NOT success;

        CREATE INDEX IF NOT EXISTS idx_connection_metrics_recorded_at
            ON connection_metrics(recorded_at DESC);

        -- Row Level Security policies (commented out for now, enable when authentication is implemented)
        -- ALTER TABLE service_sessions ENABLE ROW LEVEL SECURITY;
        -- ALTER TABLE hook_events ENABLE ROW LEVEL SECURITY;
        -- ALTER TABLE event_metrics ENABLE ROW LEVEL SECURITY;

        -- Data retention function for automatic cleanup
        CREATE OR REPLACE FUNCTION cleanup_old_data() RETURNS INTEGER AS $$
        DECLARE
            deleted_count INTEGER := 0;
            temp_count INTEGER;
        BEGIN
            -- Clean up old processed hook events
            DELETE FROM hook_events
            WHERE processed = TRUE
            AND created_at < NOW() - INTERVAL '"""
            + str(DatabaseDefaults.HOOK_EVENTS_RETENTION_DAYS)
            + """ days';
            GET DIAGNOSTICS temp_count = ROW_COUNT;
            deleted_count := deleted_count + temp_count;

            -- Clean up old event metrics
            DELETE FROM event_metrics
            WHERE created_at < NOW() - INTERVAL '"""
            + str(DatabaseDefaults.EVENT_METRICS_RETENTION_DAYS)
            + """ days';
            GET DIAGNOSTICS temp_count = ROW_COUNT;
            deleted_count := deleted_count + temp_count;

            -- Clean up old ended sessions (older than 7 days)
            DELETE FROM service_sessions
            WHERE status IN ('ended', 'terminated', 'failed')
            AND session_end < NOW() - INTERVAL '7 days';
            GET DIAGNOSTICS temp_count = ROW_COUNT;
            deleted_count := deleted_count + temp_count;

            -- Clean up old security audit logs (older than 180 days)
            DELETE FROM security_audit_log
            WHERE created_at < NOW() - INTERVAL '180 days';
            GET DIAGNOSTICS temp_count = ROW_COUNT;
            deleted_count := deleted_count + temp_count;

            -- Clean up old connection metrics (keep only last 30 days)
            DELETE FROM connection_metrics
            WHERE recorded_at < NOW() - INTERVAL '30 days';
            GET DIAGNOSTICS temp_count = ROW_COUNT;
            deleted_count := deleted_count + temp_count;

            RETURN deleted_count;
        END;
        $$ LANGUAGE plpgsql;
        """
        )

        await conn.execute(create_tables_sql)
        logger.info("Database tables and security enhancements created successfully")

    async def record_security_event(
        self,
        event_type: str,
        client_info: dict[str, Any] | None = None,
        success: bool = True,
        error_details: str | None = None,
    ) -> bool:
        """Record security-related events for audit purposes.

        Args:
            event_type: Type of security event (connection, authentication, etc.)
            client_info: Client connection information
            success: Whether the event was successful
            error_details: Error details if unsuccessful

        Returns:
            True if recorded successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            async with self.pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO security_audit_log (event_type, client_info, success, error_details)
                    VALUES ($1, $2, $3, $4)
                """,
                    event_type,
                    client_info or {},
                    success,
                    error_details,
                )

            return True

        except Exception as e:
            logger.error(f"Error recording security event: {e}")
            return False

    async def record_connection_metrics(self) -> bool:
        """Record current connection pool metrics for monitoring.

        Returns:
            True if recorded successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            pool_size = self.pool.get_size()
            # Note: asyncpg doesn't provide direct access to active/idle connection counts
            # We'll estimate based on pool size for now
            active_connections = min(pool_size, self.max_size // 2)  # Estimation
            idle_connections = pool_size - active_connections

            async with self.pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO connection_metrics (
                        pool_size, active_connections, idle_connections, total_queries, failed_queries
                    ) VALUES ($1, $2, $3, $4, $5)
                """,
                    pool_size,
                    active_connections,
                    idle_connections,
                    self._connection_metrics.query_count,
                    self._connection_metrics.failed_queries,
                )

            return True

        except Exception as e:
            logger.error(f"Error recording connection metrics: {e}")
            return False

    async def cleanup_old_data(self) -> CleanupResult:
        """Clean up old data using the database cleanup function.

        Returns:
            CleanupResult with cleanup results
        """
        if not self.pool:
            # Return empty result when no pool available
            return CleanupResult(total_deleted=0, operation_time_ms=0.0)

        start_time = time.time()

        try:
            async with self.pool.acquire() as conn:
                result = await conn.fetchval("SELECT cleanup_old_data()")
                operation_time = (time.time() - start_time) * 1000

            return CleanupResult(
                total_deleted=result or 0, operation_time_ms=operation_time
            )

        except Exception as e:
            logger.error(f"Error during data cleanup: {e}")
            operation_time = (time.time() - start_time) * 1000
            return CleanupResult(total_deleted=0, operation_time_ms=operation_time)

    async def validate_ssl_connection(self) -> SSLValidationResult:
        """Validate that SSL/TLS is properly configured and active.

        Returns:
            SSLValidationResult with SSL validation results
        """
        if not self.pool:
            return SSLValidationResult(
                ssl_enabled=False, validation_errors=["No database connection"]
            )

        if not self._ssl_context:
            return SSLValidationResult(
                ssl_enabled=False, validation_errors=["SSL not configured"]
            )

        try:
            async with self.pool.acquire() as conn:
                ssl_info = await conn.fetchrow(
                    """
                    SELECT
                        ssl_is_used() as ssl_active,
                        ssl_version() as ssl_version,
                        ssl_cipher() as ssl_cipher
                """,
                )

                if ssl_info and ssl_info["ssl_active"]:
                    return SSLValidationResult(
                        ssl_enabled=True,
                        ssl_version=ssl_info["ssl_version"],
                        cipher_suite=ssl_info["ssl_cipher"],
                        certificate_info={
                            "hostname_verification": str(
                                self._ssl_context.check_hostname
                            ),
                            "verify_mode": self._ssl_context.verify_mode.name,
                        },
                    )
                else:
                    return SSLValidationResult(
                        ssl_enabled=False,
                        validation_errors=[
                            "SSL configured but not active on connection"
                        ],
                    )

        except Exception as e:
            logger.error(f"Error validating SSL connection: {e}")
            return SSLValidationResult(ssl_enabled=False, validation_errors=[str(e)])

    async def create_service_session(
        self,
        session_id: UUID,
        service_name: str,
        instance_id: str | None = None,
        metadata: dict[str, Any] | None = None,
    ) -> bool:
        """Create a new service session.

        Args:
            session_id: Unique session identifier
            service_name: Name of the service
            instance_id: Service instance identifier
            metadata: Additional session metadata

        Returns:
            True if created successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            async with self.pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO service_sessions (id, service_name, instance_id, metadata)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (id) DO UPDATE SET
                        updated_at = NOW(),
                        metadata = $4
                """,
                    session_id,
                    service_name,
                    instance_id,
                    metadata or {},
                )

            logger.info(f"Created session {session_id} for service {service_name}")
            return True

        except Exception as e:
            logger.error(f"Error creating session {session_id}: {e}")
            return False

    async def end_service_session(self, session_id: UUID) -> bool:
        """End a service session.

        Args:
            session_id: Session identifier to end

        Returns:
            True if ended successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            async with self.pool.acquire() as conn:
                result = await conn.execute(
                    """
                    UPDATE service_sessions
                    SET session_end = NOW(), status = 'ended', updated_at = NOW()
                    WHERE id = $1 AND status = 'active'
                """,
                    session_id,
                )

            if result == "UPDATE 1":
                logger.info(f"Ended session {session_id}")
                return True
            logger.warning(f"Session {session_id} not found or already ended")
            return False

        except Exception as e:
            logger.error(f"Error ending session {session_id}: {e}")
            return False

    async def store_hook_event(self, hook_event: HookEvent | dict[str, Any]) -> bool:
        """Store a hook event for persistence and debugging.

        Args:
            hook_event: Hook event data

        Returns:
            True if stored successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            async with self.pool.acquire() as conn:
                # Normalize input
                if isinstance(hook_event, dict):
                    try:
                        hook_event = HookEvent(**hook_event)
                    except (TypeError, ValueError) as e:
                        logger.error(
                            f"Failed to parse hook event from dict: {e}",
                            extra={"hook_data": hook_event},
                        )
                        return False
                await conn.execute(
                    """
                    INSERT INTO hook_events (
                        id, source, action, resource, resource_id,
                        payload, metadata, processed, processing_errors, retry_count
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """,
                    hook_event.id,
                    hook_event.metadata.source,
                    hook_event.payload.action,
                    hook_event.payload.resource,
                    hook_event.payload.resource_id,
                    hook_event.payload.model_dump(),
                    hook_event.metadata.model_dump(),
                    hook_event.processed,
                    hook_event.processing_errors or [],
                    hook_event.retry_count,
                )

            return True

        except Exception as e:
            logger.error(f"Error storing hook event {hook_event.id}: {e}")
            return False

    async def record_event_metrics(
        self,
        event_id: UUID,
        processing_time_ms: float,
        kafka_publish_success: bool,
        error_message: str | None = None,
    ) -> bool:
        """Record event processing metrics.

        Args:
            event_id: Event identifier
            processing_time_ms: Processing time in milliseconds
            kafka_publish_success: Whether Kafka publish succeeded
            error_message: Error message if any

        Returns:
            True if recorded successfully, False otherwise
        """
        if not self.pool:
            return False

        try:
            async with self.pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO event_metrics (
                        event_id, processing_time_ms, kafka_publish_success, error_message
                    ) VALUES ($1, $2, $3, $4)
                """,
                    event_id,
                    processing_time_ms,
                    kafka_publish_success,
                    error_message,
                )

            return True

        except Exception as e:
            logger.error(f"Error recording metrics for event {event_id}: {e}")
            return False

    async def get_active_sessions(self) -> list[ServiceSession]:
        """Get all active service sessions.

        Returns:
            List of active ServiceSession objects
        """
        if not self.pool:
            return []

        try:
            async with self.pool.acquire() as conn:
                rows = await conn.fetch(
                    """
                    SELECT id, service_name, instance_id, session_start, metadata
                    FROM service_sessions
                    WHERE status = 'active'
                    ORDER BY session_start DESC
                """,
                )

            return [
                ServiceSession(
                    id=str(row["id"]),
                    service_name=row["service_name"],
                    instance_id=row["instance_id"],
                    session_start=(
                        row["session_start"].isoformat()
                        if row["session_start"]
                        else None
                    ),
                    metadata=row["metadata"],
                    status="active",
                )
                for row in rows
            ]

        except Exception as e:
            logger.error(f"Error getting active sessions: {e}")
            return []

    async def health_check(self) -> HealthCheckResult:
        """Check PostgreSQL connection health with enhanced pool monitoring.

        Returns:
            Health status dictionary with detailed pool metrics
        """
        if not self._connected or not self.pool:
            return {
                "status": "unhealthy",
                "connected": False,
                "error": "Not connected to PostgreSQL",
            }

        # Check if pool is closed before attempting to use it
        try:
            if hasattr(self.pool, "_closed") and self.pool._closed:
                return {
                    "status": "unhealthy",
                    "connected": False,
                    "error": "Connection pool is closed",
                }
        except AttributeError:
            # Pool might not have _closed attribute in test environments
            pass

        try:
            # Test connection acquisition with timeout
            conn = await asyncio.wait_for(
                self.pool.acquire(),
                timeout=float(DatabaseDefaults.CONNECTION_ACQUIRE_TIMEOUT),
            )
            try:
                # Test basic query
                result = await conn.fetchval("SELECT 1")

                # Get enhanced connection pool stats with error handling
                try:
                    pool_size = self.pool.get_size()
                    max_size = self.pool.get_max_size()
                    min_size = self.pool.get_min_size()

                    # Calculate utilization with protection against division by zero
                    utilization_percent = (
                        (pool_size / max_size * 100) if max_size > 0 else 0
                    )

                    pool_stats = {
                        "size": pool_size,
                        "max_size": max_size,
                        "min_size": min_size,
                        "utilization_percent": utilization_percent,
                        "acquired_count": self._pool_stats["acquired_count"],
                        "released_count": self._pool_stats["released_count"],
                        "potential_leaks": self._pool_stats["acquired_count"]
                        - self._pool_stats["released_count"],
                        "exhaustion_alerts": self._pool_stats["exhaustion_alerts"],
                    }

                    # Check for pool exhaustion warning
                    if utilization_percent >= self.pool_exhaustion_threshold:
                        self._pool_stats["exhaustion_alerts"] += 1
                        logger.warning(
                            f"Connection pool utilization high: {utilization_percent:.1f}% "
                            f"(threshold: {self.pool_exhaustion_threshold}%)",
                        )

                except Exception as pool_stats_error:
                    logger.warning(f"Error getting pool statistics: {pool_stats_error}")
                    pool_stats = {
                        "error": "Could not retrieve pool statistics",
                        "acquired_count": self._pool_stats["acquired_count"],
                        "released_count": self._pool_stats["released_count"],
                        "potential_leaks": self._pool_stats["acquired_count"]
                        - self._pool_stats["released_count"],
                        "exhaustion_alerts": self._pool_stats["exhaustion_alerts"],
                    }

                return {
                    "status": "healthy",
                    "connected": True,
                    "test_query": result == 1,
                    "pool": pool_stats,
                    "pool_health": (
                        "warning"
                        if pool_stats.get("utilization_percent", 0)
                        >= self.pool_exhaustion_threshold
                        else "normal"
                    ),
                }
            finally:
                # Always release the connection back to the pool
                await self.pool.release(conn)

        except TimeoutError:
            # Connection acquisition timed out - likely pool exhaustion
            self._pool_stats["exhaustion_alerts"] += 1
            return {
                "status": "unhealthy",
                "connected": False,
                "error": "Connection acquisition timed out - possible pool exhaustion",
                "pool_exhaustion_likely": True,
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "connected": False,
                "error": str(e),
            }

    async def execute_query(
        self,
        query: str,
        *args,
        fetch: bool = False,
        fetchrow: bool = False,
        use_prepared_statements: bool = None,
    ) -> Any:
        """Execute a SQL query with connection pool monitoring and prepared statement caching.

        Args:
            query: SQL query to execute
            *args: Query parameters
            fetch: Whether to fetch all results
            fetchrow: Whether to fetch a single row
            use_prepared_statements: Override for prepared statement usage (defaults to global setting)

        Returns:
            Query results or None
        """
        if not self.pool:
            raise RuntimeError("PostgreSQL client not connected")

        # Check pool utilization before acquiring connection
        try:
            pool_size = self.pool.get_size()
            max_size = self.pool.get_max_size()
            if (
                isinstance(pool_size, int)
                and isinstance(max_size, int)
                and max_size > 0
            ):
                current_utilization = (pool_size / max_size) * 100
                if current_utilization >= self.pool_exhaustion_threshold:
                    logger.warning(
                        f"High pool utilization before query: {current_utilization:.1f}%",
                    )
        except (TypeError, AttributeError):
            # Skip utilization check if pool methods return non-numeric values (e.g., during testing)
            pass

        # Determine if we should use prepared statements
        use_prepared = (
            use_prepared_statements
            if use_prepared_statements is not None
            else self._enable_prepared_statements
        )

        query_start = time.time()

        try:
            # Track connection acquisition for leak detection and metrics
            if self.connection_leak_detection_enabled:
                self._pool_stats["acquired_count"] += 1

            self._connection_metrics.acquired_connections += 1

            async with self.pool.acquire() as conn:
                try:
                    # Use prepared statements for better performance if enabled and query has parameters
                    if (
                        use_prepared
                        and args
                        and self._should_use_prepared_statement(query)
                    ):
                        return await self._execute_with_prepared_statement(
                            conn, query, args, fetch, fetchrow
                        )
                    else:
                        # Use standard execution
                        if fetch:
                            result = await conn.fetch(query, *args)
                        elif fetchrow:
                            result = await conn.fetchrow(query, *args)
                        else:
                            result = await conn.execute(query, *args)

                        # Update metrics
                        query_duration = time.time() - query_start
                        self._connection_metrics.query_count += 1
                        self._connection_metrics.query_time_total += query_duration

                        return result

                finally:
                    # Track connection release for leak detection
                    if self.connection_leak_detection_enabled:
                        self._pool_stats["released_count"] += 1

                    self._connection_metrics.released_connections += 1

        except Exception as e:
            self._connection_metrics.failed_acquisitions += 1
            self._connection_metrics.failed_queries += 1
            logger.error(f"Failed to execute query: {e}")
            raise

    async def fetch_one(self, query: str, *args) -> Any:
        """Fetch a single row."""
        return await self.execute_query(query, *args, fetchrow=True)

    async def fetch_all(self, query: str, *args) -> list[Any]:
        """Fetch all rows."""
        return await self.execute_query(query, *args, fetch=True)

    def _should_use_prepared_statement(self, query: str) -> bool:
        """Determine if a query should use prepared statements for optimization.

        Args:
            query: SQL query to evaluate

        Returns:
            True if the query should use prepared statements, False otherwise
        """
        # Skip prepared statements for simple queries or DDL operations
        query_upper = query.strip().upper()

        # Don't use prepared statements for:
        # - DDL operations (CREATE, ALTER, DROP)
        # - Transaction control (BEGIN, COMMIT, ROLLBACK)
        # - Very simple queries without parameters
        skip_patterns = [
            "CREATE",
            "ALTER",
            "DROP",
            "BEGIN",
            "COMMIT",
            "ROLLBACK",
            "SET",
            "SHOW",
            "EXPLAIN",
            "ANALYZE",
        ]

        for pattern in skip_patterns:
            if query_upper.startswith(pattern):
                return False

        # Use prepared statements for SELECT, INSERT, UPDATE, DELETE with parameters
        return any(
            query_upper.startswith(cmd)
            for cmd in ["SELECT", "INSERT", "UPDATE", "DELETE"]
        )

    async def _execute_with_prepared_statement(
        self,
        conn: asyncpg.Connection,
        query: str,
        args: tuple,
        fetch: bool,
        fetchrow: bool,
    ) -> Any:
        """Execute query using connection-level prepared statements.

        NOTE: Prepared statement caching is DISABLED for connection pool compatibility.
        Prepared statements in asyncpg are connection-specific and cannot be shared
        across different connections from the pool. Using cached statements causes:
        "cannot call PreparedStatement.fetchrow(): the underlying connection has been released back to the pool"

        Instead, we use direct execution with parameterized queries, which is:
        - Safe for connection pooling (no cross-connection statement usage)
        - Still SQL-injection safe (parameterized queries)
        - Simpler and more maintainable
        - asyncpg internally caches prepared statements per connection

        Args:
            conn: Database connection
            query: SQL query
            args: Query parameters
            fetch: Whether to fetch all results
            fetchrow: Whether to fetch single row

        Returns:
            Query results
        """
        query_start = time.time()

        try:
            # Use direct execution - asyncpg handles per-connection statement caching internally
            # This is safe for connection pools and avoids cross-connection statement issues
            if fetch:
                result = await conn.fetch(query, *args)
            elif fetchrow:
                result = await conn.fetchrow(query, *args)
            else:
                result = await conn.execute(query, *args)

            # Update metrics
            query_duration = time.time() - query_start
            self._connection_metrics.query_count += 1
            self._connection_metrics.query_time_total += query_duration

            return result

        except Exception as e:
            self._connection_metrics.failed_queries += 1
            logger.error(f"Error executing query: {e}")
            raise

    async def get_pool_metrics(self) -> PoolMetrics:
        """Get detailed connection pool metrics for monitoring.

        Returns:
            PoolMetrics with comprehensive pool metrics
        """
        if not self.pool:
            return PoolMetrics(
                current_size=0,
                max_size=0,
                utilization_percent=0.0,
                connection_lifecycle={},
                performance_metrics={},
                health_status={"status": "no_pool"},
            )

        try:
            current_size = self.pool.get_size()
            max_size = self.pool.get_max_size()
            utilization_percent = (
                (current_size / max_size) * 100 if max_size > 0 else 0.0
            )

            connection_lifecycle = {
                "min_size": str(self.pool.get_min_size()),
                "max_queries_per_connection": str(self.max_queries_per_connection),
                "connection_max_age_seconds": str(self.connection_max_age_seconds),
                "query_timeout_seconds": str(self.query_timeout_seconds),
                "acquire_timeout_seconds": str(self.acquire_timeout_seconds),
            }

            performance_metrics = {
                "acquired_count": float(self._pool_stats["acquired_count"]),
                "released_count": float(self._pool_stats["released_count"]),
                "potential_leaks": float(
                    self._pool_stats["acquired_count"]
                    - self._pool_stats["released_count"]
                ),
                "exhaustion_threshold_percent": float(self.pool_exhaustion_threshold),
                "exhaustion_alerts_count": float(self._pool_stats["exhaustion_alerts"]),
            }

            health_status = {
                "status": "available",
                "leak_detection_enabled": str(self.connection_leak_detection_enabled),
                "current_status": (
                    "warning"
                    if utilization_percent >= self.pool_exhaustion_threshold
                    else "normal"
                ),
            }

            return PoolMetrics(
                current_size=current_size,
                max_size=max_size,
                utilization_percent=utilization_percent,
                connection_lifecycle=connection_lifecycle,
                performance_metrics=performance_metrics,
                health_status=health_status,
            )

        except Exception as e:
            logger.error(f"Error getting pool metrics: {e}")
            return PoolMetrics(
                current_size=0,
                max_size=0,
                utilization_percent=0.0,
                connection_lifecycle={},
                performance_metrics={},
                health_status={"status": "error", "error": str(e)},
            )

    async def _warmup_connection_pool(self) -> None:
        """Warm up the connection pool by pre-establishing connections.

        This improves initial connection performance and validates pool health.
        """
        if not self.pool or not self._enable_connection_warmup:
            return

        try:
            logger.info("Warming up connection pool...")

            # Pre-establish connections up to min_size
            warmup_connections = []
            for i in range(
                min(self.min_size, DatabaseDefaults.MAX_WARMUP_CONNECTIONS)
            ):  # Limit connections for warmup
                try:
                    conn = await self.pool.acquire()
                    warmup_connections.append(conn)
                    # Test the connection
                    await conn.fetchval("SELECT 1")
                except Exception as e:
                    logger.warning(f"Failed to acquire warmup connection {i+1}: {e}")
                    break

            # Release warmup connections back to pool
            for conn in warmup_connections:
                try:
                    await self.pool.release(conn)
                except Exception as e:
                    logger.warning(f"Error releasing warmup connection: {e}")

            self._warmup_completed = True
            logger.info(
                f"Connection pool warmup completed with {len(warmup_connections)} connections"
            )

        except Exception as e:
            logger.error(f"Error during connection pool warmup: {e}")
            # Don't fail the connection process if warmup fails
            self._warmup_completed = False

    async def execute_batch(
        self,
        operations: list[BatchOperation],
        use_transaction: bool = True,
        optimize_similar_queries: bool = True,
    ) -> list[Any]:
        """Execute multiple database operations in an optimized batch for maximum performance.

        This method implements several optimization strategies:
        - Transaction batching for consistency and performance
        - Prepared statement reuse across similar queries
        - executemany() optimization for identical queries with different parameters
        - Intelligent grouping by operation type and query similarity

        Args:
            operations: List of BatchOperation objects to execute
            use_transaction: Whether to wrap all operations in a single transaction
            optimize_similar_queries: Whether to optimize identical queries using executemany()

        Returns:
            List of results from each operation (preserving input order)
        """
        if not self.pool:
            raise RuntimeError("PostgreSQL client not connected")

        if not operations:
            return []

        results = [None] * len(operations)  # Pre-allocate results preserving order
        batch_start = time.time()

        try:
            async with self.pool.acquire() as conn:
                # Wrap in transaction for consistency and performance
                if use_transaction:
                    async with conn.transaction():
                        await self._execute_batch_operations(
                            conn, operations, results, optimize_similar_queries
                        )
                else:
                    await self._execute_batch_operations(
                        conn, operations, results, optimize_similar_queries
                    )

                # Update metrics
                batch_duration = time.time() - batch_start
                self._connection_metrics.query_count += len(operations)
                self._connection_metrics.query_time_total += batch_duration

                logger.debug(
                    f"Executed optimized batch of {len(operations)} operations in {batch_duration*1000:.2f}ms"
                )

        except Exception as e:
            self._connection_metrics.failed_queries += len(operations)
            logger.error(f"Optimized batch execution failed: {e}")
            raise

        return results

    async def _execute_batch_operations(
        self,
        conn: asyncpg.Connection,
        operations: list[BatchOperation],
        results: list[Any],
        optimize_similar_queries: bool,
    ) -> None:
        """Internal method to execute batch operations with advanced optimizations."""

        if optimize_similar_queries:
            # Group operations by identical queries for executemany optimization
            query_groups = self._group_operations_by_query(operations)

            for query, grouped_ops in query_groups.items():
                if len(grouped_ops) > 1:
                    # Use executemany for identical queries
                    await self._execute_similar_queries_batch(
                        conn, grouped_ops, results
                    )
                else:
                    # Execute single operation normally
                    await self._execute_single_operation(conn, grouped_ops[0], results)
        else:
            # Execute operations individually but within the same connection/transaction
            for op_index, operation in enumerate(operations):
                await self._execute_single_operation(
                    conn, (op_index, operation), results
                )

    def _group_operations_by_query(
        self, operations: list[BatchOperation]
    ) -> dict[str, list[tuple[int, BatchOperation]]]:
        """Group operations by identical query strings for batch optimization."""
        query_groups = {}

        for index, operation in enumerate(operations):
            # Create a key combining query and operation type
            key = f"{operation.query.strip()}|{operation.operation_type}"
            if key not in query_groups:
                query_groups[key] = []
            query_groups[key].append((index, operation))

        return query_groups

    async def _execute_similar_queries_batch(
        self,
        conn: asyncpg.Connection,
        grouped_ops: list[tuple[int, BatchOperation]],
        results: list[Any],
    ) -> None:
        """Execute multiple operations with identical queries using executemany optimization.

        NOTE: Prepared statement caching is DISABLED for connection pool compatibility.
        See _execute_with_prepared_statement for detailed explanation.
        """
        if not grouped_ops:
            return

        # Extract the operation details
        first_index, first_op = grouped_ops[0]
        query = first_op.query
        operation_type = first_op.operation_type

        try:
            # Use executemany for execute operations with sufficient batch size
            if (
                operation_type == "execute"
                and len(grouped_ops) > DatabaseDefaults.MIN_BATCH_FOR_EXECUTEMANY
            ):  # Only use executemany for larger batches
                # Extract all parameters
                params_list = [op.params for _, op in grouped_ops]
                batch_results = await conn.executemany(query, params_list)

                # Assign results back to their original positions
                for i, (index, _) in enumerate(grouped_ops):
                    results[index] = (
                        batch_results[i] if i < len(batch_results) else None
                    )
            else:
                # Execute individually for non-execute operations or small batches
                # This uses direct execution (no prepared statement caching)
                for index, op in grouped_ops:
                    await self._execute_single_operation(conn, (index, op), results)

            logger.debug(
                f"Executed {len(grouped_ops)} similar queries using batch optimization"
            )

        except Exception as e:
            logger.warning(
                f"Batch optimization failed, falling back to individual execution: {e}"
            )
            # Fallback to individual execution
            for index, op in grouped_ops:
                await self._execute_single_operation(conn, (index, op), results)

    async def _execute_single_operation(
        self,
        conn: asyncpg.Connection,
        operation_data: tuple[int, BatchOperation],
        results: list[Any],
    ) -> None:
        """Execute a single operation and store result at the correct index."""
        index, operation = operation_data

        try:
            if operation.operation_type == "execute":
                result = await conn.execute(operation.query, *operation.params)
            elif operation.operation_type == "fetch":
                result = await conn.fetch(operation.query, *operation.params)
            elif operation.operation_type == "fetchrow":
                result = await conn.fetchrow(operation.query, *operation.params)
            else:
                raise ValueError(f"Unknown operation type: {operation.operation_type}")

            results[index] = result

        except Exception as e:
            logger.error(f"Failed to execute operation at index {index}: {e}")
            raise

    async def store_hook_events_batch(self, hook_events: list[HookEventData]) -> bool:
        """Store multiple hook events in a single batch operation for improved performance.

        Args:
            hook_events: List of HookEventData objects

        Returns:
            True if all events stored successfully, False otherwise
        """
        if not hook_events:
            return True

        if not self.pool:
            return False

        try:
            batch_operations = []
            for hook_event in hook_events:
                operation = BatchOperation(
                    query="""
                        INSERT INTO hook_events (
                            id, source, action, resource, resource_id,
                            payload, metadata, processed, processing_errors, retry_count
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    """,
                    params=(
                        UUID(hook_event.id),
                        hook_event.source,
                        hook_event.action,
                        hook_event.resource,
                        hook_event.resource_id,
                        hook_event.payload,
                        hook_event.metadata,
                        hook_event.processed,
                        hook_event.processing_errors or [],
                        hook_event.retry_count,
                    ),
                    operation_type="execute",
                )
                batch_operations.append(operation)

            await self.execute_batch(batch_operations)
            logger.info(f"Successfully stored batch of {len(hook_events)} hook events")
            return True

        except Exception as e:
            logger.error(f"Error storing hook events batch: {e}")
            return False

    async def bulk_insert(
        self,
        table_name: str,
        columns: list[str],
        data: list[tuple[Any, ...]],
        chunk_size: int = 1000,
    ) -> bool:
        """Perform bulk insert operation for maximum performance.

        Args:
            table_name: Name of the table to insert into
            columns: List of column names
            data: List of tuples containing row data
            chunk_size: Number of rows to insert per chunk

        Returns:
            True if successful, False otherwise
        """
        if not self.pool or not data:
            return False

        try:
            async with self.pool.acquire() as conn:
                # Create the bulk insert query
                placeholders = ", ".join(f"${i+1}" for i in range(len(columns)))
                query = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"

                # Process data in chunks to avoid memory issues
                for i in range(0, len(data), chunk_size):
                    chunk = data[i : i + chunk_size]

                    # Use executemany for bulk operations
                    await conn.executemany(query, chunk)

                logger.info(f"Bulk inserted {len(data)} rows into {table_name}")
                return True

        except Exception as e:
            logger.error(f"Bulk insert failed for table {table_name}: {e}")
            return False

    async def copy_bulk_insert(
        self,
        table_name: str,
        columns: list[str],
        data: list[tuple[Any, ...]],
        format_type: str = "csv",
    ) -> bool:
        """Ultra-high-performance bulk insert using PostgreSQL COPY for maximum throughput.

        This method uses PostgreSQL's COPY protocol, which is the fastest way to load
        large amounts of data. Significantly faster than executemany() or individual inserts.

        Args:
            table_name: Name of the table to insert into
            columns: List of column names
            data: List of tuples containing row data
            format_type: Format for COPY operation ('csv', 'binary', 'text')

        Returns:
            True if successful, False otherwise
        """
        if not self.pool or not data:
            return False

        try:
            async with self.pool.acquire() as conn:
                # Prepare the COPY command
                columns_str = ", ".join(columns)
                copy_sql = f"COPY {table_name} ({columns_str}) FROM STDIN WITH (FORMAT {format_type})"

                if format_type.lower() == "csv":
                    # Convert data to CSV format
                    import csv
                    import io

                    buffer = io.StringIO()
                    writer = csv.writer(buffer)
                    for row in data:
                        # Handle None values and ensure proper CSV formatting
                        formatted_row = [
                            (
                                ""
                                if value is None
                                else (
                                    str(value)
                                    if not isinstance(value, int | float | bool)
                                    else str(value)
                                )
                            )
                            for value in row
                        ]
                        writer.writerow(formatted_row)

                    buffer.seek(0)
                    copy_data = buffer.getvalue()

                    # Execute COPY operation
                    await conn.copy_from_table(
                        table_name,
                        source=io.StringIO(copy_data),
                        columns=columns,
                        format="csv",
                    )

                elif format_type.lower() == "binary":
                    # Use binary format for better performance with large datasets
                    await conn.copy_records_to_table(
                        table_name,
                        records=data,
                        columns=columns,
                    )

                else:  # text format
                    # Convert to tab-separated values
                    formatted_rows = []
                    for row in data:
                        formatted_row = "\t".join(
                            "" if value is None else str(value) for value in row
                        )
                        formatted_rows.append(formatted_row)

                    copy_data = "\n".join(formatted_rows)

                    await conn.copy_from_table(
                        table_name,
                        source=io.StringIO(copy_data),
                        columns=columns,
                        format="text",
                    )

                logger.info(
                    f"COPY bulk inserted {len(data)} rows into {table_name} using {format_type} format"
                )
                return True

        except Exception as e:
            logger.error(f"COPY bulk insert failed for table {table_name}: {e}")
            # Fallback to regular bulk insert
            logger.info("Falling back to regular bulk insert method")
            return await self.bulk_insert(table_name, columns, data)

    async def adaptive_bulk_insert(
        self,
        table_name: str,
        columns: list[str],
        data: list[tuple[Any, ...]],
        size_threshold: int = 1000,
    ) -> bool:
        """Automatically choose the optimal bulk insert method based on data size.

        For large datasets (>threshold), uses COPY for maximum performance.
        For smaller datasets, uses executemany() to avoid COPY overhead.

        Args:
            table_name: Name of the table to insert into
            columns: List of column names
            data: List of tuples containing row data
            size_threshold: Threshold for choosing COPY vs executemany

        Returns:
            True if successful, False otherwise
        """
        if not data:
            return True

        data_size = len(data)

        if data_size >= size_threshold:
            logger.debug(f"Using COPY method for large dataset ({data_size} rows)")
            return await self.copy_bulk_insert(table_name, columns, data)
        else:
            logger.debug(
                f"Using executemany method for small dataset ({data_size} rows)"
            )
            return await self.bulk_insert(table_name, columns, data)

    async def get_performance_metrics(self) -> PerformanceMetrics:
        """Get detailed performance metrics for monitoring and optimization.

        Returns:
            PerformanceMetrics containing comprehensive performance metrics
        """
        cache_hit_rate = self._calculate_cache_hit_ratio()
        avg_query_time_ms = self._connection_metrics.avg_query_time_ms

        pool_efficiency = {
            "queries_per_second": float(
                self._connection_metrics.query_count
                / max(self._connection_metrics.query_time_total, 0.001)
            ),
            "prepared_statements_cached": float(
                len(self._prepared_statement_cache._cache)
            ),
            "warmup_completed": float(1.0 if self._warmup_completed else 0.0),
        }

        recent_errors = []
        # Get recent errors if any are tracked
        if hasattr(self, "_recent_errors") and self._recent_errors:
            recent_errors = list(self._recent_errors)[-10:]  # Last 10 errors

        return PerformanceMetrics(
            cache_hit_rate=cache_hit_rate,
            avg_query_time_ms=avg_query_time_ms,
            connection_metrics=self._connection_metrics,
            pool_efficiency=pool_efficiency,
            recent_errors=recent_errors,
        )

    def _calculate_cache_hit_ratio(self) -> float:
        """Calculate prepared statement cache hit ratio."""
        if not self._prepared_statement_cache._usage_count:
            return 0.0

        total_requests = sum(self._prepared_statement_cache._usage_count.values())
        cache_hits = sum(
            1
            for count in self._prepared_statement_cache._usage_count.values()
            if count > 1
        )

        return cache_hits / max(total_requests, 1) * 100

    @property
    def is_connected(self) -> bool:
        """Check if client is connected to PostgreSQL."""
        return self._connected and self.pool is not None

    def get_connection_pool(self) -> Pool | None:
        """Get the connection pool if available.

        Returns:
            The asyncpg Pool if connected and available, None otherwise.

        This method provides a safe way to access the connection pool
        without requiring runtime attribute checking (hasattr).
        """
        return self.pool if self.is_connected else None

    async def execute(self, query: str, *args) -> Any:
        """Execute a SQL query without fetching results (alias for execute_query).

        This method is provided for API compatibility with code that expects
        an execute() method. It delegates to execute_query() with fetch=False.

        Args:
            query: SQL query to execute
            *args: Query parameters

        Returns:
            Query execution result (typically a status string like 'INSERT 0 1')
        """
        return await self.execute_query(query, *args, fetch=False, fetchrow=False)

    async def close(self) -> None:
        """Close the database connection (alias for disconnect).

        This method is provided for API compatibility with code that expects
        a close() method. It delegates to disconnect().
        """
        await self.disconnect()
