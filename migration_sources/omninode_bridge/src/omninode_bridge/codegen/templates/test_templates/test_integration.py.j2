#!/usr/bin/env python3
"""
Integration tests for {{ node_name }}.

Generated by NodeTestGeneratorEffect.
ONEX v2.0 Compliance: Tests end-to-end workflows with real dependencies.

Test Coverage Target: {{ test_contract.coverage_target }}%
Node Type: {{ node_type }}

NOTE: These tests require external services to be running.
"""

import pytest
import asyncio
from uuid import UUID, uuid4
from datetime import datetime, UTC

from omnibase_core.models.core import ModelContainer
from omnibase_core.models.contracts.model_contract_{{ node_type }} import ModelContract{{ node_type.capitalize() }}
from omnibase_core.models.contracts.model_io_operation_config import ModelIOOperationConfig
from omnibase_core.errors.model_onex_error import ModelOnexError
from omnibase_core.errors.error_codes import EnumCoreErrorCode
from omnibase_core.enums.enum_node_type import EnumNodeType

# Use relative import for standalone generated nodes
from ..node import {{ node_name }}


# ============================================================================
# INTEGRATION TEST MARKERS
# ============================================================================

pytestmark = [
    pytest.mark.integration,
    {% for marker in test_contract.test_configuration.pytest_markers %}
    pytest.mark.{{ marker }},
    {% endfor %}
]


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture(scope="module")
def integration_container():
    """
    Create container with REAL dependencies for integration testing.

    This fixture uses actual external services rather than mocks.
    Ensure required services are running before executing these tests.
    """
    config_dict = {}

    # Configure for integration testing
    {% if test_contract.test_configuration.use_test_database %}
    config_dict["database_host"] = "{{ test_contract.test_configuration.test_database_config.get('host', 'localhost') }}"
    config_dict["database_port"] = {{ test_contract.test_configuration.test_database_config.get('port', 5432) }}
    config_dict["database_name"] = "{{ test_contract.test_configuration.test_database_config.get('name', 'test_db') }}"
    {% endif %}

    {% if test_contract.mock_requirements.mock_kafka_producer == False %}
    # Configure real Kafka connection
    config_dict["kafka_enabled"] = True
    config_dict["kafka_bootstrap_servers"] = "localhost:9092"
    {% endif %}

    container = ModelContainer(
        value=config_dict,
        container_type="config"
    )

    return container


@pytest.fixture(scope="module")
def integration_node(integration_container):
    """Create node instance for integration testing."""
    return {{ node_name }}(integration_container)


@pytest.fixture
def integration_contract():
    """Create contract for integration testing."""
    return ModelContract{{ node_type.capitalize() }}(
        name="integration_test_contract",
        version={"major": 1, "minor": 0, "patch": 0},
        description="Integration test contract for {{ node_name }}",
        node_type=EnumNodeType.{{ node_type.upper() }},
        input_model="ModelTestInput",
        output_model="ModelTestOutput",
        tool_specification={
            "tool_name": "integration_test_tool",
            "main_tool_class": "{{ module_path }}.{{ node_name }}"
        },
        correlation_id=uuid4(),
        execution_id=uuid4(),
        {% if node_type == 'effect' %}
        io_operations=[
            ModelIOOperationConfig(
                operation_type="integration_test_operation",
                atomic=True,
                timeout_seconds=60
            )
        ],
        {% endif %}
    )


{% if test_contract.test_configuration.use_test_database %}
@pytest.fixture(scope="module")
async def setup_test_database(integration_container):
    """
    Setup test database schema and seed data.

    Runs before integration tests and cleans up after.
    """
    # Setup
    # TODO: Add database schema setup
    # TODO: Add seed data

    yield

    # Teardown
    # TODO: Add database cleanup
    pass
{% endif %}


# ============================================================================
# END-TO-END WORKFLOW TESTS
# ============================================================================

{% if test_contract.include_docstrings %}
@pytest.mark.asyncio
async def test_complete_workflow_success(integration_node, integration_contract{% if test_contract.test_configuration.use_test_database %}, setup_test_database{% endif %}):
    """
    Test: Complete end-to-end workflow executes successfully.

    Validates:
    - All workflow steps complete in order
    - External dependencies are called correctly
    - Data flows through entire pipeline
    - Final result is correct

    Workflow Steps:
    1. Initialize with contract
    2. Execute {{ node_type }} operation
    3. Verify result
    4. Validate side effects
    """
    # Arrange
    start_time = datetime.now(UTC)

    # Act
    result = await integration_node.execute_{{ node_type }}(integration_contract)

    # Assert
    assert result is not None, "Workflow should complete with result"
    assert isinstance(result, dict), "Result should be a dictionary"

    # Verify timing
    execution_time = (datetime.now(UTC) - start_time).total_seconds()
    timeout_seconds = {{ test_contract.test_configuration.timeout_seconds | default(60) }}
    assert execution_time < timeout_seconds, \
        f"Workflow should complete within {timeout_seconds}s, took {execution_time}s"

    # TODO: Add workflow-specific assertions


@pytest.mark.asyncio
async def test_workflow_with_multiple_operations(integration_node, integration_contract):
    """
    Test: Workflow handles multiple sequential operations.

    Validates:
    - Multiple operations execute in sequence
    - State is maintained across operations
    - Each operation produces expected results
    """
    # Arrange
    operations = [
        {"operation": "step_1", "data": "test_data_1"},
        {"operation": "step_2", "data": "test_data_2"},
        {"operation": "step_3", "data": "test_data_3"},
    ]

    # Act
    results = []
    for operation in operations:
        # Update contract with operation-specific data
        result = await integration_node.execute_{{ node_type }}(integration_contract)
        results.append(result)

    # Assert
    assert len(results) == len(operations), "All operations should complete"
    # TODO: Add operation-specific assertions


@pytest.mark.asyncio
@pytest.mark.skip(reason="Error recovery test not yet implemented - requires transient error scenario setup")
async def test_workflow_error_recovery(integration_node, integration_contract):
    """
    Test: Workflow recovers from transient errors.

    Validates:
    - Retry logic works correctly
    - Partial failures don't corrupt state
    - Error details are captured

    Implementation needed:
    - Setup transient error scenario (e.g., temporary network failure)
    - Configure retry mechanism
    - Verify recovery and state integrity
    """
    # Arrange
    # Configure for transient error scenario

    # Act & Assert
    # IMPLEMENTATION REQUIRED: Add error recovery test logic
    pass
{% else %}
@pytest.mark.asyncio
async def test_complete_workflow_success(integration_node, integration_contract{% if test_contract.test_configuration.use_test_database %}, setup_test_database{% endif %}):
    start_time = datetime.now(UTC)
    result = await integration_node.execute_{{ node_type }}(integration_contract)

    assert result is not None
    assert isinstance(result, dict)

    execution_time = (datetime.now(UTC) - start_time).total_seconds()
    timeout_seconds = {{ test_contract.test_configuration.timeout_seconds | default(60) }}
    assert execution_time < timeout_seconds


@pytest.mark.asyncio
async def test_workflow_with_multiple_operations(integration_node, integration_contract):
    operations = [
        {"operation": "step_1", "data": "test_data_1"},
        {"operation": "step_2", "data": "test_data_2"},
        {"operation": "step_3", "data": "test_data_3"},
    ]

    results = []
    for operation in operations:
        result = await integration_node.execute_{{ node_type }}(integration_contract)
        results.append(result)

    assert len(results) == len(operations)


@pytest.mark.asyncio
@pytest.mark.skip(reason="Error recovery test not yet implemented - requires transient error scenario setup")
async def test_workflow_error_recovery(integration_node, integration_contract):
    # IMPLEMENTATION REQUIRED: Add error recovery test logic
    # See detailed docstring in the with-docstrings version above
    pass
{% endif %}


# ============================================================================
# EXTERNAL SERVICE INTEGRATION TESTS
# ============================================================================

{% if test_contract.mock_requirements.mock_external_services | length > 0 %}
{% for service in test_contract.mock_requirements.mock_external_services %}
@pytest.mark.asyncio
async def test_{{ service | replace('.', '_') | lower }}_integration(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Integration with {{ service }} service.

    Validates:
    - Connection to {{ service }} succeeds
    - Request/response cycle works correctly
    - Error handling for {{ service }} failures
    """
{% endif %}
    # Arrange
    # Configure for {{ service }} testing

    # Act
    result = await integration_node.execute_{{ node_type }}(integration_contract)

    # Assert
    assert result is not None
    # TODO: Add {{ service }}-specific assertions


{% endfor %}
{% endif %}

{% if test_contract.mock_requirements.mock_database %}
@pytest.mark.asyncio
async def test_database_integration(integration_node, integration_contract, setup_test_database):
{% if test_contract.include_docstrings %}
    """
    Test: Database integration with real database connection.

    Validates:
    - Database queries execute successfully
    - Transactions commit correctly
    - Data persistence works
    - Rollback on errors
    """
{% endif %}
    # Arrange
    test_data = {
        "id": str(uuid4()),
        "name": "test_entity",
        "timestamp": datetime.now(UTC),
    }

    # Act
    result = await integration_node.execute_{{ node_type }}(integration_contract)

    # Assert
    assert result is not None
    # TODO: Verify database state
    # TODO: Verify data was persisted


@pytest.mark.skip(reason="Requires implementation to trigger transaction failure - add error simulation")
@pytest.mark.asyncio
async def test_database_transaction_rollback(integration_node, integration_contract, setup_test_database):
{% if test_contract.include_docstrings %}
    """
    Test: Database transaction rollback on error.

    Validates:
    - Failed operations rollback correctly
    - No partial data is committed
    - Database state remains consistent

    IMPLEMENTATION REQUIRED: Add error simulation to trigger transaction failure.
    Example:
        # Mock database operation to raise error during transaction
        with patch.object(integration_node, 'db_client', side_effect=Exception("Simulated DB error")):
            # This will trigger transaction rollback
    """
{% endif %}
    # Arrange
    # IMPLEMENTATION REQUIRED: Configure for transaction failure scenario
    # Example: Mock database client or inject invalid data to trigger constraint violation

    # Act & Assert
    with pytest.raises(ModelOnexError):
        await integration_node.execute_{{ node_type }}(integration_contract)

    # IMPLEMENTATION REQUIRED: Verify database state was rolled back
    # Example: Query database to ensure no partial data exists
{% endif %}

{% if test_contract.mock_requirements.mock_kafka_producer %}
@pytest.mark.asyncio
async def test_kafka_integration(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Kafka event publishing integration.

    Validates:
    - Events are published to correct topics
    - Event format is correct
    - Delivery confirmation works
    """
{% endif %}
    # Arrange
    expected_topic = "test_topic"

    # Act
    result = await integration_node.execute_{{ node_type }}(integration_contract)

    # Assert
    assert result is not None
    # TODO: Verify Kafka events were published
    # TODO: Verify event payload
{% endif %}


# ============================================================================
# CONCURRENT EXECUTION TESTS
# ============================================================================

{% if test_contract.test_configuration.parallel_execution %}
@pytest.mark.asyncio
async def test_concurrent_workflow_execution(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Multiple concurrent workflow executions.

    Validates:
    - Node handles concurrent requests correctly
    - No race conditions or deadlocks
    - Results are correct for all executions
    """
{% endif %}
    # Arrange
    num_concurrent = {{ test_contract.test_configuration.parallel_workers }}
    contracts = [
        ModelContract{{ node_type.capitalize() }}(
            name=f"concurrent_test_{i}",
            version={"major": 1, "minor": 0, "patch": 0},
            description=f"Concurrent test contract {i}",
            node_type=EnumNodeType.{{ node_type.upper() }},
            input_model="ModelTestInput",
            output_model="ModelTestOutput",
            tool_specification={
                "tool_name": f"concurrent_test_tool_{i}",
                "main_tool_class": "{{ module_path }}.{{ node_name }}"
            },
            correlation_id=uuid4(),
            {% if node_type == 'effect' %}
            io_operations=[
                ModelIOOperationConfig(
                    operation_type="concurrent_test_operation",
                    atomic=True,
                    timeout_seconds=60
                )
            ],
            {% endif %}
        )
        for i in range(num_concurrent)
    ]

    # Act
    tasks = [
        integration_node.execute_{{ node_type }}(contract)
        for contract in contracts
    ]
    results = await asyncio.gather(*tasks)

    # Assert
    assert len(results) == num_concurrent, "All concurrent executions should complete"
    assert all(r is not None for r in results), "All results should be valid"
{% endif %}


# ============================================================================
# DATA PERSISTENCE TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_data_persistence_across_executions(integration_node, integration_contract{% if test_contract.test_configuration.use_test_database %}, setup_test_database{% endif %}):
{% if test_contract.include_docstrings %}
    """
    Test: Data persists correctly across multiple executions.

    Validates:
    - First execution creates data
    - Second execution can read persisted data
    - Data integrity is maintained
    """
{% endif %}
    # Arrange
    test_id = str(uuid4())

    # Act - First execution (create)
    result1 = await integration_node.execute_{{ node_type }}(integration_contract)

    # Act - Second execution (read)
    result2 = await integration_node.execute_{{ node_type }}(integration_contract)

    # Assert
    assert result1 is not None
    assert result2 is not None
    # TODO: Verify data was persisted and retrieved correctly


# ============================================================================
# ERROR HANDLING INTEGRATION TESTS
# ============================================================================

@pytest.mark.skip(reason="Requires implementation to trigger timeout - add timeout simulation")
@pytest.mark.asyncio
async def test_external_service_timeout_handling(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Graceful handling of external service timeouts.

    Validates:
    - Timeout errors are caught and handled
    - Proper error response is returned
    - System remains stable after timeout

    IMPLEMENTATION REQUIRED: Add timeout simulation to trigger error condition.
    Example:
        # Mock external service to simulate timeout
        with patch.object(integration_node, 'external_client') as mock_client:
            mock_client.call.side_effect = asyncio.TimeoutError("Simulated timeout")
            # This will trigger timeout error handling
    """
{% endif %}
    # Arrange
    # IMPLEMENTATION REQUIRED: Configure for timeout scenario
    # Example: Mock external service calls to raise TimeoutError after delay

    # Act & Assert
    with pytest.raises(ModelOnexError) as exc_info:
        await integration_node.execute_{{ node_type }}(integration_contract)

    assert exc_info.value.error_code == EnumCoreErrorCode.EXECUTION_ERROR
    # IMPLEMENTATION REQUIRED: Verify timeout was handled gracefully
    # Example: Check that circuit breaker opened, retries occurred, etc.


@pytest.mark.asyncio
@pytest.mark.skip(reason="Network failure test not yet implemented - requires network failure simulation")
async def test_network_failure_handling(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Graceful handling of network failures.

    Validates:
    - Network errors are caught and handled
    - Retry logic works correctly
    - Circuit breaker pattern (if applicable)

    Implementation needed:
    - Setup network failure simulation (e.g., mock connection timeout)
    - Verify proper error handling and retries
    - Test circuit breaker if applicable
    """
{% endif %}
    # Arrange
    # Configure for network failure scenario

    # Act & Assert
    # IMPLEMENTATION REQUIRED: Add network failure test logic
    pass


# ============================================================================
# PERFORMANCE INTEGRATION TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_workflow_performance_under_load(integration_node, integration_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Workflow performance under realistic load.

    Validates:
    - Response time meets requirements
    - Throughput is acceptable
    - No performance degradation over time
    """
{% endif %}
    # Arrange
    num_requests = 100
    start_time = datetime.now(UTC)

    # Act
    tasks = [
        integration_node.execute_{{ node_type }}(integration_contract)
        for _ in range(num_requests)
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Assert
    execution_time = (datetime.now(UTC) - start_time).total_seconds()
    throughput = num_requests / execution_time

    successful_results = [r for r in results if not isinstance(r, Exception)]
    success_rate = len(successful_results) / num_requests

    assert success_rate >= 0.95, f"Success rate should be >= 95%, got {success_rate:.2%}"
    # TODO: Add throughput assertions


# ============================================================================
# CLEANUP
# ============================================================================

@pytest.fixture(scope="module", autouse=True)
async def cleanup_after_tests(integration_container):
    """
    Cleanup resources after all integration tests complete.

    This fixture runs automatically after all tests in this module.

    To implement:
    - Close database connections
    - Close Kafka/event bus connections
    - Clean up test data
    - Release any other resources

    Example:
        ```python
        yield

        # Close database connection
        if hasattr(integration_container, 'db_connection'):
            await integration_container.db_connection.close()

        # Close Kafka producer
        if hasattr(integration_container, 'kafka_producer'):
            await integration_container.kafka_producer.stop()
        ```
    """
    yield

    # IMPLEMENTATION REQUIRED: Add cleanup logic here
    # Most cleanup is handled automatically by fixture teardown,
    # but add any explicit cleanup needed for integration resources
