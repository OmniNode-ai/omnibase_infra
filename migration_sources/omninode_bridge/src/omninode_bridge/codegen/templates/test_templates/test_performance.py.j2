#!/usr/bin/env python3
"""
Performance tests for {{ node_name }}.

Generated by NodeTestGeneratorEffect.
ONEX v2.0 Compliance: Validates performance thresholds and benchmarks.

Performance Targets:
{% if performance_thresholds %}
{% for metric, threshold in performance_thresholds.items() %}
- {{ metric }}: {{ threshold }}
{% endfor %}
{% endif %}

Node Type: {{ node_type }}
"""

import pytest
import asyncio
import time
import statistics
from datetime import datetime, UTC, timedelta
from uuid import uuid4
from typing import List, Dict
from dataclasses import dataclass

from omnibase_core.models.core import ModelContainer
from omnibase_core.models.contracts.model_contract_{{ node_type }} import ModelContract{{ node_type.capitalize() }}

from {{ module_path }} import {{ node_name }}


# ============================================================================
# PERFORMANCE TEST MARKERS
# ============================================================================

pytestmark = [
    pytest.mark.performance,
    pytest.mark.slow,
    {% for marker in test_contract.test_configuration.pytest_markers %}
    pytest.mark.{{ marker }},
    {% endfor %}
]


# ============================================================================
# PERFORMANCE THRESHOLDS
# ============================================================================

# Execution time thresholds (milliseconds)
EXECUTION_TIME_TARGET_MS = {{ performance_thresholds.get('execution_time_target_ms', 100) }}
EXECUTION_TIME_MAX_MS = {{ performance_thresholds.get('execution_time_max_ms', 1000) }}

# Throughput thresholds (requests per second)
THROUGHPUT_MIN_RPS = {{ performance_thresholds.get('throughput_min_rps', 100) }}
THROUGHPUT_TARGET_RPS = {{ performance_thresholds.get('throughput_target_rps', 500) }}

# Memory thresholds (MB)
MEMORY_TARGET_MB = {{ performance_thresholds.get('memory_target_mb', 128) }}
MEMORY_MAX_MB = {{ performance_thresholds.get('memory_max_mb', 512) }}

# Concurrency thresholds
MAX_CONCURRENT_REQUESTS = {{ performance_thresholds.get('max_concurrent_requests', 100) }}

# Latency percentiles (milliseconds)
P50_LATENCY_MS = {{ performance_thresholds.get('p50_latency_ms', 50) }}
P95_LATENCY_MS = {{ performance_thresholds.get('p95_latency_ms', 200) }}
P99_LATENCY_MS = {{ performance_thresholds.get('p99_latency_ms', 500) }}


# ============================================================================
# PERFORMANCE MEASUREMENT UTILITIES
# ============================================================================

@dataclass
class PerformanceMetrics:
    """Container for performance measurement results."""
    duration_ms: float
    throughput_rps: float
    latencies_ms: List[float]
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    min_latency_ms: float
    max_latency_ms: float
    avg_latency_ms: float
    success_count: int
    error_count: int
    success_rate: float


def calculate_percentile(values: List[float], percentile: float) -> float:
    """Calculate percentile from list of values."""
    if not values:
        return 0.0
    return statistics.quantiles(sorted(values), n=100)[int(percentile) - 1]


async def measure_performance(
    node: {{ node_name }},
    contract: ModelContract{{ node_type.capitalize() }},
    num_requests: int,
    concurrent: bool = False,
    max_concurrency: int = 10
) -> PerformanceMetrics:
    """
    Measure performance metrics for node execution.

    Args:
        node: Node instance to test
        contract: Contract for execution
        num_requests: Number of requests to execute
        concurrent: Whether to execute requests concurrently
        max_concurrency: Maximum concurrent requests

    Returns:
        PerformanceMetrics with measured values
    """
    latencies = []
    errors = 0
    start_time = time.perf_counter()

    if concurrent:
        # Concurrent execution with semaphore
        semaphore = asyncio.Semaphore(max_concurrency)

        async def execute_with_semaphore():
            async with semaphore:
                req_start = time.perf_counter()
                try:
                    await node.execute_{{ node_type }}(contract)
                    req_end = time.perf_counter()
                    return (req_end - req_start) * 1000  # Convert to ms
                except Exception:
                    return None

        tasks = [execute_with_semaphore() for _ in range(num_requests)]
        results = await asyncio.gather(*tasks)

        for result in results:
            if result is not None:
                latencies.append(result)
            else:
                errors += 1
    else:
        # Sequential execution
        for _ in range(num_requests):
            req_start = time.perf_counter()
            try:
                await node.execute_{{ node_type }}(contract)
                req_end = time.perf_counter()
                latencies.append((req_end - req_start) * 1000)
            except Exception:
                errors += 1

    end_time = time.perf_counter()
    duration_ms = (end_time - start_time) * 1000

    # Calculate metrics
    success_count = len(latencies)
    total_count = num_requests
    success_rate = success_count / total_count if total_count > 0 else 0.0
    throughput_rps = (success_count / duration_ms) * 1000 if duration_ms > 0 else 0.0

    return PerformanceMetrics(
        duration_ms=duration_ms,
        throughput_rps=throughput_rps,
        latencies_ms=latencies,
        p50_latency_ms=calculate_percentile(latencies, 50) if latencies else 0.0,
        p95_latency_ms=calculate_percentile(latencies, 95) if latencies else 0.0,
        p99_latency_ms=calculate_percentile(latencies, 99) if latencies else 0.0,
        min_latency_ms=min(latencies) if latencies else 0.0,
        max_latency_ms=max(latencies) if latencies else 0.0,
        avg_latency_ms=statistics.mean(latencies) if latencies else 0.0,
        success_count=success_count,
        error_count=errors,
        success_rate=success_rate,
    )


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture
def performance_container():
    """Create container optimized for performance testing."""
    config_dict = {}
    # Configure for performance testing
    {% if test_contract.mock_requirements.mock_database %}
    config_dict["database_enabled"] = False  # Use mocks for performance tests
    {% endif %}
    container = ModelContainer(value=config_dict, container_type="config")
    return container


@pytest.fixture
def performance_node(performance_container):
    """Create node instance for performance testing."""
    return {{ node_name }}(performance_container)


@pytest.fixture
def performance_contract():
    """Create contract for performance testing."""
    return ModelContract{{ node_type.capitalize() }}(
        name="performance_test_contract",
        version={"major": 1, "minor": 0, "patch": 0},
        description="Performance test contract for {{ node_name }}",
        node_type="{{ node_type.upper() }}",
        input_model="ModelTestInput",
        output_model="ModelTestOutput",
        tool_specification={
            "tool_name": "performance_test_tool",
            "main_tool_class": "{{ module_path }}.{{ node_name }}"
        },
        correlation_id=uuid4(),
    )


# ============================================================================
# EXECUTION TIME TESTS
# ============================================================================

{% if test_contract.include_docstrings %}
@pytest.mark.asyncio
async def test_single_execution_time(performance_node, performance_contract):
    """
    Test: Single execution completes within time threshold.

    Validates:
    - Execution time < {{ performance_thresholds.get('execution_time_max_ms', 1000) }}ms
    - Target time < {{ performance_thresholds.get('execution_time_target_ms', 100) }}ms (warning if exceeded)
    """
    # Warmup
    await performance_node.execute_{{ node_type }}(performance_contract)

    # Measure
    start_time = time.perf_counter()
    await performance_node.execute_{{ node_type }}(performance_contract)
    end_time = time.perf_counter()

    execution_time_ms = (end_time - start_time) * 1000

    # Assert hard threshold
    assert execution_time_ms < EXECUTION_TIME_MAX_MS, \
        f"Execution time {execution_time_ms:.2f}ms exceeds maximum {EXECUTION_TIME_MAX_MS}ms"

    # Warn on target threshold
    if execution_time_ms > EXECUTION_TIME_TARGET_MS:
        pytest.warns(
            UserWarning,
            f"Execution time {execution_time_ms:.2f}ms exceeds target {EXECUTION_TIME_TARGET_MS}ms"
        )


@pytest.mark.asyncio
async def test_average_execution_time(performance_node, performance_contract):
    """
    Test: Average execution time across multiple runs.

    Validates:
    - Average execution time meets targets
    - Consistent performance across runs
    """
    num_runs = 100
    execution_times = []

    # Warmup
    await performance_node.execute_{{ node_type }}(performance_contract)

    # Measure multiple runs
    for _ in range(num_runs):
        start_time = time.perf_counter()
        await performance_node.execute_{{ node_type }}(performance_contract)
        end_time = time.perf_counter()
        execution_times.append((end_time - start_time) * 1000)

    avg_time_ms = statistics.mean(execution_times)
    std_dev_ms = statistics.stdev(execution_times)

    assert avg_time_ms < EXECUTION_TIME_TARGET_MS, \
        f"Average execution time {avg_time_ms:.2f}ms exceeds target {EXECUTION_TIME_TARGET_MS}ms"

    # Verify consistency (standard deviation should be small)
    assert std_dev_ms < (avg_time_ms * 0.2), \
        f"High variance in execution time (std dev: {std_dev_ms:.2f}ms)"
{% else %}
@pytest.mark.asyncio
async def test_single_execution_time(performance_node, performance_contract):
    await performance_node.execute_{{ node_type }}(performance_contract)

    start_time = time.perf_counter()
    await performance_node.execute_{{ node_type }}(performance_contract)
    end_time = time.perf_counter()

    execution_time_ms = (end_time - start_time) * 1000

    assert execution_time_ms < EXECUTION_TIME_MAX_MS


@pytest.mark.asyncio
async def test_average_execution_time(performance_node, performance_contract):
    num_runs = 100
    execution_times = []

    await performance_node.execute_{{ node_type }}(performance_contract)

    for _ in range(num_runs):
        start_time = time.perf_counter()
        await performance_node.execute_{{ node_type }}(performance_contract)
        end_time = time.perf_counter()
        execution_times.append((end_time - start_time) * 1000)

    avg_time_ms = statistics.mean(execution_times)
    std_dev_ms = statistics.stdev(execution_times)

    assert avg_time_ms < EXECUTION_TIME_TARGET_MS
    assert std_dev_ms < (avg_time_ms * 0.2)
{% endif %}


# ============================================================================
# THROUGHPUT TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_sequential_throughput(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Sequential throughput meets minimum requirements.

    Validates:
    - Throughput >= {{ performance_thresholds.get('throughput_min_rps', 100) }} RPS
    - No errors during sustained load
    """
{% endif %}
    num_requests = 1000

    metrics = await measure_performance(
        performance_node,
        performance_contract,
        num_requests,
        concurrent=False
    )

    assert metrics.throughput_rps >= THROUGHPUT_MIN_RPS, \
        f"Throughput {metrics.throughput_rps:.2f} RPS below minimum {THROUGHPUT_MIN_RPS} RPS"

    assert metrics.success_rate >= 0.99, \
        f"Success rate {metrics.success_rate:.2%} below 99%"


@pytest.mark.asyncio
async def test_concurrent_throughput(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Concurrent throughput with controlled concurrency.

    Validates:
    - Throughput with concurrency >= target
    - High success rate under concurrent load
    - Latency remains acceptable
    """
{% endif %}
    num_requests = 1000
    max_concurrency = 50

    metrics = await measure_performance(
        performance_node,
        performance_contract,
        num_requests,
        concurrent=True,
        max_concurrency=max_concurrency
    )

    assert metrics.throughput_rps >= THROUGHPUT_TARGET_RPS, \
        f"Concurrent throughput {metrics.throughput_rps:.2f} RPS below target {THROUGHPUT_TARGET_RPS} RPS"

    assert metrics.success_rate >= 0.95, \
        f"Success rate {metrics.success_rate:.2%} below 95% under concurrent load"


# ============================================================================
# LATENCY PERCENTILE TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_latency_percentiles(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Latency percentiles meet requirements.

    Validates:
    - P50 latency < {{ performance_thresholds.get('p50_latency_ms', 50) }}ms
    - P95 latency < {{ performance_thresholds.get('p95_latency_ms', 200) }}ms
    - P99 latency < {{ performance_thresholds.get('p99_latency_ms', 500) }}ms
    """
{% endif %}
    num_requests = 1000

    metrics = await measure_performance(
        performance_node,
        performance_contract,
        num_requests,
        concurrent=True,
        max_concurrency=50
    )

    assert metrics.p50_latency_ms < P50_LATENCY_MS, \
        f"P50 latency {metrics.p50_latency_ms:.2f}ms exceeds {P50_LATENCY_MS}ms"

    assert metrics.p95_latency_ms < P95_LATENCY_MS, \
        f"P95 latency {metrics.p95_latency_ms:.2f}ms exceeds {P95_LATENCY_MS}ms"

    assert metrics.p99_latency_ms < P99_LATENCY_MS, \
        f"P99 latency {metrics.p99_latency_ms:.2f}ms exceeds {P99_LATENCY_MS}ms"


# ============================================================================
# LOAD TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_sustained_load(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Performance under sustained load.

    Validates:
    - Performance remains stable over time
    - No memory leaks
    - No degradation in throughput
    """
{% endif %}
    duration_seconds = 60
    requests_per_second = 100
    total_requests = duration_seconds * requests_per_second

    start_time = time.perf_counter()

    metrics = await measure_performance(
        performance_node,
        performance_contract,
        total_requests,
        concurrent=True,
        max_concurrency=10
    )

    actual_duration = (time.perf_counter() - start_time)

    # Verify sustained throughput
    assert metrics.throughput_rps >= (requests_per_second * 0.9), \
        f"Sustained throughput {metrics.throughput_rps:.2f} RPS below 90% of target"

    # Verify high success rate
    assert metrics.success_rate >= 0.95, \
        f"Success rate {metrics.success_rate:.2%} degraded under sustained load"


@pytest.mark.asyncio
async def test_spike_load(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Handling sudden load spikes.

    Validates:
    - System handles sudden increase in load
    - No cascading failures
    - Recovery after spike
    """
{% endif %}
    # Baseline load
    baseline_metrics = await measure_performance(
        performance_node,
        performance_contract,
        100,
        concurrent=True,
        max_concurrency=10
    )

    # Spike load (10x increase)
    spike_metrics = await measure_performance(
        performance_node,
        performance_contract,
        1000,
        concurrent=True,
        max_concurrency=100
    )

    # Verify spike handling
    assert spike_metrics.success_rate >= 0.90, \
        f"Success rate {spike_metrics.success_rate:.2%} dropped too much during spike"

    # Latency should not increase by more than 5x
    assert spike_metrics.p95_latency_ms < (baseline_metrics.p95_latency_ms * 5), \
        "Latency increased too much during spike"


# ============================================================================
# CONCURRENCY TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_maximum_concurrency(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: Maximum supported concurrency.

    Validates:
    - System handles maximum concurrent requests
    - No deadlocks or race conditions
    - All requests complete successfully
    """
{% endif %}
    num_requests = MAX_CONCURRENT_REQUESTS

    metrics = await measure_performance(
        performance_node,
        performance_contract,
        num_requests,
        concurrent=True,
        max_concurrency=num_requests  # All at once
    )

    assert metrics.success_rate >= 0.95, \
        f"Success rate {metrics.success_rate:.2%} below 95% at max concurrency"

    assert metrics.error_count == 0 or metrics.error_count < (num_requests * 0.05), \
        f"Too many errors ({metrics.error_count}) at max concurrency"


# ============================================================================
# MEMORY TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_memory_usage():
{% if test_contract.include_docstrings %}
    """
    Test: Memory usage under load.

    Validates:
    - Memory usage stays within limits
    - No memory leaks
    - Memory released after operations
    """
{% endif %}
    pytest.skip("Memory profiling requires additional setup")
    # TODO: Implement memory profiling
    # - Use tracemalloc or memory_profiler
    # - Measure baseline memory
    # - Execute operations
    # - Measure peak memory
    # - Verify memory is released


# ============================================================================
# PERFORMANCE REGRESSION TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_performance_regression(performance_node, performance_contract):
{% if test_contract.include_docstrings %}
    """
    Test: No performance regression from baseline.

    Validates:
    - Current performance >= baseline
    - No unexpected slowdowns
    """
{% endif %}
    # TODO: Load baseline metrics from file
    # TODO: Compare current metrics to baseline
    # TODO: Assert no significant regression
    pytest.skip("Baseline metrics not yet established")


# ============================================================================
# REPORTING
# ============================================================================

def print_performance_summary(metrics: PerformanceMetrics, test_name: str):
    """Print performance metrics summary."""
    print(f"\n{'='*60}")
    print(f"Performance Summary: {test_name}")
    print(f"{'='*60}")
    print(f"Throughput:      {metrics.throughput_rps:.2f} RPS")
    print(f"Success Rate:    {metrics.success_rate:.2%}")
    print(f"Total Requests:  {metrics.success_count + metrics.error_count}")
    print(f"Successful:      {metrics.success_count}")
    print(f"Errors:          {metrics.error_count}")
    print(f"\nLatency Metrics (ms):")
    print(f"  Min:    {metrics.min_latency_ms:.2f}")
    print(f"  P50:    {metrics.p50_latency_ms:.2f}")
    print(f"  P95:    {metrics.p95_latency_ms:.2f}")
    print(f"  P99:    {metrics.p99_latency_ms:.2f}")
    print(f"  Max:    {metrics.max_latency_ms:.2f}")
    print(f"  Avg:    {metrics.avg_latency_ms:.2f}")
    print(f"{'='*60}\n")
