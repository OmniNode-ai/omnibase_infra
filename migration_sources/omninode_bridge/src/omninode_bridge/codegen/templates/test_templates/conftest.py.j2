#!/usr/bin/env python3
"""
Pytest configuration and shared fixtures for {{ node_name }} tests.

Generated by NodeTestGeneratorEffect.
ONEX v2.0 Compliance: Provides reusable test fixtures and configuration.

This file is automatically loaded by pytest and provides shared fixtures
for all test modules in this directory.
"""

import pytest
import asyncio
import logging
from typing import AsyncGenerator, Generator
from uuid import uuid4
from pathlib import Path
from datetime import datetime, UTC

from omnibase_core.models.core import ModelContainer
from omnibase_core.models.contracts.model_contract_{{ node_type }} import ModelContract{{ node_type.capitalize() }}
from omnibase_core.models.contracts.model_io_operation_config import ModelIOOperationConfig
from omnibase_core.enums.enum_node_type import EnumNodeType

# Use relative import for standalone generated nodes
from ..node import {{ node_name }}


# ============================================================================
# PYTEST CONFIGURATION
# ============================================================================

def pytest_configure(config):
    """
    Configure pytest with custom markers and settings.
    """
    # Standard ONEX markers (always registered)
    standard_markers = {
        "unit": "Unit tests with mocked dependencies",
        "integration": "Integration tests with real dependencies",
        "contract": "Contract compliance tests",
        "performance": "Performance and load tests",
        "slow": "Slow-running tests",
    }

    for marker, description in standard_markers.items():
        config.addinivalue_line("markers", f"{marker}: {description}")

    # Custom markers from test configuration (avoid duplicates)
    {% for marker in test_contract.test_configuration.pytest_markers %}
    {% if marker not in ["unit", "integration", "contract", "performance", "slow"] %}
    config.addinivalue_line(
        "markers",
        "{{ marker }}: Custom marker for {{ marker }} tests"
    )
    {% endif %}
    {% endfor %}


def pytest_collection_modifyitems(config, items):
    """
    Modify test collection to add markers based on test names.
    """
    for item in items:
        # Add markers based on test file names
        if "test_unit" in str(item.fspath):
            item.add_marker(pytest.mark.unit)
        if "test_integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)
        if "test_contract" in str(item.fspath):
            item.add_marker(pytest.mark.contract)
        if "test_performance" in str(item.fspath):
            item.add_marker(pytest.mark.performance)
            item.add_marker(pytest.mark.slow)


# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

@pytest.fixture(scope="session", autouse=True)
def configure_logging():
    """Configure logging for test execution."""
    logging.basicConfig(
        level=logging.{% if test_contract.test_configuration.verbose_output %}DEBUG{% else %}INFO{% endif %},
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # Suppress noisy loggers
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)


# ============================================================================
# EVENT LOOP CONFIGURATION
# ============================================================================

@pytest.fixture(scope="session")
def event_loop():
    """
    Create event loop for async tests.

    Provides a session-scoped event loop for all async tests.
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    yield loop
    loop.close()


# ============================================================================
# CORE FIXTURES
# ============================================================================

@pytest.fixture
def test_container() -> ModelContainer:
    """
    Create test container with default configuration.

    This fixture provides a fresh container for each test with
    minimal configuration suitable for most tests.
    """
    config_dict = {
        "environment": "test",
        "log_level": "{% if test_contract.test_configuration.verbose_output %}DEBUG{% else %}INFO{% endif %}",
        {% if test_contract.test_configuration.environment_variables %}
        # Environment variables from configuration
        {% for key, value in test_contract.test_configuration.environment_variables.items() %}
        "{{ key }}": "{{ value }}",
        {% endfor %}
        {% endif %}
    }

    container = ModelContainer(
        value=config_dict,
        container_type="config"
    )

    return container


@pytest.fixture
def test_node(test_container: ModelContainer) -> {{ node_name }}:
    """
    Create test node instance.

    Provides a fresh node instance for each test.
    """
    return {{ node_name }}(test_container)


@pytest.fixture
def test_contract() -> ModelContract{{ node_type.capitalize() }}:
    """
    Create test contract with standard configuration.

    Provides a valid contract for testing node execution.
    """
    return ModelContract{{ node_type.capitalize() }}(
        name="test_contract",
        version={"major": 1, "minor": 0, "patch": 0},
        description="Test contract for {{ node_name }}",
        node_type=EnumNodeType.{{ node_type.upper() }},
        input_model="ModelTestInput",
        output_model="ModelTestOutput",
        tool_specification={
            "tool_name": "test_tool",
            "main_tool_class": "{{ module_path }}.{{ node_name }}"
        },
        correlation_id=uuid4(),
        execution_id=uuid4(),
        {% if node_type == 'effect' %}
        io_operations=[
            ModelIOOperationConfig(
                operation_type="test_operation",
                atomic=True,
                timeout_seconds=30
            )
        ],
        {% endif %}
    )


# ============================================================================
# MOCK FIXTURES
# ============================================================================

{% if test_contract.mock_requirements.mock_dependencies %}
{% for dependency in test_contract.mock_requirements.mock_dependencies %}
@pytest.fixture
def mock_{{ dependency.split('.')[-1] | lower }}():
    """
    Mock for {{ dependency }}.

    Provides a configured mock for {{ dependency }} dependency.
    """
    from unittest.mock import {% if test_contract.use_async_tests %}AsyncMock{% else %}Mock{% endif %}

    mock = {% if test_contract.use_async_tests %}AsyncMock{% else %}Mock{% endif %}()

    {% if dependency.split('.')[-1] | lower in test_contract.mock_requirements.mock_return_values %}
    # Configure return values
    mock.return_value = {{ test_contract.mock_requirements.mock_return_values[dependency.split('.')[-1] | lower] }}
    {% endif %}

    return mock


{% endfor %}
{% endif %}

{% if test_contract.mock_requirements.mock_database %}
@pytest.fixture
def mock_database():
    """
    Mock database connection and cursor.

    Provides a configured mock for database operations.
    """
    from unittest.mock import AsyncMock, MagicMock

    mock_connection = AsyncMock()
    mock_cursor = AsyncMock()

    mock_connection.cursor.return_value = mock_cursor
    mock_cursor.fetchone.return_value = None
    mock_cursor.fetchall.return_value = []
    mock_cursor.execute.return_value = None

    return {
        "connection": mock_connection,
        "cursor": mock_cursor,
    }
{% endif %}

{% if test_contract.mock_requirements.mock_kafka_producer %}
@pytest.fixture
def mock_kafka_producer():
    """
    Mock Kafka producer.

    Provides a configured mock for Kafka event publishing.
    """
    from unittest.mock import AsyncMock

    mock = AsyncMock()
    mock.send.return_value = AsyncMock()
    mock.send.return_value.get.return_value = {"topic": "test_topic", "partition": 0, "offset": 123}

    return mock
{% endif %}

{% if test_contract.mock_requirements.mock_http_clients %}
@pytest.fixture
def mock_http_client():
    """
    Mock HTTP client.

    Provides a configured mock for HTTP requests.
    """
    from unittest.mock import AsyncMock

    mock = AsyncMock()
    mock.get.return_value.status_code = 200
    mock.get.return_value.json.return_value = {"status": "success"}

    mock.post.return_value.status_code = 201
    mock.post.return_value.json.return_value = {"id": "test_id"}

    return mock
{% endif %}

{% if test_contract.mock_requirements.mock_datetime %}
@pytest.fixture
def mock_datetime(monkeypatch):
    """
    Mock datetime for deterministic time-based tests.

    Provides a fixed datetime for testing time-dependent behavior.
    """
    from datetime import datetime, UTC
    from unittest.mock import Mock

    fixed_time = datetime(2024, 1, 1, 12, 0, 0, tzinfo=UTC)

    mock_dt = Mock()
    mock_dt.now.return_value = fixed_time

    monkeypatch.setattr("datetime.datetime", mock_dt)

    return fixed_time
{% endif %}


# ============================================================================
# DATA FIXTURES
# ============================================================================

@pytest.fixture
def test_data_directory() -> Path:
    """
    Path to test data directory.

    Provides access to test data files.
    Uses Path(__file__).parent for directory-independent path resolution.
    """
    return Path(__file__).parent / "data"


@pytest.fixture
def sample_test_data() -> dict:
    """
    Sample test data for common scenarios.

    Provides predefined test data for standard use cases.
    """
    return {
        "test_id": str(uuid4()),
        "timestamp": datetime.now(UTC).isoformat(),
        "status": "active",
        "metadata": {
            "environment": "test",
            "version": "1.0.0",
        },
    }


# ============================================================================
# CLEANUP FIXTURES
# ============================================================================

@pytest.fixture(autouse=True)
async def cleanup_after_test():
    """
    Cleanup resources after each test.

    This fixture runs automatically after every test to ensure
    proper cleanup and prevent resource leaks.
    """
    yield

    # Cleanup logic - extend as needed for specific resources
    # - Close database connections
    # - Close file handles
    # - Reset global state
    # Note: Most cleanup is handled by fixture-level teardown


{% if test_contract.test_configuration.use_test_database %}
@pytest.fixture(scope="session")
async def test_database_setup():
    """
    Setup test database for integration tests.

    Creates test database schema and seeds initial data.
    Runs once per test session.

    To implement:
    1. Setup phase (before yield):
       - Create database schema/tables
       - Seed initial test data
       - Create necessary indices
       - Initialize connections

    2. Cleanup phase (after yield):
       - Drop test tables
       - Close database connections
       - Reset any shared state

    Example implementation:
        ```python
        # Setup
        connection = await asyncpg.connect(DATABASE_URL)
        await connection.execute("CREATE TABLE test_table (...)")
        await connection.execute("INSERT INTO test_table VALUES (...)")

        yield

        # Cleanup
        await connection.execute("DROP TABLE IF EXISTS test_table")
        await connection.close()
        ```
    """
    # IMPLEMENTATION REQUIRED: Add database setup logic here
    # See docstring above for implementation guidance

    yield

    # IMPLEMENTATION REQUIRED: Add database cleanup logic here
{% endif %}


# ============================================================================
# UTILITY FIXTURES
# ============================================================================

@pytest.fixture
def capture_logs():
    """
    Capture log output for verification.

    Provides access to log messages for assertion in tests.
    """
    import logging
    from io import StringIO

    log_stream = StringIO()
    handler = logging.StreamHandler(log_stream)
    handler.setLevel(logging.DEBUG)

    logger = logging.getLogger()
    logger.addHandler(handler)

    yield log_stream

    logger.removeHandler(handler)


@pytest.fixture
def temp_directory(tmp_path: Path) -> Path:
    """
    Temporary directory for test file operations.

    Provides a clean temporary directory that is automatically
    cleaned up after the test.
    """
    return tmp_path


@pytest.fixture
def correlation_id() -> str:
    """
    Generate correlation ID for test tracing.

    Provides a unique correlation ID for each test execution.
    """
    return str(uuid4())


# ============================================================================
# PARAMETRIZATION HELPERS
# ============================================================================

{% if test_contract.parametrize_tests %}
@pytest.fixture(
    params=[
        "scenario_1",
        "scenario_2",
        "scenario_3",
    ]
)
def test_scenario(request):
    """
    Parametrized test scenarios.

    Provides different test scenarios for parametrized testing.
    """
    return request.param
{% endif %}


# ============================================================================
# PERFORMANCE FIXTURES
# ============================================================================

@pytest.fixture
def performance_timer():
    """
    Timer for performance measurements.

    Provides a simple timer for tracking execution time.
    """
    import time

    class Timer:
        def __init__(self):
            self.start_time = None
            self.end_time = None

        def start(self):
            self.start_time = time.perf_counter()

        def stop(self):
            self.end_time = time.perf_counter()

        @property
        def elapsed_ms(self) -> float:
            if self.start_time is None or self.end_time is None:
                return 0.0
            return (self.end_time - self.start_time) * 1000

    return Timer()


# ============================================================================
# CUSTOM FIXTURES
# ============================================================================

{% for fixture in fixtures %}
@pytest.fixture{% if fixture.scope %}(scope="{{ fixture.scope }}"){% endif %}

def {{ fixture.name }}():
    """{{ fixture.description }}"""
    {{ fixture.code | indent(4) }}

{% endfor %}

# ============================================================================
# PYTEST HOOKS
# ============================================================================

def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """
    Add custom summary information to test output.
    """
    if config.getoption("verbose") > 0:
        terminalreporter.write_sep("=", "{{ node_name }} Test Summary")
        terminalreporter.write_line(f"Node Type: {{ node_type }}")
        terminalreporter.write_line(f"Coverage Target: {{ test_contract.coverage_target }}%")
