"""
Lifecycle Management Pattern Generators for ONEX v2.0 Nodes.

Provides production-ready lifecycle management patterns for node initialization,
startup, runtime monitoring, and graceful shutdown. Reduces manual completion
from 50% â†’ 10% by generating complete, working lifecycle implementations.

Pattern Coverage:
- Initialization (__init__): Container setup, metrics, correlation tracking
- Startup (startup): Consul registration, Kafka connection, health checks
- Runtime: Resource monitoring, health updates, metrics publication
- Shutdown (shutdown): Graceful cleanup, deregistration, resource release

Performance Requirements:
- Startup: Complete in <5 seconds
- Shutdown: Graceful in <2 seconds
- No resource leaks
- Proper error handling at each phase

Integration:
- Works with health check patterns (Workstream 1)
- Integrates with Consul patterns (Workstream 2)
- Uses OnexEnvelopeV1 events (Workstream 3)
- Publishes metrics (Workstream 4)

Generated: 2025-11-05
Phase: 2 (Production Patterns)
Workstream: 5 (Lifecycle Management)
"""

from typing import Any


class LifecyclePatternGenerator:
    """
    Generator for ONEX v2.0 lifecycle management patterns.

    Provides complete lifecycle implementations with proper resource management,
    error handling, and integration with other ONEX infrastructure components.

    Example:
        >>> generator = LifecyclePatternGenerator()
        >>> init_code = generator.generate_init_method(
        ...     node_type="effect",
        ...     operations=["database_query", "api_call"]
        ... )
        >>> startup_code = generator.generate_startup_method(
        ...     node_type="effect",
        ...     dependencies=["consul", "kafka", "postgres"]
        ... )
        >>> shutdown_code = generator.generate_shutdown_method(
        ...     dependencies=["kafka", "postgres", "consul"]
        ... )
    """

    def __init__(self) -> None:
        """Initialize lifecycle pattern generator."""
        self.startup_timeout = 5000  # 5 seconds in ms
        self.shutdown_timeout = 2000  # 2 seconds in ms

    def generate_init_method(
        self,
        node_type: str,
        operations: list[str],
        enable_health_checks: bool = True,
        enable_introspection: bool = True,
        enable_metrics: bool = True,
        custom_config: dict[str, Any] | None = None,
    ) -> str:
        """
        Generate __init__ method with lifecycle setup.

        Creates initialization code with:
        - Container configuration extraction
        - Correlation tracking setup
        - Metrics initialization
        - Health check system initialization
        - Introspection system setup
        - Resource pre-allocation

        Args:
            node_type: Node type (effect, compute, reducer, orchestrator)
            operations: List of operations this node supports
            enable_health_checks: Enable health check initialization
            enable_introspection: Enable introspection system
            enable_metrics: Enable metrics collection
            custom_config: Additional configuration parameters

        Returns:
            Complete __init__ method implementation as string

        Performance:
            - Initialization: <100ms
            - Memory overhead: <10MB
            - No blocking operations

        Example:
            >>> gen = LifecyclePatternGenerator()
            >>> code = gen.generate_init_method(
            ...     node_type="effect",
            ...     operations=["query", "update"]
            ... )
            >>> assert "def __init__" in code
            >>> assert "initialize_health_checks" in code
        """
        # Input validation
        if not node_type or not isinstance(node_type, str):
            raise ValueError(
                f"node_type must be a non-empty string, got: {node_type!r}. "
                f"Valid options: 'effect', 'compute', 'reducer', 'orchestrator'"
            )

        VALID_NODE_TYPES = {"effect", "compute", "reducer", "orchestrator"}
        if node_type.lower() not in VALID_NODE_TYPES:
            raise ValueError(
                f"Invalid node_type: {node_type!r}. "
                f"Valid options: {', '.join(sorted(VALID_NODE_TYPES))}"
            )

        if not isinstance(operations, list):
            raise TypeError(
                f"operations must be a list, got: {type(operations).__name__}. "
                f"Example: ['query', 'update', 'delete']"
            )

        if not operations:
            raise ValueError(
                "operations must contain at least one operation, got empty list. "
                "Example: ['query', 'update', 'delete']"
            )

        for op in operations:
            if not isinstance(op, str) or not op:
                raise ValueError(
                    f"All operations must be non-empty strings, got invalid operation: {op!r}. "
                    f"Valid examples: 'query', 'update', 'aggregate'"
                )

        if not isinstance(enable_health_checks, bool):
            raise TypeError(
                f"enable_health_checks must be a boolean, got: {type(enable_health_checks).__name__}"
            )

        if not isinstance(enable_introspection, bool):
            raise TypeError(
                f"enable_introspection must be a boolean, got: {type(enable_introspection).__name__}"
            )

        if not isinstance(enable_metrics, bool):
            raise TypeError(
                f"enable_metrics must be a boolean, got: {type(enable_metrics).__name__}"
            )

        if custom_config is not None and not isinstance(custom_config, dict):
            raise TypeError(
                f"custom_config must be a dict or None, got: {type(custom_config).__name__}. "
                f"Example: {{'timeout': 30, 'batch_size': 100}}"
            )

        operations_repr = repr(operations)

        health_check_init = ""
        if enable_health_checks:
            health_check_init = """
        # Initialize health checks (if mixins available)
        if MIXINS_AVAILABLE:
            self.initialize_health_checks()
            self._register_component_checks()"""

        introspection_init = ""
        if enable_introspection:
            introspection_init = """
            # Initialize introspection system
            self.initialize_introspection()"""

        metrics_init = ""
        if enable_metrics:
            metrics_init = """
        # Initialize metrics collection
        self._metrics_enabled = True
        self._operation_metrics: dict[str, dict[str, Any]] = {}"""

        custom_config_init = ""
        if custom_config:
            config_items = "\n        ".join(
                f'self.{key} = self.config.get("{key}", {value!r})'
                for key, value in custom_config.items()
            )
            custom_config_init = f"""
        # Custom configuration
        {config_items}"""

        return f'''    def __init__(self, container: ModelContainer) -> None:
        """
        Initialize node with lifecycle management.

        Sets up:
        - Container configuration
        - Correlation tracking
        - Health check system
        - Introspection system
        - Metrics collection
        - Resource pre-allocation

        Args:
            container: ONEX container with configuration and services

        Performance:
            - Initialization time: <100ms
            - Memory overhead: <10MB
        """
        from uuid import uuid4

        super().__init__(container)

        # Extract configuration from container
        # ModelContainer stores config in value field
        self.config = container.value if isinstance(container.value, dict) else {{}}

        # Correlation tracking setup
        existing_node_id = getattr(self, "node_id", None)
        self.node_id = str(existing_node_id) if existing_node_id is not None else str(uuid4())
        self.active_correlations: set[str] = set()
{health_check_init}{introspection_init}{metrics_init}{custom_config_init}

        emit_log_event(
            LogLevel.INFO,
            f"{{self.__class__.__name__}} initialized with lifecycle management",
            {{
                "node_id": str(self.node_id),
                "node_type": "{node_type}",
                "operations": {operations_repr},
                "health_checks_enabled": {enable_health_checks},
                "introspection_enabled": {enable_introspection},
                "metrics_enabled": {enable_metrics},
            }}
        )

    def _register_component_checks(self) -> None:
        """
        Register component health checks for this node.

        Override this method to add custom health checks specific to
        this node's dependencies (database, Kafka, external APIs, etc.).

        Example:
            >>> def _register_component_checks(self):
            ...     # Database health check
            ...     self.register_component_check(
            ...         "postgres",
            ...         self._check_postgres_health
            ...     )
            ...     # Kafka health check
            ...     self.register_component_check(
            ...         "kafka",
            ...         self._check_kafka_health
            ...     )
        """
        # Base node runtime check is registered by HealthCheckMixin
        # Add custom checks here as needed
        pass'''

    def generate_startup_method(
        self,
        node_type: str,
        dependencies: list[str],
        enable_consul: bool = True,
        enable_kafka: bool = True,
        enable_health_checks: bool = True,
        enable_metrics: bool = True,
        enable_introspection: bool = True,
        background_tasks: list[str] | None = None,
    ) -> str:
        """
        Generate startup() method with dependency initialization.

        Creates startup code with:
        - Dependency initialization (Consul, Kafka, etc.)
        - Health check system startup
        - Background task initialization
        - Introspection broadcasting
        - Service registration

        Args:
            node_type: Node type (effect, compute, reducer, orchestrator)
            dependencies: List of dependencies to initialize
            enable_consul: Register with Consul service discovery
            enable_kafka: Connect to Kafka event bus
            enable_health_checks: Start health check monitoring
            enable_metrics: Start metrics collection
            enable_introspection: Broadcast introspection data
            background_tasks: List of background task names to start

        Returns:
            Complete startup() method implementation as string

        Performance:
            - Startup time: <5 seconds (target)
            - Parallel initialization where possible
            - Graceful degradation on non-critical failures

        Example:
            >>> gen = LifecyclePatternGenerator()
            >>> code = gen.generate_startup_method(
            ...     node_type="effect",
            ...     dependencies=["consul", "kafka", "postgres"]
            ... )
            >>> assert "async def startup" in code
            >>> assert "register_with_consul" in code
        """
        # Input validation
        if not node_type or not isinstance(node_type, str):
            raise ValueError(
                f"node_type must be a non-empty string, got: {node_type!r}. "
                f"Valid options: 'effect', 'compute', 'reducer', 'orchestrator'"
            )

        VALID_NODE_TYPES = {"effect", "compute", "reducer", "orchestrator"}
        if node_type.lower() not in VALID_NODE_TYPES:
            raise ValueError(
                f"Invalid node_type: {node_type!r}. "
                f"Valid options: {', '.join(sorted(VALID_NODE_TYPES))}"
            )

        if not isinstance(dependencies, list):
            raise TypeError(
                f"dependencies must be a list, got: {type(dependencies).__name__}. "
                f"Example: ['consul', 'kafka', 'postgres']"
            )

        VALID_DEPENDENCIES = {
            "postgres",
            "database",
            "kafka",
            "redpanda",
            "consul",
            "redis",
            "http_service",
            "vault",
        }
        invalid_deps = set(dependencies) - VALID_DEPENDENCIES
        if invalid_deps:
            raise ValueError(
                f"Invalid dependencies: {invalid_deps}. "
                f"Valid options: {', '.join(sorted(VALID_DEPENDENCIES))}"
            )

        if background_tasks is not None and not isinstance(background_tasks, list):
            raise TypeError(
                f"background_tasks must be a list or None, got: {type(background_tasks).__name__}. "
                f"Example: ['metrics_collector', 'health_monitor']"
            )

        background_tasks = background_tasks or []

        # Build initialization steps in order
        init_steps: list[str] = []

        if enable_health_checks:
            init_steps.append(
                """
        # Initialize health checks
        await self._initialize_health_checks()
        emit_log_event(
            LogLevel.INFO,
            "Health checks initialized",
            {"node_id": str(self.node_id)}
        )"""
            )

        if enable_consul and "consul" in dependencies:
            init_steps.append(
                """
        # Register with Consul
        if self.container.consul_client:
            await self._register_with_consul()
            emit_log_event(
                LogLevel.INFO,
                "Consul registration complete",
                {"node_id": str(self.node_id)}
            )
        else:
            emit_log_event(
                LogLevel.WARNING,
                "Consul client not available - skipping registration",
                {"node_id": str(self.node_id)}
            )"""
            )

        if enable_kafka and "kafka" in dependencies:
            init_steps.append(
                """
        # Connect to Kafka
        if self.container.kafka_client:
            await self._connect_kafka()
            emit_log_event(
                LogLevel.INFO,
                "Kafka connection established",
                {"node_id": str(self.node_id)}
            )
        else:
            emit_log_event(
                LogLevel.WARNING,
                "Kafka client not available - events will not be published",
                {"node_id": str(self.node_id)}
            )"""
            )

        # Database connections
        if "postgres" in dependencies:
            init_steps.append(
                """
        # Connect to PostgreSQL
        if self.container.postgres_client:
            await self._connect_postgres()
            emit_log_event(
                LogLevel.INFO,
                "PostgreSQL connection established",
                {"node_id": str(self.node_id)}
            )"""
            )

        if enable_metrics:
            init_steps.append(
                """
        # Start metrics collection
        await self._start_metrics_collection()
        emit_log_event(
            LogLevel.INFO,
            "Metrics collection started",
            {"node_id": str(self.node_id)}
        )"""
            )

        if enable_introspection:
            init_steps.append(
                """
        # Publish introspection broadcast to registry
        await self.publish_introspection(reason="startup")

        # Start introspection background tasks (heartbeat, registry listener)
        await self.start_introspection_tasks(
            enable_heartbeat=True,
            heartbeat_interval_seconds=30,
            enable_registry_listener=True,
        )
        emit_log_event(
            LogLevel.INFO,
            "Introspection system started",
            {"node_id": str(self.node_id)}
        )"""
            )

        # Background tasks
        for task in background_tasks:
            init_steps.append(
                f"""
        # Start background task: {task}
        await self._start_{task}()"""
            )

        startup_steps = "\n".join(init_steps)

        return f'''    async def startup(self) -> None:
        """
        Node startup lifecycle hook.

        Initializes all dependencies and starts background services:
        - Health check monitoring
        - Consul service registration
        - Kafka event bus connection
        - Database connections
        - Metrics collection
        - Introspection broadcasting
        - Background task execution

        Should be called when node is ready to serve requests.

        Performance:
            - Target: <5 seconds total startup time
            - Parallel initialization where safe
            - Graceful degradation on non-critical failures

        Raises:
            Exception: If critical startup steps fail
        """
        if not MIXINS_AVAILABLE:
            emit_log_event(
                LogLevel.WARNING,
                "Mixins not available - skipping startup registration",
                {{"node_id": str(self.node_id)}}
            )
            return

        emit_log_event(
            LogLevel.INFO,
            f"{{self.__class__.__name__}} starting up",
            {{
                "node_id": str(self.node_id),
                "node_type": "{node_type}",
                "dependencies": {dependencies!r},
            }}
        )

        try:{startup_steps}

            emit_log_event(
                LogLevel.INFO,
                f"{{self.__class__.__name__}} startup complete",
                {{
                    "node_id": str(self.node_id),
                    "status": "operational",
                }}
            )

        except Exception as e:
            emit_log_event(
                LogLevel.ERROR,
                f"{{self.__class__.__name__}} startup failed",
                {{
                    "node_id": str(self.node_id),
                    "error": str(e),
                    "error_type": type(e).__name__,
                }}
            )
            # Attempt cleanup of partially initialized resources
            await self._cleanup_partial_startup()
            raise'''

    def generate_shutdown_method(
        self,
        dependencies: list[str],
        enable_consul: bool = True,
        enable_kafka: bool = True,
        enable_metrics: bool = True,
        enable_introspection: bool = True,
        background_tasks: list[str] | None = None,
    ) -> str:
        """
        Generate shutdown() method with cleanup.

        Creates shutdown code with:
        - Final metrics publication
        - Consul deregistration
        - Kafka disconnection
        - Background task cancellation
        - Container resource cleanup

        Args:
            dependencies: List of dependencies to clean up
            enable_consul: Deregister from Consul
            enable_kafka: Disconnect from Kafka
            enable_metrics: Publish final metrics report
            enable_introspection: Stop introspection tasks
            background_tasks: List of background task names to stop

        Returns:
            Complete shutdown() method implementation as string

        Performance:
            - Shutdown time: <2 seconds (target)
            - Graceful operation completion
            - No data loss
            - Proper resource release

        Example:
            >>> gen = LifecyclePatternGenerator()
            >>> code = gen.generate_shutdown_method(
            ...     dependencies=["kafka", "postgres", "consul"]
            ... )
            >>> assert "async def shutdown" in code
            >>> assert "disconnect" in code
        """
        # Input validation
        if not isinstance(dependencies, list):
            raise TypeError(
                f"dependencies must be a list, got: {type(dependencies).__name__}. "
                f"Example: ['consul', 'kafka', 'postgres']"
            )

        VALID_DEPENDENCIES = {
            "postgres",
            "database",
            "kafka",
            "redpanda",
            "consul",
            "redis",
            "http_service",
            "vault",
        }
        invalid_deps = set(dependencies) - VALID_DEPENDENCIES
        if invalid_deps:
            raise ValueError(
                f"Invalid dependencies: {invalid_deps}. "
                f"Valid options: {', '.join(sorted(VALID_DEPENDENCIES))}"
            )

        if background_tasks is not None and not isinstance(background_tasks, list):
            raise TypeError(
                f"background_tasks must be a list or None, got: {type(background_tasks).__name__}. "
                f"Example: ['metrics_collector', 'health_monitor']"
            )

        background_tasks = background_tasks or []

        # Build cleanup steps in reverse order of startup
        cleanup_steps: list[str] = []

        # Background tasks (stop first)
        for task in background_tasks:
            cleanup_steps.append(
                f"""
        # Stop background task: {task}
        await self._stop_{task}()"""
            )

        if enable_metrics:
            cleanup_steps.append(
                """
        # Stop metrics collection and publish final report
        await self._stop_metrics_collection()
        emit_log_event(
            LogLevel.INFO,
            "Metrics collection stopped",
            {"node_id": str(self.node_id)}
        )"""
            )

        if enable_introspection:
            cleanup_steps.append(
                """
        # Stop introspection background tasks
        await self.stop_introspection_tasks()
        emit_log_event(
            LogLevel.INFO,
            "Introspection tasks stopped",
            {"node_id": str(self.node_id)}
        )"""
            )

        if enable_consul and "consul" in dependencies:
            cleanup_steps.append(
                """
        # Deregister from Consul
        if hasattr(self, '_consul_service_id'):
            await self._deregister_from_consul()
            emit_log_event(
                LogLevel.INFO,
                "Consul deregistration complete",
                {"node_id": str(self.node_id)}
            )"""
            )

        # Database disconnection
        if "postgres" in dependencies:
            cleanup_steps.append(
                """
        # Disconnect from PostgreSQL
        if self.container.postgres_client:
            await self._disconnect_postgres()
            emit_log_event(
                LogLevel.INFO,
                "PostgreSQL disconnected",
                {"node_id": str(self.node_id)}
            )"""
            )

        if enable_kafka and "kafka" in dependencies:
            cleanup_steps.append(
                """
        # Disconnect from Kafka
        if self.container.kafka_client:
            await self._disconnect_kafka()
            emit_log_event(
                LogLevel.INFO,
                "Kafka disconnected",
                {"node_id": str(self.node_id)}
            )"""
            )

        cleanup_steps.append(
            """
        # Cleanup container resources
        if hasattr(self.container, 'cleanup'):
            await self.container.cleanup()"""
        )

        shutdown_steps = "\n".join(cleanup_steps)

        return f'''    async def shutdown(self) -> None:
        """
        Node shutdown lifecycle hook.

        Gracefully shuts down all services and releases resources:
        - Stops background tasks
        - Publishes final metrics
        - Stops introspection heartbeat
        - Deregisters from Consul
        - Disconnects from databases
        - Disconnects from Kafka
        - Cleans up container resources

        Should be called when node is preparing to exit.

        Performance:
            - Target: <2 seconds total shutdown time
            - Graceful operation completion
            - No data loss
            - Proper resource release
        """
        if not MIXINS_AVAILABLE:
            return

        emit_log_event(
            LogLevel.INFO,
            f"{{self.__class__.__name__}} shutting down",
            {{
                "node_id": str(self.node_id),
                "active_correlations": len(getattr(self, 'active_correlations', [])),
            }}
        )

        try:{shutdown_steps}

            emit_log_event(
                LogLevel.INFO,
                f"{{self.__class__.__name__}} shutdown complete",
                {{"node_id": str(self.node_id)}}
            )

        except Exception as e:
            emit_log_event(
                LogLevel.ERROR,
                f"Error during {{self.__class__.__name__}} shutdown",
                {{
                    "node_id": str(self.node_id),
                    "error": str(e),
                    "error_type": type(e).__name__,
                }},
            )
            # Continue with shutdown despite errors
            # Critical to release resources'''

    def generate_runtime_monitoring(
        self,
        monitor_health: bool = True,
        monitor_metrics: bool = True,
        monitor_resources: bool = True,
        interval_seconds: int = 60,
    ) -> str:
        """
        Generate runtime health/metrics monitoring code.

        Creates background monitoring tasks that:
        - Update health check status periodically
        - Publish metrics snapshots
        - Monitor resource usage (memory, connections, etc.)
        - Detect anomalies and degradation

        Args:
            monitor_health: Enable health check updates
            monitor_metrics: Enable metrics publication
            monitor_resources: Enable resource monitoring
            interval_seconds: Monitoring interval in seconds

        Returns:
            Complete runtime monitoring implementation as string

        Performance:
            - Monitoring overhead: <1% CPU
            - Memory overhead: <5MB
            - Non-blocking execution

        Example:
            >>> gen = LifecyclePatternGenerator()
            >>> code = gen.generate_runtime_monitoring()
            >>> assert "async def _runtime_monitor" in code
            >>> assert "asyncio.sleep" in code
        """
        monitors: list[str] = []

        if monitor_health:
            monitors.append(
                """
            # Update health check status
            health_status = await self._check_node_health()

            # Normalize response - handle both object and dict
            healthy = getattr(health_status, "healthy", health_status.get("healthy", True) if isinstance(health_status, dict) else True)
            status = getattr(health_status, "status", health_status.get("status", "unknown") if isinstance(health_status, dict) else "unknown")
            components = getattr(health_status, "components", health_status.get("components", {}) if isinstance(health_status, dict) else {})

            if not healthy:
                emit_log_event(
                    LogLevel.WARNING,
                    "Health check degraded",
                    {
                        "node_id": str(self.node_id),
                        "status": status,
                        "components": components,
                    }
                )"""
            )

        if monitor_metrics:
            monitors.append(
                """
            # Publish metrics snapshot
            await self._publish_metrics_snapshot()"""
            )

        if monitor_resources:
            monitors.append(
                """
            # Monitor resource usage
            resource_usage = await self._check_resource_usage()
            if resource_usage.memory_mb > 512:
                emit_log_event(
                    LogLevel.WARNING,
                    "High memory usage detected",
                    {
                        "node_id": str(self.node_id),
                        "memory_mb": resource_usage.memory_mb,
                    }
                )

            # Check connection pool health
            if hasattr(self, 'container') and self.container.postgres_client:
                pool_utilization = await self._check_pool_utilization()
                if pool_utilization > 0.9:
                    emit_log_event(
                        LogLevel.WARNING,
                        "Connection pool near exhaustion",
                        {
                            "node_id": str(self.node_id),
                            "utilization": pool_utilization,
                        }
                    )"""
            )

        monitoring_tasks = "\n".join(monitors)

        return f'''    async def _runtime_monitor(self) -> None:
        """
        Background runtime monitoring task.

        Periodically monitors:
        - Health check status
        - Metrics and performance
        - Resource usage (memory, connections)
        - Connection pool health

        Runs continuously until shutdown.

        Performance:
            - Overhead: <1% CPU
            - Memory: <5MB
            - Interval: {interval_seconds}s
        """
        emit_log_event(
            LogLevel.INFO,
            "Runtime monitoring started",
            {{
                "node_id": str(self.node_id),
                "interval_seconds": {interval_seconds},
            }}
        )

        try:
            while True:
                try:{monitoring_tasks}

                except Exception as e:
                    emit_log_event(
                        LogLevel.ERROR,
                        f"Error in runtime monitoring: {{e!s}}",
                        {{
                            "node_id": str(self.node_id),
                            "error_type": type(e).__name__,
                        }}
                    )

                # Wait for next monitoring interval
                await asyncio.sleep({interval_seconds})

        except asyncio.CancelledError:
            emit_log_event(
                LogLevel.INFO,
                "Runtime monitoring cancelled",
                {{"node_id": str(self.node_id)}}
            )
            raise'''

    def generate_helper_methods(
        self,
        dependencies: list[str],
    ) -> str:
        """
        Generate helper methods for lifecycle management.

        Creates internal helper methods for:
        - Consul registration/deregistration
        - Kafka connection/disconnection
        - Database connection/disconnection
        - Partial startup cleanup
        - Metrics collection start/stop

        Args:
            dependencies: List of dependencies requiring helpers

        Returns:
            Complete helper methods implementation as string
        """
        helpers: list[str] = []

        if "consul" in dependencies:
            helpers.append(
                '''
    async def _register_with_consul(self) -> None:
        """Register node with Consul service discovery."""
        self._consul_service_id = f"{self.__class__.__name__}-{self.node_id}"
        await self.container.consul_client.register_service(
            service_id=self._consul_service_id,
            service_name=self.__class__.__name__,
            address=self.config.get("service_address", "localhost"),
            port=self.config.get("service_port", 8000),
            tags=["onex", "node", self.__class__.__name__.lower()],
            check={
                "http": f"http://{self.config.get('service_address', 'localhost')}:{self.config.get('service_port', 8000)}/health",
                "interval": "10s",
                "timeout": "5s",
            },
        )

    async def _deregister_from_consul(self) -> None:
        """Deregister node from Consul service discovery."""
        await self.container.consul_client.deregister_service(
            service_id=self._consul_service_id
        )'''
            )

        if "kafka" in dependencies:
            helpers.append(
                '''
    async def _connect_kafka(self) -> None:
        """Connect to Kafka event bus."""
        await self.container.kafka_client.connect()

    async def _disconnect_kafka(self) -> None:
        """Disconnect from Kafka event bus."""
        await self.container.kafka_client.disconnect()'''
            )

        if "postgres" in dependencies:
            helpers.append(
                '''
    async def _connect_postgres(self) -> None:
        """Connect to PostgreSQL database."""
        await self.container.postgres_client.connect()

    async def _disconnect_postgres(self) -> None:
        """Disconnect from PostgreSQL database."""
        await self.container.postgres_client.disconnect()'''
            )

        helpers.append(
            '''
    async def _initialize_health_checks(self) -> None:
        """Initialize health check system."""
        # Health checks are initialized in __init__ via mixins
        pass

    async def _start_metrics_collection(self) -> None:
        """Start metrics collection background task."""
        if hasattr(self, '_metrics_enabled') and self._metrics_enabled:
            # Metrics collection is handled by MixinMetrics
            pass

    async def _stop_metrics_collection(self) -> None:
        """Stop metrics collection and publish final report."""
        if hasattr(self, '_metrics_enabled') and self._metrics_enabled:
            # Publish final metrics snapshot
            await self._publish_metrics_snapshot()

    async def _publish_metrics_snapshot(self) -> None:
        """Publish current metrics snapshot."""
        # Implementation depends on metrics backend
        pass

    async def _check_node_health(self) -> Any:
        """Check node health status."""
        # Delegates to MixinHealthCheck
        if hasattr(self, 'get_health_status'):
            return await self.get_health_status()
        return {"healthy": True, "status": "unknown"}

    async def _check_resource_usage(self) -> Any:
        """Check resource usage (memory, connections)."""
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        return type('ResourceUsage', (), {
            'memory_mb': memory_info.rss / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
        })()

    async def _check_pool_utilization(self) -> float:
        """Check database connection pool utilization."""
        if hasattr(self.container, 'postgres_client'):
            pool = self.container.postgres_client.pool
            if pool:
                return pool.get_size() / pool.get_max_size()
        return 0.0

    async def _cleanup_partial_startup(self) -> None:
        """Clean up resources from partial startup failure."""
        emit_log_event(
            LogLevel.WARNING,
            "Cleaning up partial startup",
            {"node_id": str(self.node_id)}
        )
        # Attempt to clean up any initialized resources
        try:
            await self.shutdown()
        except Exception as e:
            emit_log_event(
                LogLevel.ERROR,
                f"Error during partial startup cleanup: {e!s}",
                {"node_id": str(self.node_id)}
            )'''
        )

        return "\n".join(helpers)


# Convenience functions for direct pattern generation


def generate_init_method(
    node_type: str,
    operations: list[str],
    enable_health_checks: bool = True,
    enable_introspection: bool = True,
    enable_metrics: bool = True,
) -> str:
    """
    Generate __init__ method with lifecycle setup.

    Convenience function for quick pattern generation.

    Args:
        node_type: Node type (effect, compute, reducer, orchestrator)
        operations: List of operations this node supports
        enable_health_checks: Enable health check initialization
        enable_introspection: Enable introspection system
        enable_metrics: Enable metrics collection

    Returns:
        Complete __init__ method implementation as string

    Example:
        >>> code = generate_init_method(
        ...     node_type="effect",
        ...     operations=["query", "update"]
        ... )
        >>> assert "def __init__" in code
    """
    generator = LifecyclePatternGenerator()
    return generator.generate_init_method(
        node_type=node_type,
        operations=operations,
        enable_health_checks=enable_health_checks,
        enable_introspection=enable_introspection,
        enable_metrics=enable_metrics,
    )


def generate_startup_method(
    node_type: str,
    dependencies: list[str],
    enable_consul: bool = True,
    enable_kafka: bool = True,
    background_tasks: list[str] | None = None,
) -> str:
    """
    Generate startup() method with dependency initialization.

    Convenience function for quick pattern generation.

    Args:
        node_type: Node type (effect, compute, reducer, orchestrator)
        dependencies: List of dependencies to initialize
        enable_consul: Register with Consul service discovery
        enable_kafka: Connect to Kafka event bus
        background_tasks: List of background task names to start

    Returns:
        Complete startup() method implementation as string

    Example:
        >>> code = generate_startup_method(
        ...     node_type="effect",
        ...     dependencies=["consul", "kafka"]
        ... )
        >>> assert "async def startup" in code
    """
    generator = LifecyclePatternGenerator()
    return generator.generate_startup_method(
        node_type=node_type,
        dependencies=dependencies,
        enable_consul=enable_consul,
        enable_kafka=enable_kafka,
        background_tasks=background_tasks,
    )


def generate_shutdown_method(
    dependencies: list[str],
    enable_consul: bool = True,
    enable_kafka: bool = True,
    background_tasks: list[str] | None = None,
) -> str:
    """
    Generate shutdown() method with cleanup.

    Convenience function for quick pattern generation.

    Args:
        dependencies: List of dependencies to clean up
        enable_consul: Deregister from Consul
        enable_kafka: Disconnect from Kafka
        background_tasks: List of background task names to stop

    Returns:
        Complete shutdown() method implementation as string

    Example:
        >>> code = generate_shutdown_method(
        ...     dependencies=["kafka", "postgres"]
        ... )
        >>> assert "async def shutdown" in code
    """
    generator = LifecyclePatternGenerator()
    return generator.generate_shutdown_method(
        dependencies=dependencies,
        enable_consul=enable_consul,
        enable_kafka=enable_kafka,
        background_tasks=background_tasks,
    )


def generate_runtime_monitoring(
    monitor_health: bool = True,
    monitor_metrics: bool = True,
    interval_seconds: int = 60,
) -> str:
    """
    Generate runtime health/metrics monitoring code.

    Convenience function for quick pattern generation.

    Args:
        monitor_health: Enable health check updates
        monitor_metrics: Enable metrics publication
        interval_seconds: Monitoring interval in seconds

    Returns:
        Complete runtime monitoring implementation as string

    Example:
        >>> code = generate_runtime_monitoring()
        >>> assert "async def _runtime_monitor" in code
    """
    generator = LifecyclePatternGenerator()
    return generator.generate_runtime_monitoring(
        monitor_health=monitor_health,
        monitor_metrics=monitor_metrics,
        interval_seconds=interval_seconds,
    )


def generate_helper_methods(dependencies: list[str]) -> str:
    """
    Generate helper methods for lifecycle management.

    Convenience function for quick pattern generation.

    Args:
        dependencies: List of dependencies requiring helpers

    Returns:
        Complete helper methods implementation as string

    Example:
        >>> code = generate_helper_methods(["consul", "kafka"])
        >>> assert "_register_with_consul" in code
    """
    generator = LifecyclePatternGenerator()
    return generator.generate_helper_methods(dependencies=dependencies)


__all__ = [
    "LifecyclePatternGenerator",
    "generate_init_method",
    "generate_startup_method",
    "generate_shutdown_method",
    "generate_runtime_monitoring",
    "generate_helper_methods",
]
