# ONEX v2.0 Contract - Auto-generated by ContractInferencer
# Generated from: llm_effect
# DO NOT EDIT - Regenerate using ContractInferencer

schema_version: v2.0.0
name: llm_effect
version:
  major: 1
  minor: 0
  patch: 0
node_type: effect
description: 'LLM Effect Node for multi-tier LLM API calls.


  Tier Configuration:

  - LOCAL: Not implemented yet (future Ollama/vLLM support)

  - CLOUD_FAST: GLM-4.5 via Z.ai (128K context, PRIMARY for Phase 1)

  - CLOUD_PREMIUM: GLM-4.6 via Z.ai (128K context, future)


  Circuit Breaker:

  - Failure threshold: 5 consecutive failures

  - Recovery timeout: 60 seconds

  - Protected: All external LLM API calls


  Retry Policy:

  - Max attempts: 3

  - Backoff: Exponential (1s, 2s, 4s)

  - Retryable errors: Timeout, 429 (rate limit), 5xx (server errors)


  Performance:

  - CLOUD_FAST latency: < 3000ms (P95)

  - Throughput: 10+ concurrent requests

  - Cost tracking: Sub-cent accuracy'
capabilities:
- name: http_request
  description: Http Request support
advanced_features:
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout_ms: 60000
    half_open_max_calls: 3
  retry_policy:
    enabled: true
    max_attempts: 3
    initial_delay_ms: 1000
    max_delay_ms: 10000
    backoff_multiplier: 2.0
  observability:
    tracing:
      enabled: true
      sample_rate: 1.0
    metrics:
      enabled: true
      export_interval_seconds: 15
    logging:
      structured: true
      json_format: true
      correlation_tracking: true
  dead_letter_queue:
    enabled: true
    max_retries: 3
    topic_suffix: .dlq
    retry_delay_ms: 5000
    alert_threshold: 100
  security_validation:
    enabled: true
    sanitize_inputs: true
    sanitize_logs: true
    validate_sql: false
subcontracts:
  effect:
    operations:
    - initialize
    - cleanup
    - execute_effect
