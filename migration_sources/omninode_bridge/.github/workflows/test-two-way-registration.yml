name: Two-Way Registration E2E Tests

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'src/omninode_bridge/nodes/orchestrator/**'
      - 'src/omninode_bridge/nodes/reducer/**'
      - 'src/omninode_bridge/nodes/registry/**'
      - 'src/omninode_bridge/nodes/mixins/introspection_mixin.py'
      - 'src/omninode_bridge/config/**'
      - 'src/omninode_bridge/services/**'
      - 'src/omninode_bridge/constants.py'
      - 'src/metadata_stamping/**'
      - 'tests/integration/test_two_way_registration_e2e.py'
      - 'tests/load/test_introspection_load.py'
      - 'docker/bridge-nodes/**'
      - '.github/workflows/test-two-way-registration.yml'
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'src/omninode_bridge/nodes/orchestrator/**'
      - 'src/omninode_bridge/nodes/reducer/**'
      - 'src/omninode_bridge/nodes/registry/**'
      - 'src/omninode_bridge/nodes/mixins/introspection_mixin.py'
      - 'src/omninode_bridge/config/**'
      - 'src/omninode_bridge/services/**'
      - 'src/omninode_bridge/constants.py'
      - 'src/metadata_stamping/**'
      - 'tests/integration/test_two_way_registration_e2e.py'
      - 'tests/load/test_introspection_load.py'
      - 'docker/bridge-nodes/**'
      - '.github/workflows/test-two-way-registration.yml'
  workflow_dispatch:
    inputs:
      run_load_tests:
        description: 'Run load tests in addition to E2E tests'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'

env:
  PYTHON_VERSION: '3.12'
  POETRY_VERSION: '2.1.3'

jobs:
  # ============================================================================
  # Job 1: E2E Tests
  # ============================================================================
  e2e-tests:
    name: E2E Tests - Two-Way Registration
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --no-root

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start test environment
        run: |
          docker compose -f tests/docker-compose.test.yml up -d --build
          echo "Waiting for services to be healthy..."

      - name: Wait for services to be healthy
        run: |
          ./tests/wait_for_services.sh 120

      - name: Check service health
        run: |
          docker compose -f tests/docker-compose.test.yml ps

          # Check PostgreSQL
          docker exec registration-test-postgres pg_isready -U test -d bridge_test || exit 1

          # Check Consul
          docker exec registration-test-consul consul members || exit 1

          # Check RedPanda
          docker exec registration-test-redpanda rpk cluster info || exit 1

      - name: Create test directories
        run: |
          mkdir -p test-results
          mkdir -p test-logs
          echo "âœ… Created test directories"
          ls -la test-results test-logs

      - name: Run E2E tests
        run: |
          echo "ðŸ” Running E2E tests with enhanced debugging..."
          poetry run pytest tests/integration/test_two_way_registration_e2e.py \
            -v \
            --tb=short \
            --junitxml=test-results/junit-e2e.xml \
            --html=test-results/report-e2e.html \
            --self-contained-html \
            --cov=src/omninode_bridge/nodes \
            --cov-report=xml:test-results/coverage-e2e.xml \
            --cov-report=html:test-results/htmlcov-e2e \
            --override-ini="addopts=" \
            --maxfail=5 \
            --durations=10
        env:
          PYTHONPATH: ${{ github.workspace }}/src
          PYTEST_TIMEOUT: 300

      - name: Collect container logs on failure
        if: failure()
        run: |
          echo "ðŸ“‹ Collecting container logs..."
          mkdir -p test-logs
          docker compose -f tests/docker-compose.test.yml logs > test-logs/docker-compose.log
          docker logs registration-test-orchestrator > test-logs/orchestrator.log 2>&1 || true
          docker logs registration-test-reducer > test-logs/reducer.log 2>&1 || true
          docker logs registration-test-registry > test-logs/registry.log 2>&1 || true
          docker logs registration-test-postgres > test-logs/postgres.log 2>&1 || true
          docker logs registration-test-redpanda > test-logs/redpanda.log 2>&1 || true
          docker logs registration-test-consul > test-logs/consul.log 2>&1 || true
          docker logs registration-test-metadata-stamping > test-logs/metadata-stamping.log 2>&1 || true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            test-results/
            test-logs/

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: test-results/junit-e2e.xml
          check_name: E2E Test Results

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v4
        with:
          files: test-results/coverage-e2e.xml
          flags: e2e-tests
          name: codecov-e2e

      - name: Stop test environment
        if: always()
        run: |
          docker compose -f tests/docker-compose.test.yml down -v

  # ============================================================================
  # Job 2: Load Tests (Optional)
  # ============================================================================
  load-tests:
    name: Load Tests - Introspection System
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: |
      github.event_name == 'workflow_dispatch' &&
      github.event.inputs.run_load_tests == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --no-root

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start test environment
        run: |
          docker compose -f tests/docker-compose.test.yml up -d --build
          echo "Waiting for services to be healthy..."

      - name: Wait for services to be healthy
        run: |
          ./tests/wait_for_services.sh 120

      - name: Check service health
        run: |
          docker compose -f tests/docker-compose.test.yml ps
          docker exec registration-test-postgres pg_isready -U test -d bridge_test || exit 1
          docker exec registration-test-consul consul members || exit 1
          docker exec registration-test-redpanda rpk cluster info || exit 1

      - name: Create test directories for load tests
        run: |
          mkdir -p test-results
          mkdir -p test-results/metrics
          mkdir -p test-logs
          echo "âœ… Created load test directories"
          ls -la test-results test-results/metrics test-logs

      - name: Run load tests
        run: |
          poetry run pytest tests/load/test_introspection_load.py \
            -v \
            -s \
            -m load \
            --tb=short \
            --junitxml=test-results/junit-load.xml \
            --html=test-results/report-load.html \
            --self-contained-html \
            --override-ini="addopts="
        env:
          PYTHONPATH: ${{ github.workspace }}/src

      - name: Collect performance metrics
        if: always()
        run: |
          echo "ðŸ“Š Collecting performance metrics..."
          curl -s http://localhost:8062/metrics > test-results/metrics/registry.txt || true
          curl -s http://localhost:8060/metrics > test-results/metrics/orchestrator.txt || true
          curl -s http://localhost:8061/metrics > test-results/metrics/reducer.txt || true

      - name: Collect container logs on failure
        if: failure()
        run: |
          echo "ðŸ“‹ Collecting container logs for load tests..."
          mkdir -p test-logs
          docker compose -f tests/docker-compose.test.yml logs > test-logs/docker-compose.log
          docker logs registration-test-orchestrator > test-logs/orchestrator.log 2>&1 || true
          docker logs registration-test-reducer > test-logs/reducer.log 2>&1 || true
          docker logs registration-test-registry > test-logs/registry.log 2>&1 || true
          docker logs registration-test-metadata-stamping > test-logs/metadata-stamping.log 2>&1 || true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            test-results/
            test-logs/

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: test-results/junit-load.xml
          check_name: Load Test Results

      - name: Stop test environment
        if: always()
        run: |
          docker compose -f tests/docker-compose.test.yml down -v

  # ============================================================================
  # Job 3: Performance Report
  # ============================================================================
  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [e2e-tests]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test artifacts
        uses: actions/download-artifact@v4
        with:
          name: e2e-test-results
          path: test-results/
        continue-on-error: true

      - name: Ensure test directory exists
        run: |
          mkdir -p test-results
          echo "âœ… Ensured test-results directory exists"
          echo "Contents of test-results:"
          ls -la test-results/ || echo "test-results directory is empty"

      - name: Generate performance summary
        run: |
          cat > test-results/performance-summary.md << EOF
          # Two-Way Registration Performance Summary

          ## Test Run Information
          - **Date**: $(date)
          - **Branch**: ${{ github.ref_name }}
          - **Commit**: ${{ github.sha }}
          - **Workflow**: ${{ github.run_number }}
          - **Test Status**: ${{ needs.e2e-tests.result }}

          ## Performance Thresholds

          | Metric | Threshold | Status |
          |--------|-----------|--------|
          | Introspection broadcast latency | < 50ms | â³ |
          | Registry processing latency | < 100ms | â³ |
          | Dual registration time | < 200ms | â³ |
          | Heartbeat overhead | < 10ms | â³ |

          ## Test Results

          ### E2E Test Status: ${{ needs.e2e-tests.result }}

          EOF

          # Add test results if available
          if [ -f "test-results/junit-e2e.xml" ]; then
            echo "### âœ… Test Artifacts Available" >> test-results/performance-summary.md
            echo "- JUnit XML Results: Available" >> test-results/performance-summary.md
            echo "- HTML Report: Available" >> test-results/performance-summary.md
            echo "- Coverage Report: Available" >> test-results/performance-summary.md
          else
            echo "### âš ï¸ Test Artifacts Unavailable" >> test-results/performance-summary.md
            echo "- Test execution may have failed before artifact generation" >> test-results/performance-summary.md
          fi

          # Add logs if available
          if [ -f "test-logs/docker-compose.log" ]; then
            echo "- Service Logs: Available" >> test-results/performance-summary.md
          fi

          cat >> test-results/performance-summary.md << EOF

          ## Notes

          - Performance metrics will be available when tests pass successfully
          - Service logs are collected for debugging when tests fail
          - Use local test runner for detailed debugging: \`./tests/run_e2e_tests.sh --no-cleanup\`

          ## Artifacts

          Check the "e2e-test-results" artifacts in the GitHub Actions run for:
          - Detailed test results (JUnit XML)
          - HTML test reports
          - Coverage reports
          - Service logs (on failure)

          EOF

          cat test-results/performance-summary.md

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: test-results/performance-summary.md

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            let summary = '';

            try {
              summary = fs.readFileSync('test-results/performance-summary.md', 'utf8');
            } catch (error) {
              summary = `# Test Results Summary\n\n## Test Run Information\n- **Date**: ${new Date().toISOString()}\n- **Branch**: '${{ github.ref_name }}'\n- **Commit**: '${{ github.sha }}'\n- **Workflow**: ${{ github.run_number }}\n- **Test Status**: ${{ needs.e2e-tests.result }}\n\n## Notes\nPerformance summary could not be generated due to test failures. Check the full workflow logs for details.`;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
