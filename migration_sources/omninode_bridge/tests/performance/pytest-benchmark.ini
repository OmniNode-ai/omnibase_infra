[pytest-benchmark]
# Pytest-Benchmark Configuration for Bridge Performance Tests

# ============================================================================
# Output Configuration
# ============================================================================

# Save benchmark results to JSON for comparison
autosave = true
save-data = true
json = true
json-path = test-artifacts/benchmark.json

# CSV output for analysis
csv = test-artifacts/benchmark.csv

# Histogram visualization
histogram = test-artifacts/benchmark-histogram.svg
histogram-max-percent = 10.0

# ============================================================================
# Statistical Configuration
# ============================================================================

# Number of rounds and iterations
rounds = 5
iterations = 10
warmup = true
warmup-iterations = 5

# Timer precision
timer = time.perf_counter

# Statistical measures
statistics = min,max,mean,stddev,median,iqr,ops

# ============================================================================
# Comparison Configuration
# ============================================================================

# Comparison thresholds (fail if performance degrades)
compare = baseline
compare-fail = mean:10%,min:10%,max:10%

# ============================================================================
# Performance Requirements
# ============================================================================

# Maximum allowed times for different operations
max-time = {
    "test_batch_aggregation_100_items": 0.1,
    "test_batch_aggregation_1000_items": 1.0,
    "test_batch_aggregation_10000_items": 10.0,
    "test_single_workflow_execution": 0.3,
    "test_concurrent_workflow_throughput": 0.5,
    "test_fsm_state_transition": 0.05,
    "test_complete_workflow_pipeline": 0.5
}

# ============================================================================
# Calibration
# ============================================================================

# Disable calibration (use explicit rounds/iterations)
disable-gc = false
calibration-precision = 10
calibration-runs = 5

# ============================================================================
# Display Configuration
# ============================================================================

# Column sorting and display
columns = min,max,mean,stddev,median,iqr,ops,rounds,iterations
sort = mean
verbose = true
name-format = short

# ============================================================================
# Memory Profiling
# ============================================================================

# Enable memory profiling (requires memory_profiler)
# memory = true
# memory-only = false

# ============================================================================
# Grouping
# ============================================================================

# Group benchmarks by test class
group-by = group,name,param:iterations

# ============================================================================
# CI/CD Integration
# ============================================================================

# Storage for baseline comparisons
storage = file://test-artifacts/.benchmarks
netrc = false
