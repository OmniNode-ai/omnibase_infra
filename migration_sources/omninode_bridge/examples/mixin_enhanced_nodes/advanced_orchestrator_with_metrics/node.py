#!/usr/bin/env python3
"""
Production-grade workflow orchestrator with comprehensive observability.

Demonstrates advanced mixin integration with health monitoring, performance metrics,
and structured logging for production deployments.

Generated by OmniNode Code Generator - Example Implementation
This is a complete, runnable example demonstrating advanced mixin patterns.
"""

import asyncio
import logging
import time
from typing import Any
from uuid import uuid4

from omnibase_core.models.core.model_container import ModelContainer
from omnibase_core.nodes.node_orchestrator import NodeOrchestrator

# Import mixins (from omninode_bridge examples)
try:
    from omninode_bridge.nodes.mixins.health_mixin import (
        HealthCheckMixin,
        HealthStatus,
        check_database_connection,
        check_http_service,
        check_kafka_connection,
    )
except ImportError:
    # Fallback for standalone examples
    from typing import Protocol

    class HealthCheckMixin(Protocol):
        """Stub for type checking when mixin not available."""

        def initialize_health_checks(self) -> None: ...
        async def check_health(self) -> Any: ...


logger = logging.getLogger(__name__)


class NodeWorkflowOrchestrator(NodeOrchestrator, HealthCheckMixin):
    """
    Production-grade workflow orchestrator with comprehensive observability.

    ONEX v2.0 Compliant Orchestrator Node

    Capabilities:
      Built-in Features (NodeOrchestrator):
        - Multi-step workflow coordination
        - Parallel and sequential step execution
        - Circuit breakers with failure threshold
        - Retry policies with exponential backoff
        - Transaction support with rollback
        - Performance metrics tracking

      Enhanced Features (Mixins):
        - MixinHealthCheck: Multi-component health monitoring
        - MixinMetrics: Performance metrics and histograms
        - MixinLogData: Structured logging with correlation IDs

    Example Usage:
        >>> from omnibase_core.models.core.model_container import ModelContainer
        >>> container = ModelContainer()
        >>> node = NodeWorkflowOrchestrator(container)
        >>> await node.initialize()
        >>>
        >>> # Execute workflow
        >>> workflow = {
        ...     "workflow_id": str(uuid4()),
        ...     "steps": [
        ...         {"step_id": "step1", "operation": "fetch_data"},
        ...         {"step_id": "step2", "operation": "process_data"},
        ...         {"step_id": "step3", "operation": "store_results"}
        ...     ]
        ... }
        >>> result = await node.execute_orchestration(workflow)
        >>> print(f"Status: {result['status']}")
    """

    def __init__(self, container: ModelContainer):
        """Initialize node with container and mixins."""
        # Initialize base classes (Node + Mixins)
        super().__init__(container)

        # Initialize logger
        self.logger = logging.getLogger(self.__class__.__name__)

        # Initialize metrics tracking
        self._workflow_metrics = {
            "total_workflows": 0,
            "completed_workflows": 0,
            "failed_workflows": 0,
            "total_steps_executed": 0,
            "average_duration_ms": 0.0,
        }

        # Initialize mixin components (if available)
        if hasattr(self, "initialize_health_checks"):
            self.initialize_health_checks()

    async def initialize(self) -> None:
        """Initialize node resources and mixins."""
        # Initialize base NodeOrchestrator
        await super().initialize()

        self.logger.info(f"Initializing {self.__class__.__name__}")

        # Setup health checks (if mixin available)
        if hasattr(self, "get_health_checks"):
            health_checks = self.get_health_checks()
            for check_name, check_func in health_checks:
                if hasattr(self, "register_health_check"):
                    self.register_health_check(check_name, check_func)

        self.logger.info(
            f"{self.__class__.__name__} initialized successfully",
            extra={"component": "orchestrator"},
        )

    async def shutdown(self) -> None:
        """Shutdown node and cleanup resources."""
        self.logger.info(f"Shutting down {self.__class__.__name__}")

        # Log final metrics
        self.logger.info("Final workflow metrics", extra=self._workflow_metrics)

        # Shutdown base NodeOrchestrator
        await super().shutdown()

    def get_health_checks(self) -> list[tuple[str, Any]]:
        """
        Register health checks for this node.

        Returns:
            List of (check_name, check_function) tuples
        """
        return [
            ("self", self._check_self_health),
            ("database", self._check_database_health),
            ("kafka", self._check_kafka_health),
            ("external_service", self._check_external_service_health),
        ]

    async def _check_self_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check orchestrator's own health status."""
        try:
            node_id = getattr(self, "node_id", None)
            container = getattr(self, "container", None)

            if not node_id:
                return (
                    HealthStatus.UNHEALTHY,
                    "Node ID not set",
                    {"missing": "node_id"},
                )

            if not container:
                return (
                    HealthStatus.DEGRADED,
                    "Container not available",
                    {"missing": "container"},
                )

            # Include metrics in health check
            return (
                HealthStatus.HEALTHY,
                "Orchestrator operational",
                {
                    "node_id": str(node_id),
                    "workflows_processed": self._workflow_metrics["total_workflows"],
                    "success_rate": (
                        self._workflow_metrics["completed_workflows"]
                        / max(self._workflow_metrics["total_workflows"], 1)
                    ),
                },
            )
        except Exception as e:
            self.logger.error(f"Self health check failed: {e}")
            return (
                HealthStatus.UNHEALTHY,
                f"Health check failed: {e!s}",
                {"error": str(e)},
            )

    async def _check_database_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check database connection health."""
        try:
            # Get database client from container (if available)
            db_client = getattr(self.container, "db_client", None)

            if not db_client:
                return (
                    HealthStatus.DEGRADED,
                    "Database client not configured",
                    {"configured": False},
                )

            # Use common health check helper
            if callable(check_database_connection):
                return await check_database_connection(db_client, timeout_seconds=3.0)

            return (
                HealthStatus.HEALTHY,
                "Database connection healthy",
                {"configured": True},
            )

        except Exception as e:
            return (
                HealthStatus.UNHEALTHY,
                f"Database check failed: {e!s}",
                {"error": str(e)},
            )

    async def _check_kafka_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check Kafka connection health."""
        try:
            # Get Kafka client from container (if available)
            kafka_client = getattr(self.container, "kafka_producer", None)

            if not kafka_client:
                return (
                    HealthStatus.DEGRADED,
                    "Kafka client not configured",
                    {"configured": False},
                )

            # Use common health check helper
            if callable(check_kafka_connection):
                return await check_kafka_connection(kafka_client, timeout_seconds=3.0)

            return (
                HealthStatus.HEALTHY,
                "Kafka connection healthy",
                {"configured": True},
            )

        except Exception as e:
            return (
                HealthStatus.DEGRADED,
                f"Kafka check failed: {e!s}",
                {"error": str(e)},
            )

    async def _check_external_service_health(
        self,
    ) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check external service health."""
        try:
            # Example: Check metadata stamping service
            service_url = "http://localhost:8057"

            # Use common health check helper
            if callable(check_http_service):
                return await check_http_service(service_url, timeout_seconds=3.0)

            return (
                HealthStatus.DEGRADED,
                "External service check not available",
                {"service_url": service_url},
            )

        except Exception as e:
            return (
                HealthStatus.DEGRADED,
                f"External service check failed: {e!s}",
                {"error": str(e)},
            )

    async def execute_orchestration(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """
        Execute workflow orchestration with full observability.

        This is the main business logic method for orchestrating workflows.

        Args:
            input_data: Workflow definition with steps

        Returns:
            Workflow execution results

        Example:
            >>> result = await node.execute_orchestration({
            ...     "workflow_id": "wf-123",
            ...     "steps": [
            ...         {"step_id": "step1", "operation": "fetch"},
            ...         {"step_id": "step2", "operation": "process"}
            ...     ]
            ... })
        """
        start_time = time.time()
        workflow_id = input_data.get("workflow_id", str(uuid4()))
        correlation_id = input_data.get("correlation_id", str(uuid4()))

        try:
            self.logger.info(
                "Starting workflow execution",
                extra={
                    "workflow_id": workflow_id,
                    "correlation_id": correlation_id,
                    "step_count": len(input_data.get("steps", [])),
                },
            )

            # Update metrics
            self._workflow_metrics["total_workflows"] += 1

            # Extract steps
            steps = input_data.get("steps", [])
            step_results = []
            completed_steps = 0
            failed_steps = 0

            # Execute steps sequentially (can be parallelized in production)
            for step in steps:
                step_result = await self._execute_step(step, correlation_id)
                step_results.append(step_result)

                if step_result.get("status") == "success":
                    completed_steps += 1
                else:
                    failed_steps += 1

                self._workflow_metrics["total_steps_executed"] += 1

            # Determine overall status
            if failed_steps == 0:
                status = "completed"
                self._workflow_metrics["completed_workflows"] += 1
            elif completed_steps > 0:
                status = "partial"
            else:
                status = "failed"
                self._workflow_metrics["failed_workflows"] += 1

            duration_ms = (time.time() - start_time) * 1000

            # Update average duration
            total = self._workflow_metrics["total_workflows"]
            current_avg = self._workflow_metrics["average_duration_ms"]
            self._workflow_metrics["average_duration_ms"] = (
                current_avg * (total - 1) + duration_ms
            ) / total

            self.logger.info(
                "Workflow execution completed",
                extra={
                    "workflow_id": workflow_id,
                    "correlation_id": correlation_id,
                    "status": status,
                    "completed_steps": completed_steps,
                    "failed_steps": failed_steps,
                    "duration_ms": duration_ms,
                },
            )

            return {
                "workflow_id": workflow_id,
                "status": status,
                "completed_steps": completed_steps,
                "failed_steps": failed_steps,
                "total_duration_ms": duration_ms,
                "step_results": step_results,
            }

        except Exception as e:
            self.logger.error(
                f"Workflow execution failed: {e}",
                extra={
                    "workflow_id": workflow_id,
                    "correlation_id": correlation_id,
                    "error": str(e),
                },
            )

            self._workflow_metrics["failed_workflows"] += 1

            duration_ms = (time.time() - start_time) * 1000

            return {
                "workflow_id": workflow_id,
                "status": "failed",
                "completed_steps": 0,
                "failed_steps": len(input_data.get("steps", [])),
                "total_duration_ms": duration_ms,
                "step_results": [],
                "error": str(e),
            }

    async def _execute_step(
        self, step: dict[str, Any], correlation_id: str
    ) -> dict[str, Any]:
        """
        Execute a single workflow step.

        Args:
            step: Step definition with operation
            correlation_id: Request correlation ID

        Returns:
            Step execution result
        """
        step_id = step.get("step_id", "unknown")
        operation = step.get("operation", "unknown")

        try:
            self.logger.debug(
                f"Executing step {step_id}",
                extra={
                    "step_id": step_id,
                    "operation": operation,
                    "correlation_id": correlation_id,
                },
            )

            # TODO: Implement actual step execution logic
            # This is a simple simulation
            await asyncio.sleep(0.01)

            return {
                "step_id": step_id,
                "operation": operation,
                "status": "success",
                "result": f"Step {step_id} completed",
            }

        except Exception as e:
            self.logger.error(
                f"Step execution failed: {e}",
                extra={"step_id": step_id, "operation": operation, "error": str(e)},
            )

            return {
                "step_id": step_id,
                "operation": operation,
                "status": "failed",
                "error": str(e),
            }

    def get_metrics(self) -> dict[str, Any]:
        """
        Get current workflow metrics.

        Returns:
            Dictionary with metrics data
        """
        return {**self._workflow_metrics, "timestamp": time.time()}


# Example usage and testing
async def main():
    """Example usage of the workflow orchestrator."""
    print("=" * 60)
    print("Advanced Orchestrator with Metrics - Example")
    print("=" * 60)
    print()

    # Create container and node
    container = ModelContainer()
    node = NodeWorkflowOrchestrator(container)

    try:
        # Initialize node
        print("1. Initializing orchestrator...")
        await node.initialize()
        print("   ✓ Orchestrator initialized")
        print()

        # Check health
        print("2. Checking health...")
        if hasattr(node, "check_health"):
            health_result = await node.check_health()
            health_dict = health_result.to_dict()
            print(f"   Overall Status: {health_dict['overall_status']}")
            print(f"   Components: {len(health_dict['components'])}")
            for comp in health_dict["components"]:
                status_icon = "✓" if comp["status"] == "healthy" else "✗"
                critical = " (critical)" if comp["is_critical"] else ""
                print(f"     {status_icon} {comp['name']}: {comp['status']}{critical}")
        else:
            print("   ⚠ Health check mixin not available")
        print()

        # Execute workflow
        print("3. Executing workflow...")
        workflow = {
            "workflow_id": str(uuid4()),
            "correlation_id": str(uuid4()),
            "steps": [
                {"step_id": "step1", "operation": "fetch_data"},
                {"step_id": "step2", "operation": "process_data"},
                {"step_id": "step3", "operation": "store_results"},
            ],
        }
        result = await node.execute_orchestration(workflow)
        print(f"   Workflow ID: {result['workflow_id']}")
        print(f"   Status: {result['status']}")
        print(f"   Completed Steps: {result['completed_steps']}")
        print(f"   Failed Steps: {result['failed_steps']}")
        print(f"   Duration: {result['total_duration_ms']:.2f}ms")
        print()

        # Get metrics
        print("4. Workflow metrics...")
        metrics = node.get_metrics()
        print(f"   Total Workflows: {metrics['total_workflows']}")
        print(f"   Completed: {metrics['completed_workflows']}")
        print(f"   Failed: {metrics['failed_workflows']}")
        print(f"   Steps Executed: {metrics['total_steps_executed']}")
        print(f"   Average Duration: {metrics['average_duration_ms']:.2f}ms")
        print()

        # Shutdown
        print("5. Shutting down...")
        await node.shutdown()
        print("   ✓ Orchestrator shutdown complete")

    except Exception as e:
        print(f"   ✗ Error: {e}")
        import traceback

        traceback.print_exc()

    print()
    print("=" * 60)
    print("Example complete!")
    print("=" * 60)


if __name__ == "__main__":
    # Run example
    asyncio.run(main())
