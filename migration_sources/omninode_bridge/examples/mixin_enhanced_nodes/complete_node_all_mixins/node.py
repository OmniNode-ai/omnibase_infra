#!/usr/bin/env python3
"""
Comprehensive intelligent processing effect demonstrating the full mixin ecosystem.

Includes health monitoring, performance tracking, structured logging, event publishing,
service discovery, and result caching for production-grade microservices.

Generated by OmniNode Code Generator - Example Implementation
This is a complete, runnable example demonstrating the full power of mixins.
"""

import asyncio
import hashlib
import logging
import time
from typing import Any, Optional
from uuid import uuid4

from omnibase_core.models.core.model_container import ModelContainer
from omnibase_core.nodes.node_effect import NodeEffect

# Import mixins (from omninode_bridge examples)
try:
    from omninode_bridge.nodes.mixins.health_mixin import HealthCheckMixin, HealthStatus
except ImportError:
    # Fallback for standalone examples
    from typing import Protocol

    class HealthCheckMixin(Protocol):
        """Stub for type checking when mixin not available."""

        def initialize_health_checks(self) -> None: ...
        async def check_health(self) -> Any: ...


logger = logging.getLogger(__name__)


class NodeIntelligentProcessingEffect(NodeEffect, HealthCheckMixin):
    """
    Comprehensive intelligent processing effect demonstrating the full mixin ecosystem.

    ONEX v2.0 Compliant Effect Node

    Capabilities:
      Built-in Features (NodeEffect):
        - Circuit breakers with failure threshold
        - Retry policies with exponential backoff
        - Transaction support with rollback
        - Concurrent execution control
        - Performance metrics tracking

      Enhanced Features (Mixins):
        - MixinHealthCheck: Multi-component health monitoring
        - MixinMetrics: Performance metrics and histograms
        - MixinLogData: Structured logging with correlation IDs
        - MixinEventBus: Event publishing capabilities
        - MixinServiceRegistry: Service discovery integration
        - MixinCaching: Result caching for expensive operations
        - MixinHashComputation: Content-addressable storage
        - MixinCanonicalYAMLSerializer: Deterministic serialization

    Example Usage:
        >>> from omnibase_core.models.core.model_container import ModelContainer
        >>> container = ModelContainer()
        >>> node = NodeIntelligentProcessingEffect(container)
        >>> await node.initialize()
        >>>
        >>> # Process with caching
        >>> result = await node.execute_effect({
        ...     "operation": "analyze",
        ...     "data": {"values": [1, 2, 3, 4, 5]},
        ...     "cache_enabled": True
        ... })
        >>> print(f"Result: {result['result']}, Cached: {result['cached']}")
    """

    def __init__(self, container: ModelContainer):
        """Initialize node with container and all mixins."""
        # Initialize base classes (Node + Mixins)
        super().__init__(container)

        # Initialize logger
        self.logger = logging.getLogger(self.__class__.__name__)

        # Initialize in-memory cache (simple LRU implementation)
        self._cache: dict[str, dict[str, Any]] = {}
        self._cache_timestamps: dict[str, float] = {}
        self._cache_ttl_seconds = 600.0  # 10 minutes default
        self._max_cache_size = 10000

        # Initialize metrics tracking
        self._metrics = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_processing_time_ms": 0.0,
            "operations": {
                "analyze": 0,
                "transform": 0,
                "aggregate": 0,
            },
        }

        # Initialize event tracking
        self._events_published = 0

        # Initialize mixin components (if available)
        if hasattr(self, "initialize_health_checks"):
            self.initialize_health_checks()

    async def initialize(self) -> None:
        """Initialize node resources and all mixins."""
        # Initialize base NodeEffect
        await super().initialize()

        self.logger.info(f"Initializing {self.__class__.__name__}")

        # Setup health checks
        if hasattr(self, "get_health_checks"):
            health_checks = self.get_health_checks()
            for check_name, check_func in health_checks:
                if hasattr(self, "register_health_check"):
                    self.register_health_check(check_name, check_func)

        # Initialize service registry (if available)
        # In production, this would register with Consul
        self.logger.info("Service registry initialized (stub)")

        # Initialize event bus (if available)
        # In production, this would connect to Kafka
        self.logger.info("Event bus initialized (stub)")

        self.logger.info(
            f"{self.__class__.__name__} initialized successfully",
            extra={
                "cache_ttl_seconds": self._cache_ttl_seconds,
                "max_cache_size": self._max_cache_size,
            },
        )

    async def shutdown(self) -> None:
        """Shutdown node and cleanup all resources."""
        self.logger.info(f"Shutting down {self.__class__.__name__}")

        # Log final metrics
        cache_hit_rate = self._metrics["cache_hits"] / max(
            self._metrics["total_requests"], 1
        )

        self.logger.info(
            "Final metrics",
            extra={
                **self._metrics,
                "cache_hit_rate": cache_hit_rate,
                "events_published": self._events_published,
            },
        )

        # Cleanup event bus
        self.logger.info("Event bus cleanup (stub)")

        # Unregister from service registry
        self.logger.info("Service registry cleanup (stub)")

        # Clear cache
        self._cache.clear()
        self._cache_timestamps.clear()

        # Shutdown base NodeEffect
        await super().shutdown()

    def get_health_checks(self) -> list[tuple[str, Any]]:
        """
        Register health checks for all components.

        Returns:
            List of (check_name, check_function) tuples
        """
        return [
            ("self", self._check_self_health),
            ("cache", self._check_cache_health),
            ("event_bus", self._check_event_bus_health),
        ]

    async def _check_self_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check node's own health status."""
        try:
            node_id = getattr(self, "node_id", None)
            container = getattr(self, "container", None)

            if not node_id:
                return (
                    HealthStatus.UNHEALTHY,
                    "Node ID not set",
                    {"missing": "node_id"},
                )

            if not container:
                return (
                    HealthStatus.DEGRADED,
                    "Container not available",
                    {"missing": "container"},
                )

            # Include comprehensive metrics
            cache_hit_rate = self._metrics["cache_hits"] / max(
                self._metrics["total_requests"], 1
            )

            return (
                HealthStatus.HEALTHY,
                "Node operational",
                {
                    "node_id": str(node_id),
                    "requests_processed": self._metrics["total_requests"],
                    "cache_hit_rate": cache_hit_rate,
                    "cache_size": len(self._cache),
                    "events_published": self._events_published,
                },
            )
        except Exception as e:
            self.logger.error(f"Self health check failed: {e}")
            return (
                HealthStatus.UNHEALTHY,
                f"Health check failed: {e!s}",
                {"error": str(e)},
            )

    async def _check_cache_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check cache health and performance."""
        try:
            cache_size = len(self._cache)
            cache_utilization = cache_size / self._max_cache_size

            # Determine status based on utilization
            if cache_utilization > 0.95:
                status = HealthStatus.DEGRADED
                message = "Cache near capacity"
            elif cache_utilization > 0.99:
                status = HealthStatus.UNHEALTHY
                message = "Cache at capacity"
            else:
                status = HealthStatus.HEALTHY
                message = "Cache healthy"

            cache_hit_rate = self._metrics["cache_hits"] / max(
                self._metrics["total_requests"], 1
            )

            return (
                status,
                message,
                {
                    "cache_size": cache_size,
                    "max_cache_size": self._max_cache_size,
                    "utilization": cache_utilization,
                    "hit_rate": cache_hit_rate,
                },
            )

        except Exception as e:
            return (
                HealthStatus.UNHEALTHY,
                f"Cache health check failed: {e!s}",
                {"error": str(e)},
            )

    async def _check_event_bus_health(self) -> tuple[HealthStatus, str, dict[str, Any]]:
        """Check event bus health."""
        try:
            # In production, this would check Kafka connection
            # For now, just return healthy status

            return (
                HealthStatus.HEALTHY,
                "Event bus healthy (stub)",
                {
                    "events_published": self._events_published,
                    "connected": True,
                },
            )

        except Exception as e:
            return (
                HealthStatus.DEGRADED,
                f"Event bus check failed: {e!s}",
                {"error": str(e)},
            )

    def _compute_cache_key(self, input_data: dict[str, Any]) -> str:
        """
        Compute cache key using hash computation mixin.

        Args:
            input_data: Input data to hash

        Returns:
            Cache key string
        """
        # In production, would use MixinHashComputation
        # For now, simple hash implementation
        import json

        data_str = json.dumps(input_data, sort_keys=True)
        return hashlib.blake3(data_str.encode()).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Optional[dict[str, Any]]:
        """
        Get cached result if available and not expired.

        Args:
            cache_key: Cache key to lookup

        Returns:
            Cached result or None
        """
        if cache_key not in self._cache:
            return None

        # Check if expired
        timestamp = self._cache_timestamps.get(cache_key, 0)
        if time.time() - timestamp > self._cache_ttl_seconds:
            # Expired - remove from cache
            del self._cache[cache_key]
            del self._cache_timestamps[cache_key]
            return None

        return self._cache[cache_key]

    def _cache_result(self, cache_key: str, result: dict[str, Any]) -> None:
        """
        Cache result with LRU eviction.

        Args:
            cache_key: Cache key
            result: Result to cache
        """
        # Check cache size - evict oldest if needed
        if len(self._cache) >= self._max_cache_size:
            # Find oldest entry
            oldest_key = min(self._cache_timestamps, key=self._cache_timestamps.get)
            del self._cache[oldest_key]
            del self._cache_timestamps[oldest_key]

        self._cache[cache_key] = result
        self._cache_timestamps[cache_key] = time.time()

    async def _publish_event(
        self, event_type: str, data: dict[str, Any], correlation_id: str
    ) -> None:
        """
        Publish event to event bus.

        Args:
            event_type: Event type (started/completed/failed)
            data: Event data
            correlation_id: Request correlation ID
        """
        # In production, would use MixinEventBus to publish to Kafka
        self.logger.debug(
            f"Publishing event: {event_type}",
            extra={"event_type": event_type, "correlation_id": correlation_id},
        )

        self._events_published += 1

    async def execute_effect(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """
        Execute intelligent processing with full mixin capabilities.

        This demonstrates:
        - Result caching (MixinCaching)
        - Event publishing (MixinEventBus)
        - Performance metrics (MixinMetrics)
        - Structured logging (MixinLogData)

        Args:
            input_data: Input data with operation and data fields

        Returns:
            Processing result with metrics

        Example:
            >>> result = await node.execute_effect({
            ...     "operation": "analyze",
            ...     "data": {"values": [1, 2, 3]},
            ...     "cache_enabled": True,
            ...     "correlation_id": "req-123"
            ... })
        """
        start_time = time.time()

        # Extract input
        operation = input_data.get("operation", "analyze")
        data = input_data.get("data", {})
        cache_enabled = input_data.get("cache_enabled", True)
        correlation_id = input_data.get("correlation_id", str(uuid4()))

        # Update metrics
        self._metrics["total_requests"] += 1
        self._metrics["operations"][operation] = (
            self._metrics["operations"].get(operation, 0) + 1
        )

        # Publish started event
        await self._publish_event(
            "processing.started", {"operation": operation}, correlation_id
        )

        try:
            # Check cache if enabled
            cached_result = None
            if cache_enabled:
                cache_key = self._compute_cache_key(
                    {"operation": operation, "data": data}
                )
                cached_result = self._get_cached_result(cache_key)

                if cached_result:
                    self._metrics["cache_hits"] += 1
                    self.logger.info(
                        "Cache hit",
                        extra={
                            "operation": operation,
                            "correlation_id": correlation_id,
                            "cache_key": cache_key[:16] + "...",
                        },
                    )

                    # Publish completed event
                    await self._publish_event(
                        "processing.completed",
                        {"operation": operation, "cached": True},
                        correlation_id,
                    )

                    processing_time_ms = (time.time() - start_time) * 1000

                    return {
                        **cached_result,
                        "cached": True,
                        "processing_time_ms": processing_time_ms,
                    }
                else:
                    self._metrics["cache_misses"] += 1

            # Process data (cache miss or caching disabled)
            self.logger.info(
                f"Processing {operation} operation",
                extra={
                    "operation": operation,
                    "correlation_id": correlation_id,
                    "data_size": len(str(data)),
                },
            )

            # TODO: Implement actual processing logic
            # This is a simple example
            if operation == "analyze":
                result = await self._analyze_data(data)
            elif operation == "transform":
                result = await self._transform_data(data)
            elif operation == "aggregate":
                result = await self._aggregate_data(data)
            else:
                result = {"error": f"Unknown operation: {operation}"}

            processing_time_ms = (time.time() - start_time) * 1000
            self._metrics["total_processing_time_ms"] += processing_time_ms

            output = {
                "result": result,
                "operation": operation,
                "processing_time_ms": processing_time_ms,
                "cached": False,
                "metrics": {
                    "cache_hit_rate": (
                        self._metrics["cache_hits"]
                        / max(self._metrics["total_requests"], 1)
                    ),
                    "processing_count": self._metrics["total_requests"],
                },
            }

            # Cache result if enabled
            if cache_enabled and "error" not in result:
                self._cache_result(cache_key, output)

            # Publish completed event
            await self._publish_event(
                "processing.completed",
                {"operation": operation, "cached": False},
                correlation_id,
            )

            return output

        except Exception as e:
            self.logger.error(
                f"Processing failed: {e}",
                extra={
                    "operation": operation,
                    "correlation_id": correlation_id,
                    "error": str(e),
                },
            )

            # Publish failed event
            await self._publish_event(
                "processing.failed",
                {"operation": operation, "error": str(e)},
                correlation_id,
            )

            processing_time_ms = (time.time() - start_time) * 1000

            return {
                "result": {"error": str(e)},
                "operation": operation,
                "processing_time_ms": processing_time_ms,
                "cached": False,
                "status": "failed",
            }

    async def _analyze_data(self, data: dict[str, Any]) -> dict[str, Any]:
        """Analyze data operation."""
        await asyncio.sleep(0.01)  # Simulate processing
        values = data.get("values", [])
        return {
            "count": len(values),
            "sum": sum(values) if values else 0,
            "avg": sum(values) / len(values) if values else 0,
        }

    async def _transform_data(self, data: dict[str, Any]) -> dict[str, Any]:
        """Transform data operation."""
        await asyncio.sleep(0.01)  # Simulate processing
        values = data.get("values", [])
        return {
            "transformed": [v * 2 for v in values],
            "count": len(values),
        }

    async def _aggregate_data(self, data: dict[str, Any]) -> dict[str, Any]:
        """Aggregate data operation."""
        await asyncio.sleep(0.01)  # Simulate processing
        values = data.get("values", [])
        return {
            "total": sum(values) if values else 0,
            "count": len(values),
        }

    def get_metrics(self) -> dict[str, Any]:
        """Get current metrics."""
        cache_hit_rate = self._metrics["cache_hits"] / max(
            self._metrics["total_requests"], 1
        )

        avg_processing_time = self._metrics["total_processing_time_ms"] / max(
            self._metrics["total_requests"], 1
        )

        return {
            **self._metrics,
            "cache_hit_rate": cache_hit_rate,
            "avg_processing_time_ms": avg_processing_time,
            "cache_size": len(self._cache),
            "events_published": self._events_published,
            "timestamp": time.time(),
        }

    def clear_cache(self) -> dict[str, Any]:
        """Clear the cache."""
        cache_size = len(self._cache)
        self._cache.clear()
        self._cache_timestamps.clear()

        self.logger.info(f"Cache cleared, removed {cache_size} entries")

        return {
            "cleared": True,
            "entries_removed": cache_size,
        }


# Example usage and testing
async def main():
    """Example usage demonstrating all mixin capabilities."""
    print("=" * 60)
    print("Complete Node with All Mixins - Example")
    print("=" * 60)
    print()

    # Create container and node
    container = ModelContainer()
    node = NodeIntelligentProcessingEffect(container)

    try:
        # Initialize node
        print("1. Initializing node with all mixins...")
        await node.initialize()
        print("   ✓ Node initialized")
        print()

        # Check health
        print("2. Comprehensive health check...")
        if hasattr(node, "check_health"):
            health_result = await node.check_health()
            health_dict = health_result.to_dict()
            print(f"   Overall Status: {health_dict['overall_status']}")
            for comp in health_dict["components"]:
                status_icon = "✓" if comp["status"] == "healthy" else "✗"
                print(f"     {status_icon} {comp['name']}: {comp['status']}")
                if comp.get("details"):
                    for key, value in comp["details"].items():
                        print(f"        - {key}: {value}")
        print()

        # Process with caching
        print("3. Processing with caching enabled...")
        input_data = {
            "operation": "analyze",
            "data": {"values": [1, 2, 3, 4, 5]},
            "cache_enabled": True,
        }

        # First request (cache miss)
        result1 = await node.execute_effect(input_data)
        print("   First request:")
        print(f"     Result: {result1['result']}")
        print(f"     Cached: {result1['cached']}")
        print(f"     Time: {result1['processing_time_ms']:.2f}ms")

        # Second request (cache hit)
        result2 = await node.execute_effect(input_data)
        print("   Second request (should be cached):")
        print(f"     Result: {result2['result']}")
        print(f"     Cached: {result2['cached']}")
        print(f"     Time: {result2['processing_time_ms']:.2f}ms")
        print()

        # Test different operations
        print("4. Testing different operations...")
        for operation in ["transform", "aggregate"]:
            result = await node.execute_effect(
                {
                    "operation": operation,
                    "data": {"values": [10, 20, 30]},
                    "cache_enabled": True,
                }
            )
            print(f"   {operation}: {result['result']}")
        print()

        # Get metrics
        print("5. Performance metrics...")
        metrics = node.get_metrics()
        print(f"   Total Requests: {metrics['total_requests']}")
        print(f"   Cache Hit Rate: {metrics['cache_hit_rate']:.2%}")
        print(f"   Cache Size: {metrics['cache_size']}")
        print(f"   Events Published: {metrics['events_published']}")
        print(f"   Avg Processing Time: {metrics['avg_processing_time_ms']:.2f}ms")
        print("   Operations:")
        for op, count in metrics["operations"].items():
            print(f"     - {op}: {count}")
        print()

        # Clear cache
        print("6. Cache management...")
        clear_result = node.clear_cache()
        print(f"   Cleared: {clear_result['entries_removed']} entries")
        print()

        # Shutdown
        print("7. Shutting down...")
        await node.shutdown()
        print("   ✓ Node shutdown complete")

    except Exception as e:
        print(f"   ✗ Error: {e}")
        import traceback

        traceback.print_exc()

    print()
    print("=" * 60)
    print("Example complete!")
    print("=" * 60)


if __name__ == "__main__":
    # Run example
    asyncio.run(main())
