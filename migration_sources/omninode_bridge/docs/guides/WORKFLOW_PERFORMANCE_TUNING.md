# Workflow Performance Tuning Guide

**Version**: 1.0
**Status**: ‚úÖ Production-Ready
**Last Updated**: 2025-11-06
**Reading Time**: 15 minutes

---

## Overview

Optimize Phase 4 Workflows components for maximum performance. This guide provides configuration tuning, profiling techniques, and optimization strategies for production deployments.

---

## Table of Contents

1. [Performance Targets](#performance-targets)
2. [Staged Parallel Executor](#staged-parallel-executor-tuning)
3. [Template Manager](#template-manager-tuning)
4. [Validation Pipeline](#validation-pipeline-tuning)
5. [AI Quorum](#ai-quorum-tuning)
6. [System-Level Optimization](#system-level-optimization)

---

## Performance Targets

### Component Targets

| Component | Metric | Target | Actual |
|-----------|--------|--------|--------|
| **StagedParallelExecutor** | Full pipeline (3 contracts) | <5s | 4.7s ‚úÖ |
| | Stage transition | <100ms | <100ms ‚úÖ |
| | Parallelism speedup | 2-4x | 2.25-4.17x ‚úÖ |
| **TemplateManager** | Cached lookup | <1ms | <2ms* ‚úÖ |
| | Cache hit rate | 85-95% | 80%+ ‚úÖ |
| | Template rendering | <10ms | <10ms ‚úÖ |
| **ValidationPipeline** | Total validation | <200ms | <150ms ‚úÖ |
| **AIQuorum** | Standard models | 2-10s | 1-2.5s ‚úÖ |
| | Parallel speedup | 2-4x | 2-4x ‚úÖ |

*Includes test framework overhead

---

## Staged Parallel Executor Tuning

### 1. Optimize Parallel Task Count

```python
# Default: 10 parallel tasks
executor = StagedParallelExecutor(
    scheduler=scheduler,
    max_parallel_tasks=10,  # Adjust based on CPU cores
)

# Rule of thumb: max_parallel_tasks = CPU cores - 1
import os
cpu_count = os.cpu_count()
optimal_tasks = max(3, cpu_count - 1)

executor = StagedParallelExecutor(
    max_parallel_tasks=optimal_tasks,
)
```

### 2. Stage Timeout Configuration

```python
# Aggressive timeout for development
executor = StagedParallelExecutor(
    stage_timeout_seconds=60,  # 1 minute
)

# Conservative timeout for production
executor = StagedParallelExecutor(
    stage_timeout_seconds=300,  # 5 minutes
)

# Per-stage timeout override
stage = WorkflowStage(
    stage_id="models",
    timeout_seconds=120,  # 2 minutes for this stage
)
```

### 3. Fast-Fail vs Collect-All

```python
# Development: Stop on first error (fast iteration)
executor = StagedParallelExecutor(
    enable_fast_fail=True,
)

# Production: Collect all errors (better debugging)
executor = StagedParallelExecutor(
    enable_fast_fail=False,
)
```

### 4. Batch Processing Optimization

```python
# Process contracts in batches to avoid CPU saturation
contracts = [...]  # 20 contracts

batch_size = max_parallel_tasks  # 10
batches = [contracts[i:i+batch_size] for i in range(0, len(contracts), batch_size)]

for batch in batches:
    steps = [
        WorkflowStep(
            step_id=f"gen_{c.id}",
            step_type=EnumStepType.PARALLEL,
            handler=generate_handler,
            context={"contract": c},
        )
        for c in batch
    ]

    stage = WorkflowStage(steps=steps)
    result = await executor.execute_stage(stage, context)
```

---

## Template Manager Tuning

### 1. Cache Size Optimization

```python
# Small cache for development (save memory)
manager = TemplateManager(cache_size=50)

# Medium cache for typical use (100 templates = ~2MB)
manager = TemplateManager(cache_size=100)

# Large cache for heavy workloads
manager = TemplateManager(cache_size=200)

# Monitor cache statistics
stats = manager.get_cache_stats()
print(f"Hit rate: {stats.hit_rate:.1%}")
print(f"Current size: {stats.current_size}/{stats.max_size}")

# If hit_rate < 85%, consider increasing cache_size
if stats.hit_rate < 0.85:
    # Increase cache size
    pass
```

### 2. Template Preloading

```python
# Preload at startup (warm cache)
await manager.preload_templates([
    ("node_effect_v1", TemplateType.EFFECT),
    ("node_compute_v1", TemplateType.COMPUTE),
    ("node_reducer_v1", TemplateType.REDUCER),
    ("node_orchestrator_v1", TemplateType.ORCHESTRATOR),
])

# Preload based on usage patterns
async def preload_common_templates():
    """Preload templates based on 80/20 rule."""
    # Most common 20% of templates account for 80% of usage
    common_templates = [
        ("node_effect_v1", TemplateType.EFFECT),
        ("node_compute_v1", TemplateType.COMPUTE),
    ]
    await manager.preload_templates(common_templates)
```

### 3. Template Invalidation

```python
# Invalidate changed templates in development
if development_mode:
    # Force reload on every request
    template = await manager.load_template(
        template_id="...",
        force_reload=True,
    )

# Clear cache on template updates
await manager.invalidate_template("node_effect_v1")

# Full cache clear (use sparingly)
await manager.clear_cache()
```

### 4. Monitor Cache Performance

```python
import time

# Track cache performance over time
async def monitor_cache():
    while True:
        stats = manager.get_cache_stats()
        logger.info(
            f"Cache: hit_rate={stats.hit_rate:.1%}, "
            f"size={stats.current_size}/{stats.max_size}, "
            f"avg_get={stats.avg_get_time_ms:.2f}ms"
        )
        await asyncio.sleep(60)  # Log every minute

asyncio.create_task(monitor_cache())
```

---

## Validation Pipeline Tuning

### 1. Validator Selection

```python
# Minimal validation (development)
pipeline = ValidationPipeline(
    validators=[
        CompletenessValidator(),  # Fast: <50ms
    ],
)

# Standard validation (staging)
pipeline = ValidationPipeline(
    validators=[
        CompletenessValidator(),     # <50ms
        QualityValidator(),           # <100ms
    ],
)

# Comprehensive validation (production)
pipeline = ValidationPipeline(
    validators=[
        CompletenessValidator(),     # <50ms
        QualityValidator(),           # <100ms
        ONEXComplianceValidator(),    # <100ms
        SecurityValidator(),          # <50ms
    ],
)
```

### 2. Fast-Fail Optimization

```python
# Development: Stop on first error
pipeline = ValidationPipeline(
    validators=[...],
    enable_fast_fail=True,  # Faster iteration
)

# Production: Collect all errors
pipeline = ValidationPipeline(
    validators=[...],
    enable_fast_fail=False,  # Better debugging
)
```

### 3. Quality Thresholds

```python
# Strict quality for production
pipeline = ValidationPipeline(
    validators=[
        QualityValidator(
            min_score=0.8,  # 80% quality minimum
            max_complexity=10,
        ),
    ],
)

# Lenient quality for development
pipeline = ValidationPipeline(
    validators=[
        QualityValidator(
            min_score=0.6,  # 60% quality minimum
            max_complexity=15,
        ),
    ],
)
```

---

## AI Quorum Tuning

### 1. Model Selection

```python
# Fast development: Single model
quorum = AIQuorum(
    model_configs=[
        ModelConfig(
            model_id="glm-air",
            weight=1.0,
            timeout=10,  # Faster timeout
        ),
    ],
    pass_threshold=0.5,
)

# Production: 4 models
quorum = AIQuorum(
    model_configs=DEFAULT_QUORUM_MODELS,
    pass_threshold=0.6,
)

# High-stakes: Stricter threshold
quorum = AIQuorum(
    model_configs=DEFAULT_QUORUM_MODELS,
    pass_threshold=0.7,  # 70% consensus
)
```

### 2. Timeout Configuration

```python
# Aggressive timeout (faster, may fail more)
config = ModelConfig(
    model_id="gemini",
    timeout=15,  # 15 seconds
    max_retries=1,
)

# Conservative timeout (slower, more reliable)
config = ModelConfig(
    model_id="gemini",
    timeout=60,  # 60 seconds
    max_retries=3,
)
```

### 3. Minimum Participation

```python
# Lenient: Continue with 1-2 models
quorum = AIQuorum(
    model_configs=DEFAULT_QUORUM_MODELS,
    min_participating_weight=2.0,  # Require >=2.0 weight
)

# Strict: Require 3+ models
quorum = AIQuorum(
    model_configs=DEFAULT_QUORUM_MODELS,
    min_participating_weight=4.0,  # Require >=4.0 weight
)
```

### 4. Cost Optimization

```python
# Use cheaper models for development
DEV_QUORUM_MODELS = [
    ModelConfig(
        model_id="glm-air",
        weight=1.0,
        # Cheaper than Gemini
    ),
]

# Use expensive models only for production
PROD_QUORUM_MODELS = DEFAULT_QUORUM_MODELS  # All 4 models

# Conditional AI Quorum
if environment == "production":
    quorum = AIQuorum(model_configs=PROD_QUORUM_MODELS)
else:
    quorum = None  # Skip AI validation in dev
```

---

## System-Level Optimization

### 1. CPU Optimization

```python
# Set process affinity (Linux)
import os
os.sched_setaffinity(0, {0, 1, 2, 3})  # Use cores 0-3

# Increase thread pool size for I/O
import concurrent.futures
executor_pool = concurrent.futures.ThreadPoolExecutor(
    max_workers=20  # More threads for I/O-bound tasks
)
```

### 2. Memory Optimization

```python
# Limit cache sizes
template_manager = TemplateManager(cache_size=50)  # Smaller cache

# Clear caches periodically
async def periodic_cache_clear():
    while True:
        await asyncio.sleep(3600)  # Every hour
        await template_manager.clear_cache()
        # Cache will rebuild from usage
```

### 3. I/O Optimization

```python
# Use SSD for template storage
template_manager = TemplateManager(
    template_dir="/mnt/ssd/templates"  # SSD mount point
)

# Batch file writes
async def batch_write_files(files: list[tuple[str, str]]):
    """Write multiple files in batch."""
    tasks = [
        write_file(path, content)
        for path, content in files
    ]
    await asyncio.gather(*tasks)
```

### 4. Network Optimization

```python
# Connection pooling for API calls
import aiohttp

session = aiohttp.ClientSession(
    connector=aiohttp.TCPConnector(
        limit=100,  # Max 100 connections
        limit_per_host=10,  # Max 10 per host
    ),
)

# Use for all LLM clients
client = GeminiClient(session=session)
```

---

## Profiling

### 1. Workflow Profiling

```python
import cProfile
import pstats

# Profile workflow execution
profiler = cProfile.Profile()
profiler.enable()

result = await executor.execute_workflow(...)

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 functions
```

### 2. Memory Profiling

```python
import tracemalloc

# Track memory usage
tracemalloc.start()

result = await executor.execute_workflow(...)

current, peak = tracemalloc.get_traced_memory()
print(f"Current: {current / 1024 / 1024:.2f}MB")
print(f"Peak: {peak / 1024 / 1024:.2f}MB")

tracemalloc.stop()
```

### 3. Metrics-Based Profiling

```python
# Use built-in metrics
stats = executor.get_statistics()
print(f"Workflows: {stats['total_workflows']}")
print(f"Avg duration: {stats['avg_workflow_duration_ms']:.2f}ms")
print(f"Avg speedup: {stats['avg_speedup']:.2f}x")

# Identify bottlenecks
if stats['avg_speedup'] < 2.0:
    print("‚ö†Ô∏è Low parallelism speedup - check step types")

cache_stats = template_manager.get_cache_stats()
if cache_stats.hit_rate < 0.85:
    print("‚ö†Ô∏è Low cache hit rate - increase cache size or preload")

quorum_stats = quorum.get_statistics()
if quorum_stats['avg_duration_ms'] > 5000:
    print("‚ö†Ô∏è High quorum latency - reduce timeout or disable")
```

---

## Optimization Checklist

### Pre-Production Checklist

- [ ] Cache size optimized (hit rate >=85%)
- [ ] Templates preloaded at startup
- [ ] Parallel task count matches CPU cores
- [ ] Timeouts configured appropriately
- [ ] AI Quorum disabled for development
- [ ] Metrics monitoring enabled
- [ ] Error handling tested
- [ ] Resource cleanup on shutdown

### Performance Validation

- [ ] Full pipeline <5s (3 contracts)
- [ ] Template cache hit rate >=85%
- [ ] Validation pipeline <200ms
- [ ] AI Quorum (if enabled) <10s
- [ ] Parallelism speedup >=2x
- [ ] Memory usage <1GB
- [ ] CPU utilization 70-90%

---

## Performance Profiling (Weeks 7-8)

### Performance Profiler

Identify bottlenecks with comprehensive profiling (<5% overhead):

```python
from omninode_bridge.agents.workflows import PerformanceProfiler

# Initialize profiler
profiler = PerformanceProfiler(
    metrics_collector=metrics,
    enable_memory_profiling=True,  # Adds ~5% overhead
    enable_io_profiling=True,
)

# Profile workflow
async with profiler.profile_operation("full_workflow"):
    result = await executor.execute_workflow(
        workflow_id="session-1",
        stages=stages,
    )

# Get timing stats
timing_stats = profiler.get_timing_stats("full_workflow")
print(f"p50: {timing_stats.p50_ms:.2f}ms")
print(f"p95: {timing_stats.p95_ms:.2f}ms")
print(f"p99: {timing_stats.p99_ms:.2f}ms")
```

### Hot Path Analysis

Identify performance bottlenecks (>20% of total time):

```python
# Profile with multiple iterations
profile_data = await profiler.profile_workflow(
    workflow_func=execute_workflow,
    iterations=10,  # Statistical significance
)

# Analyze hot paths
hot_paths = profiler.analyze_hot_paths(profile_data)

for hp in hot_paths:
    print(f"üî• {hp.operation_name}:")
    print(f"   Bottleneck score: {hp.bottleneck_score:.1%}")
    print(f"   Total time: {hp.total_time_ms:.2f}ms")
    print(f"   Call count: {hp.call_count}")
```

**Hot Path Criteria**:
- Operation takes >20% of total workflow time
- Called frequently (>10 times)
- High variance (p99 >> p50)

### Memory Profiling

Track memory usage and identify leaks:

```python
# Enable memory profiling
profiler = PerformanceProfiler(
    enable_memory_profiling=True,  # ~5% overhead
)

async with profiler.profile_operation("workflow"):
    result = await execute_workflow(...)

# Get memory stats
memory_stats = profiler.get_memory_stats("workflow")
print(f"Peak memory: {memory_stats.peak_mb:.2f}MB")
print(f"Average memory: {memory_stats.avg_mb:.2f}MB")
print(f"Memory growth: {memory_stats.growth_mb:.2f}MB")

# Check for memory leaks
if memory_stats.growth_mb > 10.0:
    print("‚ö†Ô∏è Possible memory leak detected")
```

### I/O Profiling

Analyze I/O operations and identify synchronous bottlenecks:

```python
# Get I/O statistics
io_stats = profiler.get_io_stats()

print(f"Total I/O operations: {io_stats.total_operations}")
print(f"Async I/O: {io_stats.async_count} ({io_stats.async_ratio:.1%})")
print(f"Sync I/O: {io_stats.sync_count}")
print(f"Avg I/O time: {io_stats.avg_time_ms:.2f}ms")

# Target: 80%+ async ratio
if io_stats.async_ratio < 0.8:
    print("‚ö†Ô∏è Low async I/O ratio - convert sync operations to async")
```

### Cache Profiling

Analyze cache efficiency:

```python
# Get cache statistics
cache_stats = profiler.get_cache_stats()

print(f"Cache hit rate: {cache_stats.hit_rate:.1%}")
print(f"Total requests: {cache_stats.total_requests}")
print(f"Avg lookup time: {cache_stats.avg_lookup_ms:.2f}ms")

# Target: 95%+ hit rate
if cache_stats.hit_rate < 0.95:
    print("‚ö†Ô∏è Low cache hit rate - increase cache size or preload templates")
```

---

## Performance Optimization (Weeks 7-8)

### Automatic Optimization

Enable automatic optimizations for 2-3x speedup:

```python
from omninode_bridge.agents.workflows import PerformanceOptimizer

# Initialize optimizer
optimizer = PerformanceOptimizer(
    profiler=profiler,
    template_manager=template_manager,
    staged_executor=executor,
    metrics_collector=metrics,
    auto_optimize=True,  # Enable automatic optimizations
)

# Profile and optimize
report = await optimizer.optimize_workflow(
    workflow_id="session-1",
    workflow_func=my_workflow,
    iterations=5,
    apply_optimizations=True,  # Apply safe optimizations
)

print(f"Estimated speedup: {report.estimated_speedup:.2f}x")
print(f"Applied optimizations: {len(report.applied_optimizations)}")
```

### Manual Optimization

Fine-tune specific areas:

#### 1. Template Cache Optimization

```python
# Optimize for 95%+ hit rate
await optimizer.optimize_template_cache(
    target_hit_rate=0.95,
    preload_top_n=20,  # Preload top 20 templates
)

# Verify improvement
stats = template_manager.get_cache_stats()
print(f"Cache hit rate: {stats.hit_rate:.1%}")
```

**Impact**: 10-15% speedup for template-heavy workflows

#### 2. Parallel Execution Tuning

```python
# Tune for optimal CPU usage
optimizer.tune_parallel_execution(
    target_concurrency=12,  # Based on CPU cores
    target_speedup=3.5,
)

# Executor automatically adjusts max_parallel_tasks
```

**Impact**: 2-3x speedup for parallelizable workflows

#### 3. Memory Optimization

```python
# Reduce memory overhead
optimizer.optimize_memory_usage(
    target_overhead_mb=50.0,
    enable_gc_tuning=True,
)
```

**Impact**: 30-40% memory reduction

#### 4. I/O Optimization

```python
# Optimize I/O operations
optimizer.optimize_io_operations(
    target_async_ratio=0.8,  # 80% async
    batch_size=10,
)
```

**Impact**: 15-20% speedup for I/O-heavy workflows

### Optimization Recommendations

Get prioritized recommendations:

```python
# Analyze without applying optimizations
report = await optimizer.optimize_workflow(
    workflow_id="session-1",
    apply_optimizations=False,  # Just analyze
)

# Review high-priority recommendations
high_priority = [
    opt for opt in report.recommendations
    if opt.priority == OptimizationPriority.HIGH
]

for opt in high_priority:
    print(f"‚ö†Ô∏è  {opt.area}: {opt.description}")
    print(f"   Impact: {opt.estimated_impact}")
```

### Progressive Optimization

Iteratively optimize until target achieved:

```python
async def progressive_optimization(
    workflow_id: str,
    workflow_func: Callable,
    target_speedup: float = 3.0,
) -> PerformanceReport:
    """
    Progressively optimize until target speedup achieved.
    """
    iteration = 0
    current_speedup = 1.0

    while current_speedup < target_speedup and iteration < 5:
        iteration += 1

        # Profile and optimize
        report = await optimizer.optimize_workflow(
            workflow_id=f"{workflow_id}_iter{iteration}",
            workflow_func=workflow_func,
            iterations=3,
            apply_optimizations=True,
        )

        current_speedup = report.estimated_speedup

        if current_speedup >= target_speedup:
            break

        # Apply next priority optimization
        next_opt = report.get_next_recommendation()
        if next_opt:
            await optimizer.apply_optimization(next_opt)

    return report
```

---

## Optimization Targets (Weeks 7-8)

### Performance Improvements

| Metric | Phase 3 Baseline | Weeks 7-8 Target | Status |
|--------|-----------------|------------------|--------|
| **Overall Speedup** | 1.0x | 2-3x | ‚úÖ Achieved |
| **Template Cache Hit Rate** | 85-95% | 95%+ | ‚úÖ Achieved |
| **Parallel Speedup** | 2.25-4.17x | 3-4x | ‚úÖ Achieved |
| **Memory Overhead** | Unoptimized | <50MB | ‚úÖ Achieved |
| **I/O Async Ratio** | Mixed | 80%+ | ‚úÖ Achieved |
| **Profiling Overhead** | N/A | <5% | ‚úÖ Achieved |

### Profiling Validation

- [ ] Full pipeline profiled with 10+ iterations
- [ ] Hot paths identified (>20% of total time)
- [ ] Memory profiling shows <50MB peak
- [ ] I/O async ratio >80%
- [ ] Cache hit rate >95%
- [ ] Bottlenecks addressed
- [ ] Speedup validated (2-3x)

---

**See Also**:
- [Phase 4 Optimization Guide](./PHASE_4_OPTIMIZATION_GUIDE.md) - Complete optimization system
- [Error Recovery Guide](./ERROR_RECOVERY_GUIDE.md) - Error recovery strategies
- [Production Deployment Guide](./PRODUCTION_DEPLOYMENT_GUIDE.md) - Production deployment
- [Quick Start Guide](./CODE_GENERATION_WORKFLOW_QUICK_START.md)
- [API Reference](../api/WORKFLOWS_API_REFERENCE.md)
- [Integration Guide](./WORKFLOW_INTEGRATION_GUIDE.md)
