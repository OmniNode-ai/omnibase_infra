# SPDX-License-Identifier: MIT
# Copyright (c) 2026 OmniNode Team
#
# ONEX Node Contract
# Node: NodeLlmInferenceEffect
#
# This contract defines the interface for the LLM Inference Effect node,
# which handles provider-agnostic LLM inference operations via delegated
# handlers (OpenAI-compatible, Ollama).
# Contract identifiers
name: "node_llm_inference_effect"
contract_name: "node_llm_inference_effect"
node_name: "node_llm_inference_effect"
contract_version:
  major: 1
  minor: 0
  patch: 0
node_version:
  major: 0
  minor: 1
  patch: 0
# Node type
node_type: "EFFECT_GENERIC"
# Description
description: >
  LLM inference effect node for provider-agnostic inference operations. Delegates to provider-specific handlers (OpenAI-compatible, Ollama) via declarative operation routing. Supports chat completions and text completions with tool calling, generation parameters, and structured response parsing.

# Strongly typed I/O models
# Note: These models live in the shared omnibase_infra.nodes.effects.models
# package rather than co-located with this node. They are shared across all
# LLM inference handlers (Ollama, vLLM, OpenAI-compatible, etc.) to ensure
# a single canonical request/response contract for the inference effect layer.
input_model:
  name: "ModelLlmInferenceRequest"
  module: "omnibase_infra.nodes.effects.models"
  description: "Input model containing LLM inference parameters and routing configuration."
output_model:
  name: "ModelLlmInferenceResponse"
  module: "omnibase_infra.nodes.effects.models"
  description: "Output model containing inference result, usage, and tracing metadata."
# =============================================================================
# HANDLER ROUTING (Declarative Handler Dispatch)
# =============================================================================
handler_routing:
  routing_strategy: "operation_match"
  handlers:
    # OpenAI-Compatible Inference Handler
    - operation: "inference.openai_compatible"
      handler:
        name: "HandlerLlmOpenaiCompatible"
        module: "omnibase_infra.nodes.node_llm_inference_effect.handlers.handler_llm_openai_compatible"
      description: "Execute LLM inference via OpenAI-compatible API (vLLM, OpenAI, etc.)"
    # Ollama Inference Handler
    - operation: "inference.ollama"
      handler:
        name: "HandlerLlmOllama"
        module: "omnibase_infra.nodes.node_llm_inference_effect.handlers.handler_llm_ollama"
      description: "Execute LLM inference via Ollama API (/api/chat, /api/generate)"
  execution_mode: "single"
  max_timeout_seconds: 600
# =============================================================================
# ERROR HANDLING (Circuit Breaker + Retry + Sanitization)
# =============================================================================
error_handling:
  circuit_breaker:
    enabled: true
    per_backend: true
    backends:
      - name: "openai_compatible"
        failure_threshold: 5
        reset_timeout_ms: 60000
        half_open_max_requests: 2
      - name: "ollama"
        failure_threshold: 5
        reset_timeout_ms: 60000
        half_open_max_requests: 2
  retry_policy:
    max_retries: 3
    initial_delay_ms: 200
    max_delay_ms: 10000
    exponential_base: 2
    retry_on:
      - "InfraConnectionError"
      - "InfraTimeoutError"
      - "InfraUnavailableError"
      - "InfraRateLimitedError"
  error_sanitization:
    enabled: true
    utility: "omnibase_infra.utils.util_error_sanitization"
    sanitize_patterns:
      - "api_key"
      - "token"
      - "bearer"
      - "authorization"
      - "secret"
      - "password"
# =============================================================================
# IO OPERATIONS (EFFECT node specific)
# =============================================================================
io_operations:
  - operation: "inference.openai_compatible"
    description: "Execute LLM inference via OpenAI-compatible API"
  - operation: "inference.ollama"
    description: "Execute LLM inference via Ollama API"
# Dependencies
dependencies:
  - name: "protocol_llm_http_transport"
    type: "mixin"
    class_name: "MixinLlmHttpTransport"
    module: "omnibase_infra.mixins.mixin_llm_http_transport"
    description: "HTTP transport for LLM API calls with retry and circuit breaker"
# Capabilities
capabilities:
  - name: "openai_compatible_inference"
    description: "Execute inference via OpenAI-compatible API (vLLM, OpenAI, etc.)"
  - name: "ollama_inference"
    description: "Execute inference via Ollama native API"
  - name: "chat_completion"
    description: "Multi-turn chat completion with tool calling support"
  - name: "tool_calling"
    description: "Function/tool calling with structured arguments"
  - name: "circuit_breaker_protection"
    description: "Per-backend circuit breaker for provider fault isolation"
# Metadata
metadata:
  author: "OmniNode Team"
  license: "MIT"
  created: "2026-02-13"
  updated: "2026-02-14"
  tags:
    - effect
    - llm
    - inference
    - openai-compatible
    - ollama
    - circuit-breaker
    - tool-calling
